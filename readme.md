# ARC Solver - Multi-Provider LLM System with K-Sample Diversity

A complete pipeline for solving ARC (Abstraction and Reasoning Corpus) tasks using a multi-phase LLM approach with DSL-based code generation and diversity sampling.

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CONFIGURATION                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  config/config.yaml                                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Provider selection (Grok/Qwen/Gemini)                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Model parameters (max_tokens, retries, etc.)                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Pipeline settings (k_samples, timeout, etc.)                  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         PHASE 1: Find Similar                           ‚îÇ
‚îÇ                  Execution-Based Similarity Search                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Search library for programs with similar execution patterns   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Test programs on training examples                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Calculate grid similarity scores                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Return top-K similar programs (similarity > 0.1)              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                    ‚îÇ                                    ‚îÇ
‚îÇ                            Perfect Match (1.0)?                         ‚îÇ
‚îÇ                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ                        YES                    NO                        ‚îÇ
‚îÇ                         ‚îÇ                      ‚îÇ                        ‚îÇ
‚îÇ                       DONE                     ‚ñº                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PHASE 2A: Hypothesis Formation                       ‚îÇ
‚îÇ               Generate K diverse hypotheses per task                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  For each task, generate K samples (e.g., K=4):                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Analyze training examples sequentially                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Discover transformation patterns                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Output pattern summary in natural language                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Diversity through temperature sampling                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                    ‚îÇ                                    ‚îÇ
‚îÇ                          Batch all prompts ‚Üí Parallel API calls         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PHASE 2B: Hypothesis Validation                      ‚îÇ
‚îÇ                  Validate & refine hypotheses                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  For each hypothesis from 2A:                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Check if pattern extends to test input                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Refine hypothesis if needed                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Output validated pattern description                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                    ‚îÇ                                    ‚îÇ
‚îÇ                          Batch all prompts ‚Üí Parallel API calls         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PHASE 2C: Code Generation                            ‚îÇ
‚îÇ              Generate executable code from patterns                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  For each validated pattern:                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Generate Python code using DSL primitives                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Include similar programs as reference                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Output: def solve(I): ...                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                    ‚îÇ                                    ‚îÇ
‚îÇ                          Batch all prompts ‚Üí Parallel API calls         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BEST-OF-K SELECTION                                  ‚îÇ
‚îÇ              Test all K programs, select best                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  1. Test all K programs on TRAINING set                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  2. Select top 2 candidates                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  3. Test both on TEST set                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  4. Select best performer on test set                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  5. Compare with library fallback                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  6. Return final solution (score = 1.0 ‚Üí add to library)         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üì¶ Project Structure

```
llms_ftw/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml              # Main configuration file
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # Main pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ vlm_client.py            # Multi-provider VLM client
‚îÇ   ‚îú‚îÄ‚îÄ vlm_prompter.py          # Prompt builders for each phase
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ library.py           # Program library & similarity search
‚îÇ       ‚îú‚îÄ‚îÄ dsl.py               # Domain-Specific Language (100+ primitives)
‚îÇ       ‚îî‚îÄ‚îÄ constants.py         # DSL constants (colors, directions)
‚îú‚îÄ‚îÄ run_main.slurm              # Basic SLURM submission script
‚îú‚îÄ‚îÄ run_with_config.slurm       # Advanced SLURM with config selection
‚îú‚îÄ‚îÄ RUN_INSTRUCTIONS.md         # Detailed execution guide
‚îî‚îÄ‚îÄ readme.md                   # This file
```

## üéØ Key Features

### 1. **YAML-Based Configuration**
All hyperparameters in one place - no more hardcoded values!
- Provider selection (Grok, Qwen, Gemini)
- Model settings (tokens, retries, temperature)
- Pipeline parameters (k_samples, timeout, workers)
- Input/output directories

### 2. **Multi-Provider Support**
Unified interface for multiple LLM providers:
- **Grok** (X.AI's API)
- **Qwen** (local vLLM server)
- **Gemini** (Google's API)
- Easy to add more OpenAI-compatible providers

### 3. **K-Sample Diversity**
Generate K diverse solutions per task, select the best:
- Reduces risk of single bad hypothesis
- Temperature-based diversity
- Best-of-K selection on test set
- Configurable K value (default: 4)

### 4. **Simplified Architecture**
Clean, maintainable codebase:
- `OpenAICompatibleClient` for Grok/Qwen/etc.
- `GeminiClient` for Gemini's different API
- Built-in error suppression (no wrapper classes)
- Direct configuration ‚Üí client flow

### 5. **SLURM Support**
Ready for cluster execution:
- Pre-configured SLURM scripts
- Multiple config file support
- Job monitoring and logging
- Easy customization for your cluster

## üöÄ Quick Start

### 1. Setup Environment

```bash
# Install dependencies
pip install pyyaml requests python-dotenv

# Set API keys in .env file
echo "GROK_API_KEY=your_key_here" > .env
# echo "GEMINI_API_KEY=your_key_here" >> .env  # if using Gemini
```

### 2. Configure Settings

Edit `config/config.yaml`:

```yaml
# Choose provider
provider: "grok"  # or "qwen", "gemini"

# Model configuration
model:
  name: "grok-4-fast"
  api_base: "https://api.x.ai/v1"

# Pipeline settings
process_directory:
  data_dir: "data_v2/evaluation"
  k_samples: 4              # Number of diverse samples per task
  timeout: 2                # Execution timeout (seconds)
  max_find_similar_workers: 56
  log_dir: "logs_baseline"
  verbose: false
  similar: true
  few_shot: true

# Output
output:
  results_dir: "results/baseline"
```

### 3. Run the Solver

#### Local/Interactive:
```bash
python src/main.py
```

#### SLURM Cluster:
```bash
# Basic submission
sbatch run_main.slurm

# With specific config
sbatch run_with_config.slurm baseline
```

See `RUN_INSTRUCTIONS.md` for detailed execution guide.

## üîß Core Components

### VLM Client (`vlm_client.py`)

**Unified multi-provider client with two classes:**

```python
# OpenAI-compatible providers (Grok, Qwen, Claude, GPT, etc.)
class OpenAICompatibleClient(BaseVLMClient):
    - Handles standard chat/completions endpoint
    - Conditional API key (supports local servers)
    - Automatic retry with exponential backoff
    - Built-in error suppression

# Google Gemini (different API structure)
class GeminiClient(BaseVLMClient):
    - Custom endpoint format
    - Different payload structure
    - Same retry/error handling
```

**Configuration:**
```python
config = VLMConfig(
    api_key="your_key",
    model="grok-4-fast",
    api_base="https://api.x.ai/v1",
    max_tokens=16384,
    max_retries=3,
    suppress_errors=True  # Return empty string on errors
)

client = create_client("grok", config=config)
response = client.query(prompt, system_prompt)
```

### Pipeline (`main.py`)

**Main orchestration with batched execution:**

1. **Load Config:** Read YAML configuration
2. **Phase 1:** Find similar programs (parallel)
3. **Phase 2A:** Generate K hypotheses (batched, parallel)
4. **Phase 2B:** Validate hypotheses (batched, parallel)
5. **Phase 2C:** Generate code (batched, parallel)
6. **Selection:** Test all K programs, select best

**Key Function:**
```python
def process_directory(
    data_dir: str,
    vlm_client_phase1: BaseVLMClient,
    vlm_client_phase2: BaseVLMClient,
    prompter: VLMPrompter,
    library: ProgramLibrary,
    timeout: int = 2,
    k_samples: int = 1,
    max_find_similar_workers: int = 4,
    log_dir: str = "logs",
    verbose: bool = True,
    similar: bool = True,
    few_shot: bool = True
) -> List[TaskResult]
```

### Program Library (`utils/library.py`)

**Execution-based similarity search:**

```python
library = ProgramLibrary()

# Find similar programs by execution
similar = library.find_similar(
    train_examples=task['train'],
    top_k=5,
    min_similarity=0.1,
    timeout=2
)

# Add successful solutions
if score == 1.0:
    library.add(task_id, program_code)
```

### DSL (`utils/dsl.py`)

**100+ primitives for grid transformations:**

```python
# Transforms
hmirror(grid)          # horizontal flip
vmirror(grid)          # vertical flip
rot90/180/270(grid)    # rotations

# Composition
vconcat(a, b)          # stack vertically
hconcat(a, b)          # stack horizontally

# Objects
objects(grid, T, F, T) # find connected regions
colorfilter(objs, c)   # filter by color

# Functional
compose(f, g)          # f(g(x))
fork(combine, f, g)    # combine(f(x), g(x))
```

## üìä How K-Sample Diversity Works

### The Problem
Single LLM call can produce incorrect hypothesis ‚Üí wrong code ‚Üí failure

### The Solution
Generate K diverse hypotheses, test all, select best:

```
Task ‚Üí Phase 2A (K samples) ‚Üí K hypotheses
     ‚Üí Phase 2B (K samples) ‚Üí K validated patterns
     ‚Üí Phase 2C (K samples) ‚Üí K programs
     ‚Üí Test all K programs ‚Üí Select best on test set
```

### Selection Strategy

1. **Training Set:** Test all K programs on training examples
2. **Top 2:** Select two best performers on training set
3. **Test Set:** Test both on test examples
4. **Final:** Select whichever performs better on test set
5. **Fallback:** Compare with library, use library if better

### Example Results
With K=4:
- Sample 0 selected: 40% of tasks
- Sample 1 selected: 25% of tasks
- Sample 2 selected: 20% of tasks
- Sample 3 selected: 15% of tasks

**Key Insight:** Different samples win on different tasks!

## üéõÔ∏è Configuration Reference

### Provider Settings

```yaml
provider: "grok"  # Options: grok, qwen, gemini

model:
  name: "grok-4-fast"
  api_base: "https://api.x.ai/v1"
```

**Provider-specific defaults:**

| Provider | Model | API Base | Auth |
|----------|-------|----------|------|
| Grok | grok-4-fast | https://api.x.ai/v1 | GROK_API_KEY |
| Qwen | Qwen/Qwen2.5-7B-Instruct | http://localhost:8000/v1 | None (local) |
| Gemini | gemini-2.5-pro | https://generativelanguage.googleapis.com/v1beta | GEMINI_API_KEY |

### VLM Config

```yaml
vlm_config:
  phase1:  # Hypothesis & validation
    max_tokens: 16384
    max_retries: 3
    save_prompts: false
    prompt_log_dir: "prompts_old_dsl"

  phase2:  # Code generation
    max_tokens: 8192
    max_retries: 3
```

### Pipeline Parameters

```yaml
process_directory:
  data_dir: "data_v2/evaluation"    # Input task directory
  timeout: 2                        # Execution timeout (seconds)
  k_samples: 4                      # Number of samples per task
  max_find_similar_workers: 56      # Parallel workers for phase 1
  log_dir: "logs_baseline"          # Log output directory
  verbose: false                    # Print detailed progress
  similar: true                     # Use similarity search
  few_shot: true                    # Include similar programs in prompts
```

### Output Settings

```yaml
output:
  results_dir: "results/baseline"
```

## üìà Performance & Costs

### Batched Execution
All API calls within each phase run in parallel:
- Phase 2A: All tasks √ó K samples in parallel
- Phase 2B: All tasks √ó K samples in parallel
- Phase 2C: All tasks √ó K samples in parallel

### Token Usage (per task with K=4)
- Phase 2A: ~4 calls √ó 16K tokens = 64K tokens
- Phase 2B: ~4 calls √ó 16K tokens = 64K tokens
- Phase 2C: ~4 calls √ó 8K tokens = 32K tokens
- **Total: ~160K tokens per task**

### Approximate Costs (K=4)
With Grok API (~$5/1M input, $15/1M output tokens):
- ~$0.80-$2.40 per task (depending on output length)
- 100 tasks: ~$80-$240

*Note: Costs vary by provider and token usage*

## üîç Debugging & Monitoring

### Enable Verbose Mode
```yaml
process_directory:
  verbose: true
```

Shows:
- Phase timing breakdown
- Per-task progress with scores
- Sample selection statistics
- Library matches

### Logs
All outputs saved to configured directories:
- **Pipeline logs:** `<log_dir>/<task_id>_phase2*_*.txt`
- **Selection logs:** `<log_dir>/<task_id>_selection_summary.txt`
- **SLURM logs:** `logs/slurm_<job_id>.out`
- **Results:** `<results_dir>/results.json` and `summary.csv`

### Common Issues

**Config file not found:**
```bash
ls config/config.yaml  # Make sure it exists
```

**API key errors:**
```bash
cat .env  # Check API keys are set
```

**Import errors:**
```bash
# Make sure you're in project root
cd /home/user/llms_ftw
python src/main.py
```

## üÜö Architecture Evolution

### Before (Old)
```
VLMConfig ‚Üí create_client() ‚Üí GrokClient/QwenClient/GeminiClient
                                        ‚Üì
                             ThreadSafeVLMClient (wrapper)
                                        ‚Üì
                                  Catches errors
```
- 3 separate client classes (GrokClient, QwenClient, GeminiClient)
- Wrapper class for error handling
- Hardcoded hyperparameters in main.py

### After (Current)
```
config.yaml ‚Üí load_config() ‚Üí VLMConfig ‚Üí create_client()
                                              ‚Üì
                              OpenAICompatibleClient OR GeminiClient
                              (built-in error suppression)
```
- 2 client classes (merged OpenAI-compatible ones)
- No wrapper needed (built-in suppress_errors)
- All configuration in YAML

**Result:**
- 40+ lines of duplicate code removed
- Cleaner architecture
- Easier to add new providers
- All settings in one place

## üîÆ Future Enhancements

### Planned Features
- [ ] Command-line config file selection: `python main.py --config exp1.yaml`
- [ ] Adaptive K sampling (increase K for harder tasks)
- [ ] Semantic library search with embeddings
- [ ] LLM-guided program repair with error feedback
- [ ] Multi-model ensembles (combine Grok + Gemini)

### Adding New Providers

For OpenAI-compatible APIs (Claude, GPT, etc.):
```yaml
# Just update config.yaml - no code changes needed!
provider: "claude"
model:
  name: "claude-sonnet-4"
  api_base: "https://api.anthropic.com/v1"
```

Then add to environment:
```bash
export CLAUDE_API_KEY=your_key
```

## üìö References

- **ARC Dataset:** https://github.com/fchollet/ARC
- **DSL Design:** Based on functional programming primitives
- **K-Sample Diversity:** Inspired by best-of-N sampling strategies

## üìÑ License

Your project - use as you wish!

---

**Ready to solve ARC tasks with configurable multi-provider LLMs!** üöÄ
