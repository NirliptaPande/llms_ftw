{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628462c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flowers/work/llms_ftw/src/vlm_prompter.py:359: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  diagonal_mirror(grid) -> Grid/Patch                  # diagonal \\ mirror\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main pipeline for ARC task solving using execution-based similarity.\n",
    "\n",
    "Pipeline: Program Similarity â†’ Pattern Discovery â†’ Code Generation\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# print(\"Script starting...\", flush=True)\n",
    "# sys.stdout.flush()\n",
    "\n",
    "from src.vlm_prompter import VLMPrompter\n",
    "from src.vlm_client import VLMConfig, create_client, BaseVLMClient\n",
    "from src.utils.library import ProgramLibrary, calculate_grid_similarity\n",
    "from src.utils.dsl import *\n",
    "from src.utils.constants import *\n",
    "# from utils.render_legacy import grid_to_base64_png_oai_content\n",
    "\n",
    "print(\"Imports done...\", flush=True)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae120cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskResult:\n",
    "    \"\"\"Result of attempting to solve a task\"\"\"\n",
    "    task_id: str\n",
    "    success: bool\n",
    "    score: float\n",
    "    program: Optional[str] = None\n",
    "    phase1_output: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "def test_program(program_code: str, task: Dict) -> Tuple[float, List[Tuple[Any, Any, bool]]]:\n",
    "    \"\"\"\n",
    "    Test a program against task training examples.\n",
    "    \n",
    "    Returns:\n",
    "        - Average score across examples\n",
    "        - List of (expected_output, actual_output, passed) tuples\n",
    "    \"\"\"\n",
    "    namespace = globals().copy()\n",
    "    \n",
    "    try:\n",
    "        exec(program_code, namespace)\n",
    "        \n",
    "        if 'solve' not in namespace:\n",
    "            return 0.0, []\n",
    "        \n",
    "        solve_fn = namespace['solve']\n",
    "        scores = []\n",
    "        results = []\n",
    "        \n",
    "        for example in task['train']:\n",
    "            inp = example['input']\n",
    "            expected = example['output']\n",
    "            if isinstance(inp, list):\n",
    "                inp = tuple(tuple(row) for row in inp)\n",
    "            try:\n",
    "                actual = solve_fn(inp)\n",
    "                score = calculate_grid_similarity(actual, expected)\n",
    "                scores.append(score)\n",
    "                results.append((expected, actual, score == 1.0))\n",
    "            except Exception as e:\n",
    "                scores.append(0.0)\n",
    "                results.append((expected, None, False))\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "        return avg_score, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0, []\n",
    "\n",
    "\n",
    "def extract_code_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract Python code from LLM response.\"\"\"\n",
    "    python_blocks = re.findall(r'```python\\n(.*?)```', response, re.DOTALL)\n",
    "    \n",
    "    if python_blocks:\n",
    "        for block in python_blocks:\n",
    "            if 'def solve' in block:\n",
    "                return block.strip()\n",
    "        return python_blocks[0].strip()\n",
    "    \n",
    "    match = re.search(r'(def solve\\(I\\):.*?)(?=\\n\\ndef|\\n\\nif __name__|$)', response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def solve_task(\n",
    "    task: Dict,\n",
    "    task_id: str,\n",
    "    vlm_client_phase1: BaseVLMClient,\n",
    "    vlm_client_phase2: BaseVLMClient,\n",
    "    prompter: VLMPrompter,\n",
    "    library: ProgramLibrary,\n",
    "    verbose: bool = True,\n",
    "    n_workers: int = None,\n",
    "    timeout: int = 2,\n",
    "    log_dir: str = \"logs\"\n",
    ") -> TaskResult:\n",
    "    \"\"\"\n",
    "    Solve a single ARC task using execution-based pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Find similar programs by execution (parallelized)\n",
    "    2. Phase 1: Pattern discovery (natural language) with similar programs\n",
    "    3. Phase 2: Code generation with pattern + similar programs\n",
    "    \n",
    "    Args:\n",
    "        task: Task dictionary with 'train' examples\n",
    "        task_id: Unique task identifier\n",
    "        vlm_client: VLM client for queries\n",
    "        prompter: Prompt builder\n",
    "        library: Program library\n",
    "        verbose: Print progress\n",
    "        n_workers: Number of parallel workers (None = auto)\n",
    "        timeout: Timeout per program execution in seconds\n",
    "        log_dir: Directory to save logs (default: \"logs\")\n",
    "    \"\"\"\n",
    "    # Create log directory if it doesn't exist\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\", flush=True)\n",
    "        print(f\"Solving Task: {task_id}\", flush=True)\n",
    "        print(f\"{'='*80}\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # STEP 1: Find Similar Programs by Execution (PARALLELIZED)\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nðŸ” Finding similar programs by execution...\", flush=True)\n",
    "        \n",
    "        similar_programs = library.find_similar(\n",
    "            train_examples=task['train'],\n",
    "            top_k=5,\n",
    "            min_similarity=0.1,\n",
    "            n_workers=n_workers,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            if similar_programs:\n",
    "                print(f\"   Found {len(similar_programs)} similar programs:\", flush=True)\n",
    "                for i, prog in enumerate(similar_programs[:3], 1):\n",
    "                    print(f\"   {i}. Task {prog['task_id']}: {prog['similarity']:.2f}\", flush=True)\n",
    "            else:\n",
    "                print(\"   No similar programs found\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TEST LIBRARY PROGRAMS: Try existing solutions first\n",
    "        # ====================================================================\n",
    "        best_library_score = 0.0\n",
    "        best_library_program = None\n",
    "        \n",
    "        if similar_programs:\n",
    "            best_match = similar_programs[0]\n",
    "            best_library_score = best_match['similarity']\n",
    "            best_library_program = best_match['program']\n",
    "            if verbose:\n",
    "                print(f\"\\nâœ“ Best library match: Task {best_match['task_id']} ({best_library_score:.2f})\", flush=True)\n",
    "            \n",
    "            if best_library_score == 1.0:\n",
    "                if verbose:\n",
    "                    print(f\"   â†’ Perfect match found! Using library solution.\", flush=True)\n",
    "                return TaskResult(\n",
    "                    task_id=task_id,\n",
    "                    success=True,\n",
    "                    score=1.0,\n",
    "                    program=best_library_program\n",
    "                )\n",
    "                \n",
    "        # ====================================================================\n",
    "        # PHASE 1: Pattern Discovery (Natural Language)\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nðŸ“ Phase 1: Pattern Discovery...\", flush=True)\n",
    "        \n",
    "        # Pass the raw similar_programs list - prompter will format it\n",
    "        phase1_prompt = prompter.build_phase1_prompt(task, similar_programs)\n",
    "        \n",
    "        phase1_output = vlm_client_phase1.query(\n",
    "            phase1_prompt,\n",
    "            system_prompt=\"\"\"\"You are an expert at analyzing ARC puzzles and discovering transformation patterns.\n",
    "\n",
    "Remember: Your first hypothesis is sticky and excessively convincing to you.\n",
    "Combat this by evolving your hypothesis and actively seeking evidence against your initial guess to avoid halluciantion.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # LOG PHASE 1 OUTPUT\n",
    "        phase1_log_path = os.path.join(log_dir, f\"{task_id}_phase1_output.txt\")\n",
    "        with open(phase1_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Task ID: {task_id}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"PHASE 1: PATTERN DISCOVERY OUTPUT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            f.write(phase1_output)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   âœ“ Phase 1 complete ({len(phase1_output)} chars)\", flush=True)\n",
    "            print(f\"   ðŸ“„ Logged to: {phase1_log_path}\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 2: Code Generation\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nâš™ï¸  Phase 2: Code Generation...\", flush=True)\n",
    "        \n",
    "        # Pass the raw similar_programs list - prompter will format it\n",
    "        phase2_prompt = prompter.build_phase2_prompt(task, \n",
    "            phase1_output,\n",
    "            similar_programs\n",
    "        )\n",
    "        \n",
    "        phase2_output = vlm_client_phase2.query(\n",
    "            phase2_prompt,\n",
    "            system_prompt=\"You are an expert at generating code using the given DSL primitives to solve ARC puzzles. You are provided with a natural language description of the pattern to implement, as well as training examples and some similar programs you might find useful as reference. Generate a Python function `def solve(I):` that implements the described transformation using ONLY the provided DSL primitives. Ensure your code is syntactically correct and follows best practices.\"\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   âœ“ Phase 2 complete ({len(phase2_output)} chars)\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # EXTRACT AND TEST GENERATED CODE\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nðŸ§ª Testing generated program...\", flush=True)\n",
    "        \n",
    "        generated_code = extract_code_from_response(phase2_output)\n",
    "        \n",
    "        if not generated_code:\n",
    "            if verbose:\n",
    "                print(\"   âœ— Failed to extract code\", flush=True)\n",
    "            \n",
    "            if best_library_program and best_library_score > 0.5:\n",
    "                if verbose:\n",
    "                    print(f\"   â†’ Falling back to library (score: {best_library_score:.2f})\", flush=True)\n",
    "                return TaskResult(\n",
    "                    task_id=task_id,\n",
    "                    success=False,\n",
    "                    score=best_library_score,\n",
    "                    program=best_library_program,\n",
    "                    phase1_output=phase1_output,\n",
    "                    error=\"Code extraction failed, using library fallback\"\n",
    "                )\n",
    "            \n",
    "            return TaskResult(\n",
    "                task_id=task_id,\n",
    "                success=False,\n",
    "                score=0.0,\n",
    "                phase1_output=phase1_output,\n",
    "                error=\"Failed to extract code from response\"\n",
    "            )\n",
    "        \n",
    "        score, results = test_program(generated_code, task)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Generated score: {score:.2f}\", flush=True)\n",
    "        \n",
    "        # LOG PHASE 2 OUTPUT WITH TEST RESULTS\n",
    "        phase2_log_path = os.path.join(log_dir, f\"{task_id}_phase2_results.txt\")\n",
    "        with open(phase2_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Task ID: {task_id}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"PHASE 2: CODE GENERATION & TEST RESULTS\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"GENERATED CODE:\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(generated_code + \"\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"SCORE: {score:.2f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"TEST RESULTS:\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            for i, (expected, actual, passed) in enumerate(results, 1):\n",
    "                f.write(f\"\\nExample {i}: {'âœ“ PASS' if passed else 'âœ— FAIL'}\\n\")\n",
    "                f.write(f\"Expected Output:\\n\")\n",
    "                if expected:\n",
    "                    f.write(f\"{json.dumps([list(row) for row in expected], indent=2)}\\n\")\n",
    "                else:\n",
    "                    f.write(\"None\\n\")\n",
    "                f.write(f\"Actual Output:\\n\")\n",
    "                if actual:\n",
    "                    f.write(f\"{json.dumps([list(row) for row in actual], indent=2)}\\n\")\n",
    "                else:\n",
    "                    f.write(\"None (execution failed)\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   ðŸ“„ Logged to: {phase2_log_path}\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # DECIDE FINAL PROGRAM\n",
    "        # ====================================================================\n",
    "        success = score == 1.0\n",
    "        final_program = generated_code\n",
    "        final_score = score\n",
    "        \n",
    "        if not success and best_library_score > score:\n",
    "            if verbose:\n",
    "                print(f\"   â†’ Library program better ({best_library_score:.2f} > {score:.2f})\", flush=True)\n",
    "            final_program = best_library_program\n",
    "            final_score = best_library_score\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SAVE TO LIBRARY IF SUCCESSFUL\n",
    "        # ====================================================================\n",
    "        if success:\n",
    "            namespace = globals().copy()\n",
    "            exec(final_program, namespace)\n",
    "            if 'solve' in namespace:\n",
    "                library.add(task_id, final_program)\n",
    "                if verbose:\n",
    "                    print(f\"   âœ“ Added to library\", flush=True)\n",
    "        \n",
    "        return TaskResult(\n",
    "            task_id=task_id,\n",
    "            success=success,\n",
    "            score=final_score,\n",
    "            program=final_program,\n",
    "            phase1_output=phase1_output\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"   âœ— Error: {e}\", flush=True)\n",
    "        \n",
    "        return TaskResult(\n",
    "            task_id=task_id,\n",
    "            success=False,\n",
    "            score=0.0,\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "def process_directory(\n",
    "    data_dir: str,\n",
    "    vlm_client_phase1: BaseVLMClient,\n",
    "    vlm_client_phase2: BaseVLMClient,\n",
    "    prompter: VLMPrompter,\n",
    "    library: ProgramLibrary,\n",
    "    verbose: bool = True,\n",
    "    n_workers: int = None,\n",
    "    timeout: int = 2,\n",
    "    evaluate_on_n_pb: int = -1,\n",
    ") -> List[TaskResult]:\n",
    "    \"\"\"\n",
    "    Process all task files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing task JSON files\n",
    "        vlm_client: VLM client for queries\n",
    "        prompter: Prompt builder\n",
    "        library: Program library\n",
    "        verbose: Print progress\n",
    "        n_workers: Number of parallel workers for library search (None = auto)\n",
    "        timeout: Timeout per program execution in seconds\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Error: Directory not found: {data_dir}\", flush=True)\n",
    "        return []\n",
    "    \n",
    "    json_files = sorted(data_path.glob('*.json'))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {data_dir}\", flush=True)\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nFound {len(json_files)} tasks in {data_dir}\\n\", flush=True)\n",
    "    if evaluate_on_n_pb != -1:\n",
    "        json_files = json_files[:evaluate_on_n_pb]\n",
    "        print (f\"Evaluating on first {evaluate_on_n_pb} tasks\\n\", flush=True)\n",
    "    results = []\n",
    "    successful = 0\n",
    "    total_score = 0.0\n",
    "    \n",
    "    for i, task_file in enumerate(json_files, 1):\n",
    "        task_id = task_file.stem\n",
    "        \n",
    "        try:\n",
    "            with open(task_file, 'r') as f:\n",
    "                task = json.load(f)\n",
    "            \n",
    "            result = solve_task(\n",
    "                task=task,\n",
    "                task_id=task_id,\n",
    "                vlm_client_phase1=vlm_client_phase1,\n",
    "                vlm_client_phase2=vlm_client_phase2,\n",
    "                prompter=prompter,\n",
    "                library=library,\n",
    "                verbose=verbose,\n",
    "                n_workers=n_workers,\n",
    "                timeout=timeout,\n",
    "                log_dir=\"logs_images\"#TODO change log dir\n",
    "            )\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if result.success:\n",
    "                successful += 1\n",
    "            \n",
    "            total_score += result.score\n",
    "            \n",
    "            status = \"âœ“\" if result.success else \"âœ—\"\n",
    "            print(f\"{status} [{i}/{len(json_files)}] {task_id}: {result.score:.2f}\", flush=True)\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âœ— [{i}/{len(json_files)}] {task_id}: Invalid JSON - {e}\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— [{i}/{len(json_files)}] {task_id}: {e}\", flush=True)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\", flush=True)\n",
    "    print(f\"SUMMARY\", flush=True)\n",
    "    print(f\"{'='*80}\", flush=True)\n",
    "    print(f\"Total tasks: {len(json_files)}\", flush=True)\n",
    "    print(f\"Successful: {successful}/{len(json_files)} ({100*successful/len(json_files):.1f}%)\", flush=True)\n",
    "    print(f\"Average score: {total_score/len(json_files):.2f}\", flush=True)\n",
    "    print(f\"{'='*80}\\n\", flush=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results: List[TaskResult], output_dir: str = 'results') -> None:\n",
    "    \"\"\"Save results to JSON and CSV files.\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # JSON\n",
    "    json_file = output_path / 'results.json'\n",
    "    with open(json_file, 'w') as f:\n",
    "        json_data = [\n",
    "            {\n",
    "                'task_id': r.task_id,\n",
    "                'success': r.success,\n",
    "                'score': r.score,\n",
    "                'error': r.error,\n",
    "                'program': r.program,\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    print(f\"Saved detailed results to {json_file}\", flush=True)\n",
    "    \n",
    "    # CSV\n",
    "    csv_file = output_path / 'summary.csv'\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['task_id', 'success', 'score', 'error'])\n",
    "        for r in results:\n",
    "            writer.writerow([r.task_id, r.success, f'{r.score:.2f}', r.error or ''])\n",
    "    print(f\"Saved summary to {csv_file}\", flush=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    from dotenv import load_dotenv\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    # print(\"Initializing components...\", flush=True)\n",
    "    load_dotenv()\n",
    "    PROVIDER = \"grok\"\n",
    "    if PROVIDER == \"grok\":\n",
    "        api_key = os.getenv('GROK_API_KEY')\n",
    "        api_base = \"https://api.x.ai/v1\"\n",
    "        model = \"grok-4-fast\"\n",
    "    elif PROVIDER == \"qwen\":\n",
    "        api_key = None\n",
    "        api_base = \"http://localhost:8000/v1\"\n",
    "        model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    elif PROVIDER == \"gemini\":\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        api_base = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
    "        model = \"gemini-2.5-pro\"\n",
    "        \n",
    "    vlm_config_phase1 = VLMConfig(\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        api_base=api_base,\n",
    "        max_tokens=16384,  # Longer for analysis\n",
    "        save_prompts=False,\n",
    "        prompt_log_dir=\"prompts_testing\"\n",
    "    )\n",
    "    vlm_config_phase2 = VLMConfig(\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        api_base=api_base,\n",
    "        max_tokens=8192   # Shorter for code gen\n",
    "    )\n",
    "    \n",
    "    vlm_client_phase1 = create_client(PROVIDER, config=vlm_config_phase1)\n",
    "    # print(\"VLM client created\", flush=True)\n",
    "    \n",
    "    vlm_client_phase2 = create_client(PROVIDER, config=vlm_config_phase2)\n",
    "    prompter = VLMPrompter()\n",
    "    # print(\"Prompter created\", flush=True)\n",
    "    \n",
    "    library = ProgramLibrary()  # Auto-loads from solvers.py\n",
    "    # print(\"Loaded library...\", flush=True)\n",
    "    #sanity check\n",
    "    print(f\"Loaded {len(library)} programs from library\", flush=True)\n",
    "    if len(library) > 0:\n",
    "        print(f\"First program: {library.programs[0]['task_id']}\", flush=True)\n",
    "    \n",
    "    # Configure parallelization\n",
    "    results = process_directory(\n",
    "        data_dir='data_v1/eval_size_10',\n",
    "        vlm_client_phase1=vlm_client_phase1,\n",
    "        vlm_client_phase2=vlm_client_phase2,\n",
    "        prompter=prompter,\n",
    "        library=library,\n",
    "        verbose=True,\n",
    "        n_workers=None,  # Auto-detect CPUs (recommended)\n",
    "        timeout=2        # 2 second timeout per program\n",
    "    )\n",
    "    \n",
    "    save_results(results, output_dir='results/images')#TODO change output dir\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # print(\"Starting main...\", flush=True)\n",
    "#     sys.stdout.flush()\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0e47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = None\n",
    "api_base = \"http://localhost:8000/v1\"\n",
    "model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "PROVIDER = \"qwen\"\n",
    "vlm_config_phase1 = VLMConfig(\n",
    "    api_key=api_key,\n",
    "    model=model,\n",
    "    api_base=api_base,\n",
    "    max_tokens=16384,  # Longer for analysis\n",
    "    save_prompts=False,\n",
    "    prompt_log_dir=\"prompts_testing\"\n",
    ")\n",
    "vlm_config_phase2 = VLMConfig(\n",
    "    api_key=api_key,\n",
    "    model=model,\n",
    "    api_base=api_base,\n",
    "    max_tokens=8192   # Shorter for code gen\n",
    ")\n",
    "vlm_client_phase1 = create_client(PROVIDER, config=vlm_config_phase1)\n",
    "# print(\"VLM client created\", flush=True)\n",
    "\n",
    "vlm_client_phase2 = create_client(PROVIDER, config=vlm_config_phase2)\n",
    "prompter = VLMPrompter(use_vision=False)\n",
    "\n",
    "library = ProgramLibrary()  # Auto-loads from solvers.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6030c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 programs from library\n",
      "First program: 007bbfb7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(library)} programs from library\", flush=True)\n",
    "if len(library) > 0:\n",
    "    print(f\"First program: {library.programs[0]['task_id']}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb71e66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/0934a4d8.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/135a2760.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/136b0064.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/13e47133.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/142ca369.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/16b78196.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/16de56c4.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/1818057f.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/195c6913.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/1ae2feb7.json')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/flowers/work/llms_ftw/tasks/evaluation/\"\n",
    "data_path = Path(data_path)\n",
    "json_files = sorted(data_path.glob('*.json'))\n",
    "json_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c595fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 120 tasks in /home/flowers/work/llms_ftw/tasks/evaluation/\n",
      "\n",
      "Evaluating on first 1 tasks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/flowers/work/llms_ftw/tasks/evaluation/\"\n",
    "verbose = True\n",
    "n_workers = None\n",
    "timeout = 2\n",
    "evaluate_on_n_pb = 1\n",
    "\n",
    "\n",
    "data_path = Path(data_dir)\n",
    "\n",
    "\n",
    "json_files = sorted(data_path.glob('*.json'))\n",
    "\n",
    "\n",
    "print(f\"\\nFound {len(json_files)} tasks in {data_dir}\\n\", flush=True)\n",
    "if evaluate_on_n_pb != -1:\n",
    "    json_files = json_files[:evaluate_on_n_pb]\n",
    "    print (f\"Evaluating on first {evaluate_on_n_pb} tasks\\n\", flush=True)\n",
    "results = []\n",
    "successful = 0\n",
    "total_score = 0.0\n",
    "\n",
    "i=1\n",
    "task_file = json_files[0]\n",
    "task_id = task_file.stem\n",
    "with open(task_file, 'r') as f:\n",
    "    task = json.load(f)\n",
    "\n",
    "task=task\n",
    "task_id=task_id\n",
    "# vlm_client_phase1=vlm_client_phase1\n",
    "# vlm_client_phase2=vlm_client_phase2\n",
    "# prompter=prompter\n",
    "# library=library\n",
    "verbose=verbose\n",
    "n_workers=n_workers\n",
    "timeout=timeout\n",
    "log_dir=\"logs\"#TODO change log dir\n",
    "\n",
    "# result = solve_task(\n",
    "# task=task,\n",
    "# task_id=task_id,\n",
    "# vlm_client_phase1=vlm_client_phase1,\n",
    "# vlm_client_phase2=vlm_client_phase2,\n",
    "# prompter=prompter,\n",
    "# library=library,\n",
    "# verbose=verbose,\n",
    "# n_workers=n_workers,\n",
    "# timeout=timeout,\n",
    "# log_dir=\"logs_images\"#TODO change log dir\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a645f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG find_similar ===\n",
      "Number of train examples: 4\n",
      "Number of programs in library: 400\n",
      "min_similarity threshold: 0.1\n",
      "top_k: 5\n",
      "Using 28 parallel workers (timeout: 2s per program)\n",
      "Progress: 50/400 programs (178.6/s, 0.3s elapsed)\n",
      "Progress: 100/400 programs (217.6/s, 0.5s elapsed)\n",
      "Progress: 150/400 programs (237.6/s, 0.6s elapsed)\n",
      "Progress: 200/400 programs (263.8/s, 0.8s elapsed)\n",
      "Progress: 250/400 programs (264.5/s, 0.9s elapsed)\n",
      "Progress: 300/400 programs (270.0/s, 1.1s elapsed)\n",
      "Progress: 350/400 programs (244.2/s, 1.4s elapsed)\n",
      "Progress: 400/400 programs (26.0/s, 15.4s elapsed)\n",
      "\n",
      "Evaluation complete: 400/400 programs in 15.5s\n",
      "\n",
      "=== Execution Statistics ===\n",
      "Programs with errors: 31/400\n",
      "Max similarity achieved: 0.110\n",
      "Average similarity: 0.007\n",
      "\n",
      "Sample errors from first 3 programs:\n",
      "  228f6490: StopIteration: \n",
      "  272f95fa: StopIteration: \n",
      "  3de23699: StopIteration: \n",
      "\n",
      "Top 5 programs by similarity:\n",
      "  1cf80156: 0.110 (errors: 0/4)\n",
      "  ce4f8723: 0.103 (errors: 0/4)\n",
      "  f25fbde4: 0.101 (errors: 0/4)\n",
      "  28bf18c6: 0.088 (errors: 0/4)\n",
      "  1c786137: 0.083 (errors: 0/4)\n",
      "\n",
      "=== Results ===\n",
      "Programs with perfect examples: 0\n",
      "Other programs above threshold: 3\n",
      "Returning 3 programs\n",
      "Top program: 1cf80156 (sim: 0.110)\n",
      "Second program: ce4f8723 (sim: 0.103)\n"
     ]
    }
   ],
   "source": [
    "similar_programs = library.find_similar(\n",
    "    train_examples=task['train'],\n",
    "    top_k=5,\n",
    "    min_similarity=0.1,\n",
    "    n_workers=n_workers,\n",
    "    timeout=timeout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8823479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Best library match: Task 1cf80156 (0.11)\n"
     ]
    }
   ],
   "source": [
    "best_library_score = 0.0\n",
    "best_library_program = None\n",
    "\n",
    "if similar_programs:\n",
    "    best_match = similar_programs[0]\n",
    "    best_library_score = best_match['similarity']\n",
    "    best_library_program = best_match['program']\n",
    "    if verbose:\n",
    "        print(f\"\\nâœ“ Best library match: Task {best_match['task_id']} ({best_library_score:.2f})\", flush=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d5eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_prompt = prompter.build_phase1_prompt(task, similar_programs)\n",
    "phase1_output = vlm_client_phase1.query(\n",
    "    phase1_prompt,\n",
    "    system_prompt=\"\"\"\"You are an expert at analyzing ARC puzzles and discovering transformation patterns.\n",
    "\n",
    "Remember: Your first hypothesis is sticky and excessively convincing to you.\n",
    "Combat this by evolving your hypothesis and actively seeking evidence against your initial guess to avoid halluciantion.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d2dd2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Phase 2 complete (1337 chars)\n"
     ]
    }
   ],
   "source": [
    "phase2_prompt = prompter.build_phase2_prompt(task, \n",
    "    phase1_output,\n",
    "    similar_programs\n",
    ")\n",
    "\n",
    "phase2_output = vlm_client_phase2.query(\n",
    "    phase2_prompt,\n",
    "    system_prompt=\"You are an expert at generating code using the given DSL primitives to solve ARC puzzles. You are provided with a natural language description of the pattern to implement, as well as training examples and some similar programs you might find useful as reference. Generate a Python function `def solve(I):` that implements the described transformation using ONLY the provided DSL primitives. Ensure your code is syntactically correct and follows best practices.\"\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    print(f\"   âœ“ Phase 2 complete ({len(phase2_output)} chars)\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e202534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated score: 0.13\n"
     ]
    }
   ],
   "source": [
    "generated_code = extract_code_from_response(phase2_output)\n",
    "score, results = test_program(generated_code, task)\n",
    "print(f\"   Generated score: {score:.2f}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd438358",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m sglang.launch_server --model-path /home/flowers/work/hf/Qwen3-4B-Instruct-2507 --tp 1 --port 8000 --mem-fraction-static 0.9 --random-seed 42 --host 0.0.0.0 --log-level info --trust-remote-code --quantization fp8 --context-length 32000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_directory(\n",
    "    data_dir='data_v1/eval_size_10',\n",
    "    vlm_client_phase1=vlm_client_phase1,\n",
    "    vlm_client_phase2=vlm_client_phase2,\n",
    "    prompter=prompter,\n",
    "    library=library,\n",
    "    verbose=True,\n",
    "    n_workers=None,  # Auto-detect CPUs (recommended)\n",
    "    timeout=2        # 2 second timeout per program\n",
    ")\n",
    "\n",
    "# save_results(results, output_dir='results/images')#TODO change output dir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
