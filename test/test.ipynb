{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628462c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main pipeline for ARC task solving using execution-based similarity.\n",
    "\n",
    "Pipeline: Program Similarity â†’ Pattern Discovery â†’ Code Generation\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from src.llm_client import LLMArguments, LLMClient\n",
    "# print(\"Script starting...\", flush=True)\n",
    "# sys.stdout.flush()\n",
    "\n",
    "from src.vlm_prompter import VLMPrompter\n",
    "from src.vlm_client import VLMConfig, create_client, BaseVLMClient\n",
    "from src.utils.library import ProgramLibrary, calculate_grid_similarity\n",
    "from src.utils.dsl import *\n",
    "from src.utils.constants import *\n",
    "# from utils.render_legacy import grid_to_base64_png_oai_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff3312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m sglang.launch_server --model-path /home/flowers/work/hf/Qwen3-4B-Instruct-2507 --tp 1 --port 30014 --mem-fraction-static 0.9 --random-seed 42 --host 0.0.0.0 --log-level info --trust-remote-code --quantization fp8 --context-length 50000 \n",
      "check server run 0\n",
      "[2025-11-07 16:08:35] WARNING server_args.py:1177: Attention backend not explicitly specified. Use flashinfer backend by default.\n",
      "[2025-11-07 16:08:35] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n",
      "[2025-11-07 16:08:35] server_args=ServerArgs(model_path='/home/flowers/work/hf/Qwen3-4B-Instruct-2507', tokenizer_path='/home/flowers/work/hf/Qwen3-4B-Instruct-2507', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=50000, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=30014, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=42, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/home/flowers/work/hf/Qwen3-4B-Instruct-2507', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='flashinfer', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=[1, 2, 4, 8], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)\n",
      "[2025-11-07 16:08:35] Using default HuggingFace chat template with detected content format: string\n",
      "[2025-11-07 16:08:39] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n",
      "[2025-11-07 16:08:39] INFO trace.py:52: opentelemetry package is not installed, tracing disabled\n",
      "[2025-11-07 16:08:39] Init torch distributed begin.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[2025-11-07 16:08:39] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-11-07 16:08:39] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected\n",
      "[2025-11-07 16:08:40] Load weight begin. avail mem=15.32 GB\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.59it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.49it/s]\n",
      "\n",
      "[2025-11-07 16:08:41] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=10.68 GB, mem usage=4.63 GB.\n",
      "[2025-11-07 16:08:41] Using KV cache dtype: torch.bfloat16\n",
      "[2025-11-07 16:08:41] KV Cache is allocated. #tokens: 66634, K size: 4.58 GB, V size: 4.58 GB\n",
      "[2025-11-07 16:08:41] Memory pool end. avail mem=1.02 GB\n",
      "[2025-11-07 16:08:41] Capture cuda graph begin. This can take up to several minutes. avail mem=0.40 GB\n",
      "[2025-11-07 16:08:41] Capture cuda graph bs [1, 2, 4, 8]\n",
      "Capturing batches (bs=1 avail_mem=0.31 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 14.90it/s]\n",
      "[2025-11-07 16:08:42] Capture cuda graph end. Time elapsed: 0.53 s. mem usage=0.11 GB. avail mem=0.29 GB.\n",
      "[2025-11-07 16:08:42] max_total_num_tokens=66634, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=50000, available_gpu_mem=0.29 GB\n",
      "[2025-11-07 16:08:42] INFO:     Started server process [116233]\n",
      "[2025-11-07 16:08:42] INFO:     Waiting for application startup.\n",
      "[2025-11-07 16:08:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2025-11-07 16:08:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
      "[2025-11-07 16:08:42] INFO:     Application startup complete.\n",
      "[2025-11-07 16:08:42] INFO:     Uvicorn running on http://0.0.0.0:30014 (Press CTRL+C to quit)\n",
      "[2025-11-07 16:08:43] INFO:     127.0.0.1:34952 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-11-07 16:08:43] INFO:     127.0.0.1:34958 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-11-07 16:08:43] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 16:08:43] INFO:     127.0.0.1:34972 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-11-07 16:08:43] The server is fired up and ready to roll!\n",
      "[2025-11-07 16:09:03] INFO:     127.0.0.1:42314 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "model_id_serv /home/flowers/work/hf/Qwen3-4B-Instruct-2507\n",
      "model_path /home/flowers/work/hf/Qwen3-4B-Instruct-2507\n",
      "good_model True\n",
      "is_running True\n",
      " /!\\ Server is running /!\\ \n",
      "[2025-11-07 16:09:03] INFO:     127.0.0.1:42330 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-11-07 16:09:23] INFO:     127.0.0.1:33714 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "model_id_serv /home/flowers/work/hf/Qwen3-4B-Instruct-2507\n",
      "model_path /home/flowers/work/hf/Qwen3-4B-Instruct-2507\n",
      "good_model True\n",
      "is_running True\n",
      "Server is up and running\n"
     ]
    }
   ],
   "source": [
    "llm_args = LLMArguments()\n",
    "llm_args.temperature = 0.7\n",
    "llm_args.top_k = 20\n",
    "llm_args.top_p = 0.8\n",
    "llm_client = LLMClient(llm_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea392b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send 1 / 1 messages\n",
      "[2025-11-07 16:10:07] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 16:10:07] INFO:     127.0.0.1:50668 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"Hello! ðŸ‘‹ It's great to meet you. How can I assist you today? ðŸ˜Š\"]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_client.generate([\"Hello, world!\"])  # Warm up the LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae120cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskResult:\n",
    "    \"\"\"Result of attempting to solve a task\"\"\"\n",
    "    task_id: str\n",
    "    success: bool\n",
    "    score: float\n",
    "    program: Optional[str] = None\n",
    "    phase1_output: Optional[str] = None\n",
    "    phase2_output: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "def test_program(program_code: str, task: Dict) -> Tuple[float, List[Tuple[Any, Any, bool]]]:\n",
    "    \"\"\"\n",
    "    Test a program against task training examples.\n",
    "    \n",
    "    Returns:\n",
    "        - Average score across examples\n",
    "        - List of (expected_output, actual_output, passed) tuples\n",
    "    \"\"\"\n",
    "    namespace = globals().copy()\n",
    "    \n",
    "    try:\n",
    "        exec(program_code, namespace)\n",
    "        \n",
    "        if 'solve' not in namespace:\n",
    "            return 0.0, []\n",
    "        \n",
    "        solve_fn = namespace['solve']\n",
    "        scores = []\n",
    "        results = []\n",
    "        \n",
    "        for example in task['test']:\n",
    "            inp = example['input']\n",
    "            expected = example['output']\n",
    "            if isinstance(inp, list):\n",
    "                inp = tuple(tuple(row) for row in inp)\n",
    "            try:\n",
    "                actual = solve_fn(inp)\n",
    "                score = calculate_grid_similarity(actual, expected)\n",
    "                scores.append(score)\n",
    "                results.append((expected, actual, score == 1.0))\n",
    "            except Exception as e:\n",
    "                scores.append(0.0)\n",
    "                results.append((expected, None, False))\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "        return avg_score, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0, []\n",
    "\n",
    "\n",
    "def extract_code_from_response(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract Python code from LLM response.\"\"\"\n",
    "    try:\n",
    "        python_blocks = re.findall(r'```python\\n(.*?)```', response, re.DOTALL)\n",
    "        \n",
    "        if python_blocks:\n",
    "            for block in python_blocks:\n",
    "                if 'def solve' in block:\n",
    "                    return block.strip()\n",
    "            return python_blocks[0].strip()\n",
    "        \n",
    "        match = re.search(r'(def solve\\(I\\):.*?)(?=\\n\\ndef|\\n\\nif __name__|$)', response, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def get_similar_programs(library: ProgramLibrary,\n",
    "    task: Dict,\n",
    "    n_workers: int = None,\n",
    "    timeout: int = 2\n",
    ") -> List[Dict]:\n",
    "        \n",
    "    similar_programs = library.find_similar(\n",
    "        train_examples=task['train'],\n",
    "        top_k=5,\n",
    "        min_similarity=0.1,\n",
    "        n_workers=n_workers,\n",
    "        timeout=timeout\n",
    "    )\n",
    "    return similar_programs\n",
    "    \n",
    "def get_prompt_1(\n",
    "    task: Dict,\n",
    "    task_id: str,\n",
    "    prompter: VLMPrompter,\n",
    "    similar_programs: List[Dict] = None,\n",
    "    verbose: bool = True,\n",
    "    log_dir: str = \"logs\"\n",
    ") -> str:\n",
    "    \n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\", flush=True)\n",
    "        print(f\"Solving Task: {task_id}\", flush=True)\n",
    "        print(f\"{'='*80}\", flush=True)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # STEP 1: Find Similar Programs by Execution (PARALLELIZED)\n",
    "    # ====================================================================\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ” Finding similar programs by execution...\", flush=True)\n",
    "    \n",
    "\n",
    "    if verbose:\n",
    "        if similar_programs:\n",
    "            print(f\"   Found {len(similar_programs)} similar programs:\", flush=True)\n",
    "            for i, prog in enumerate(similar_programs[:3], 1):\n",
    "                print(f\"   {i}. Task {prog['task_id']}: {prog['similarity']:.2f}\", flush=True)\n",
    "        else:\n",
    "            print(\"   No similar programs found\", flush=True)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # TEST LIBRARY PROGRAMS: Try existing solutions first\n",
    "    # ====================================================================\n",
    "    best_library_score = 0.0\n",
    "    best_library_program = None\n",
    "    \n",
    "    if similar_programs:\n",
    "        best_match = similar_programs[0]\n",
    "        best_library_score = best_match['similarity']\n",
    "        best_library_program = best_match['program']\n",
    "        if verbose:\n",
    "            print(f\"\\nâœ“ Best library match: Task {best_match['task_id']} ({best_library_score:.2f})\", flush=True)\n",
    "        \n",
    "        if best_library_score == 1.0:\n",
    "            if verbose:\n",
    "                print(f\"   â†’ Perfect match found! Using library solution.\", flush=True)\n",
    "            return TaskResult(\n",
    "                task_id=task_id,\n",
    "                success=True,\n",
    "                score=1.0,\n",
    "                program=best_library_program\n",
    "            )\n",
    "            \n",
    "    # ====================================================================\n",
    "    # PHASE 1: Pattern Discovery (Natural Language)\n",
    "    # ====================================================================\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ“ Phase 1: Pattern Discovery...\", flush=True)\n",
    "    \n",
    "    # Pass the raw similar_programs list - prompter will format it\n",
    "    phase1_prompt = prompter.build_phase1_prompt(task, similar_programs)\n",
    "    return phase1_prompt\n",
    "\n",
    "\n",
    "def get_prompt_2(\n",
    "    phase1_output: str,\n",
    "    similar_programs: List[Dict],\n",
    "    task: Dict,\n",
    "    task_id: str,\n",
    "    prompter: VLMPrompter,\n",
    "    verbose: bool = True,\n",
    "    log_dir: str = \"logs\",\n",
    "    \n",
    ") -> str:\n",
    "        \n",
    "        # LOG PHASE 1 OUTPUT\n",
    "        phase1_log_path = os.path.join(log_dir, f\"{task_id}_phase1_output.txt\")\n",
    "        with open(phase1_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Task ID: {task_id}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"PHASE 1: PATTERN DISCOVERY OUTPUT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            f.write(phase1_output)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   âœ“ Phase 1 complete ({len(phase1_output)} chars)\", flush=True)\n",
    "            print(f\"   ðŸ“„ Logged to: {phase1_log_path}\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 2: Code Generation\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nâš™ï¸  Phase 2: Code Generation...\", flush=True)\n",
    "        \n",
    "        # Pass the raw similar_programs list - prompter will format it\n",
    "        phase2_prompt = prompter.build_phase2_prompt(task, \n",
    "            phase1_output,\n",
    "            similar_programs\n",
    "        )\n",
    "        \n",
    "        return phase2_prompt\n",
    "def get_prompt_str(phase_prompt):\n",
    "    prompt_str = \"\"\n",
    "    for i in phase_prompt:\n",
    "        prompt_str += i[\"text\"]\n",
    "    return prompt_str\n",
    "\n",
    "def solve_task(\n",
    "    task: Dict,\n",
    "    task_id: str,\n",
    "    vlm_client_phase1: BaseVLMClient,\n",
    "    vlm_client_phase2: BaseVLMClient,\n",
    "    prompter: VLMPrompter,\n",
    "    library: ProgramLibrary,\n",
    "    verbose: bool = True,\n",
    "    n_workers: int = None,\n",
    "    timeout: int = 2,\n",
    "    log_dir: str = \"logs\"\n",
    ") -> TaskResult:\n",
    "    \"\"\"\n",
    "    Solve a single ARC task using execution-based pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Find similar programs by execution (parallelized)\n",
    "    2. Phase 1: Pattern discovery (natural language) with similar programs\n",
    "    3. Phase 2: Code generation with pattern + similar programs\n",
    "    \n",
    "    Args:\n",
    "        task: Task dictionary with 'train' examples\n",
    "        task_id: Unique task identifier\n",
    "        vlm_client: VLM client for queries\n",
    "        prompter: Prompt builder\n",
    "        library: Program library\n",
    "        verbose: Print progress\n",
    "        n_workers: Number of parallel workers (None = auto)\n",
    "        timeout: Timeout per program execution in seconds\n",
    "        log_dir: Directory to save logs (default: \"logs\")\n",
    "    \"\"\"\n",
    "    # Create log directory if it doesn't exist\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\", flush=True)\n",
    "        print(f\"Solving Task: {task_id}\", flush=True)\n",
    "        print(f\"{'='*80}\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # STEP 1: Find Similar Programs by Execution (PARALLELIZED)\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nðŸ” Finding similar programs by execution...\", flush=True)\n",
    "        \n",
    "        similar_programs = library.find_similar(\n",
    "            train_examples=task['train'],\n",
    "            top_k=5,\n",
    "            min_similarity=0.1,\n",
    "            n_workers=n_workers,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            if similar_programs:\n",
    "                print(f\"   Found {len(similar_programs)} similar programs:\", flush=True)\n",
    "                for i, prog in enumerate(similar_programs[:3], 1):\n",
    "                    print(f\"   {i}. Task {prog['task_id']}: {prog['similarity']:.2f}\", flush=True)\n",
    "            else:\n",
    "                print(\"   No similar programs found\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TEST LIBRARY PROGRAMS: Try existing solutions first\n",
    "        # ====================================================================\n",
    "        best_library_score = 0.0\n",
    "        best_library_program = None\n",
    "        \n",
    "        if similar_programs:\n",
    "            best_match = similar_programs[0]\n",
    "            best_library_score = best_match['similarity']\n",
    "            best_library_program = best_match['program']\n",
    "            if verbose:\n",
    "                print(f\"\\nâœ“ Best library match: Task {best_match['task_id']} ({best_library_score:.2f})\", flush=True)\n",
    "            \n",
    "            if best_library_score == 1.0:\n",
    "                if verbose:\n",
    "                    print(f\"   â†’ Perfect match found! Using library solution.\", flush=True)\n",
    "                return TaskResult(\n",
    "                    task_id=task_id,\n",
    "                    success=True,\n",
    "                    score=1.0,\n",
    "                    program=best_library_program\n",
    "                )\n",
    "                \n",
    "        # ====================================================================\n",
    "        # PHASE 1: Pattern Discovery (Natural Language)\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nðŸ“ Phase 1: Pattern Discovery...\", flush=True)\n",
    "        \n",
    "        # Pass the raw similar_programs list - prompter will format it\n",
    "        phase1_prompt = prompter.build_phase1_prompt(task, similar_programs)\n",
    "        \n",
    "        phase1_output = vlm_client_phase1.query(\n",
    "            phase1_prompt,\n",
    "            system_prompt=\"\"\"\"You are an expert at analyzing ARC puzzles and discovering transformation patterns.\n",
    "\n",
    "Remember: Your first hypothesis is sticky and excessively convincing to you.\n",
    "Combat this by evolving your hypothesis and actively seeking evidence against your initial guess to avoid halluciantion.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # LOG PHASE 1 OUTPUT\n",
    "        phase1_log_path = os.path.join(log_dir, f\"{task_id}_phase1_output.txt\")\n",
    "        with open(phase1_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Task ID: {task_id}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"PHASE 1: PATTERN DISCOVERY OUTPUT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            f.write(phase1_output)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   âœ“ Phase 1 complete ({len(phase1_output)} chars)\", flush=True)\n",
    "            print(f\"   ðŸ“„ Logged to: {phase1_log_path}\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 2: Code Generation\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nâš™ï¸  Phase 2: Code Generation...\", flush=True)\n",
    "        \n",
    "        # Pass the raw similar_programs list - prompter will format it\n",
    "        phase2_prompt = prompter.build_phase2_prompt(task, \n",
    "            phase1_output,\n",
    "            similar_programs\n",
    "        )\n",
    "        \n",
    "        phase2_output = vlm_client_phase2.query(\n",
    "            phase2_prompt,\n",
    "            system_prompt=\"You are an expert at generating code using the given DSL primitives to solve ARC puzzles. You are provided with a natural language description of the pattern to implement, as well as training examples and some similar programs you might find useful as reference. Generate a Python function `def solve(I):` that implements the described transformation using ONLY the provided DSL primitives. Ensure your code is syntactically correct and follows best practices.\"\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   âœ“ Phase 2 complete ({len(phase2_output)} chars)\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # EXTRACT AND TEST GENERATED CODE\n",
    "        # ====================================================================\n",
    "        if verbose:\n",
    "            print(\"\\nðŸ§ª Testing generated program...\", flush=True)\n",
    "        \n",
    "        generated_code = extract_code_from_response(phase2_output)\n",
    "        \n",
    "        if not generated_code:\n",
    "            if verbose:\n",
    "                print(\"   âœ— Failed to extract code\", flush=True)\n",
    "            \n",
    "            if best_library_program and best_library_score > 0.5:\n",
    "                if verbose:\n",
    "                    print(f\"   â†’ Falling back to library (score: {best_library_score:.2f})\", flush=True)\n",
    "                return TaskResult(\n",
    "                    task_id=task_id,\n",
    "                    success=False,\n",
    "                    score=best_library_score,\n",
    "                    program=best_library_program,\n",
    "                    phase1_output=phase1_output,\n",
    "                    error=\"Code extraction failed, using library fallback\"\n",
    "                )\n",
    "            \n",
    "            return TaskResult(\n",
    "                task_id=task_id,\n",
    "                success=False,\n",
    "                score=0.0,\n",
    "                phase1_output=phase1_output,\n",
    "                error=\"Failed to extract code from response\"\n",
    "            )\n",
    "        \n",
    "        score, results = test_program(generated_code, task)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Generated score: {score:.2f}\", flush=True)\n",
    "        \n",
    "        # LOG PHASE 2 OUTPUT WITH TEST RESULTS\n",
    "        phase2_log_path = os.path.join(log_dir, f\"{task_id}_phase2_results.txt\")\n",
    "        with open(phase2_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Task ID: {task_id}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"PHASE 2: CODE GENERATION & TEST RESULTS\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"GENERATED CODE:\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(generated_code + \"\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"SCORE: {score:.2f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"TEST RESULTS:\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            for i, (expected, actual, passed) in enumerate(results, 1):\n",
    "                f.write(f\"\\nExample {i}: {'âœ“ PASS' if passed else 'âœ— FAIL'}\\n\")\n",
    "                f.write(f\"Expected Output:\\n\")\n",
    "                if expected:\n",
    "                    f.write(f\"{json.dumps([list(row) for row in expected], indent=2)}\\n\")\n",
    "                else:\n",
    "                    f.write(\"None\\n\")\n",
    "                f.write(f\"Actual Output:\\n\")\n",
    "                if actual:\n",
    "                    f.write(f\"{json.dumps([list(row) for row in actual], indent=2)}\\n\")\n",
    "                else:\n",
    "                    f.write(\"None (execution failed)\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   ðŸ“„ Logged to: {phase2_log_path}\", flush=True)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # DECIDE FINAL PROGRAM\n",
    "        # ====================================================================\n",
    "        success = score == 1.0\n",
    "        final_program = generated_code\n",
    "        final_score = score\n",
    "        \n",
    "        if not success and best_library_score > score:\n",
    "            if verbose:\n",
    "                print(f\"   â†’ Library program better ({best_library_score:.2f} > {score:.2f})\", flush=True)\n",
    "            final_program = best_library_program\n",
    "            final_score = best_library_score\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SAVE TO LIBRARY IF SUCCESSFUL\n",
    "        # ====================================================================\n",
    "        if success:\n",
    "            namespace = globals().copy()\n",
    "            exec(final_program, namespace)\n",
    "            if 'solve' in namespace:\n",
    "                library.add(task_id, final_program)\n",
    "                if verbose:\n",
    "                    print(f\"   âœ“ Added to library\", flush=True)\n",
    "        \n",
    "        return TaskResult(\n",
    "            task_id=task_id,\n",
    "            success=success,\n",
    "            score=final_score,\n",
    "            program=final_program,\n",
    "            phase1_output=phase1_output\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"   âœ— Error: {e}\", flush=True)\n",
    "        \n",
    "        return TaskResult(\n",
    "            task_id=task_id,\n",
    "            success=False,\n",
    "            score=0.0,\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "def process_directory(\n",
    "    data_dir: str,\n",
    "    llm_client: LLMClient,\n",
    "    prompter: VLMPrompter,\n",
    "    library: ProgramLibrary,\n",
    "    verbose: bool = True,\n",
    "    n_workers: int = None,\n",
    "    timeout: int = 2,\n",
    "    evaluate_on_n_pb: int = -1,\n",
    ") -> List[TaskResult]:\n",
    "    \"\"\"\n",
    "    Process all task files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing task JSON files\n",
    "        vlm_client: VLM client for queries\n",
    "        prompter: Prompt builder\n",
    "        library: Program library\n",
    "        verbose: Print progress\n",
    "        n_workers: Number of parallel workers for library search (None = auto)\n",
    "        timeout: Timeout per program execution in seconds\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Error: Directory not found: {data_dir}\", flush=True)\n",
    "        return []\n",
    "    \n",
    "    json_files = sorted(data_path.glob('*.json'))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {data_dir}\", flush=True)\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nFound {len(json_files)} tasks in {data_dir}\\n\", flush=True)\n",
    "    if evaluate_on_n_pb != -1:\n",
    "        json_files = json_files[:evaluate_on_n_pb]\n",
    "        print (f\"Evaluating on first {evaluate_on_n_pb} tasks\\n\", flush=True)\n",
    "    results = []\n",
    "    successful = 0\n",
    "    total_score = 0.0\n",
    "    list_idx = []\n",
    "    list_task_id = []\n",
    "    list_task = []\n",
    "    list_prompt_1 = []\n",
    "    \n",
    "    list_similar_programs = []\n",
    "    for i, task_file in enumerate(json_files, 1):\n",
    "        task_id = task_file.stem\n",
    "        \n",
    "        try:\n",
    "            with open(task_file, 'r') as f:\n",
    "                task = json.load(f)\n",
    "\n",
    "            similar_programs = get_similar_programs(\n",
    "                library,\n",
    "                task,\n",
    "                n_workers=n_workers,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            list_similar_programs.append(similar_programs)\n",
    "            list_idx.append(i)\n",
    "            list_task_id.append(task_id)\n",
    "            list_task.append(task)\n",
    "\n",
    "            list_prompt_1.append(get_prompt_str(get_prompt_1(\n",
    "                task,\n",
    "                task_id,\n",
    "                prompter,\n",
    "                n_workers=n_workers,\n",
    "                timeout=timeout,\n",
    "                verbose=verbose\n",
    "            )))\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âœ— [{i}/{len(json_files)}] {task_id}: Invalid JSON - {e}\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— [{i}/{len(json_files)}] {task_id}: {e}\", flush=True)\n",
    "    \n",
    "\n",
    "    out_1 = llm_client.generate(list_prompt_1)\n",
    "    out_1 = [resp[0] for resp in out_1]\n",
    "    list_prompt_2 = []\n",
    "    for i, task_id, task, phase1_output, similar_programs in zip(list_idx, list_task_id, list_task, out_1, list_similar_programs):\n",
    "        phase2_prompt = get_prompt_str(get_prompt_2(\n",
    "            phase1_output,\n",
    "            similar_programs,\n",
    "            task,\n",
    "            task_id,\n",
    "            prompter,\n",
    "            verbose=verbose\n",
    "        ))\n",
    "        list_prompt_2.append(phase2_prompt)\n",
    "\n",
    "    out_2 = llm_client.generate(list_prompt_2)\n",
    "    out_2 = [resp[0] for resp in out_2]\n",
    "    list_generated_code = [extract_code_from_response(resp) for resp in out_2]\n",
    "    list_results = []\n",
    "    for task_id, task, generated_code, phase1_output, phase2_output in zip(list_task_id, list_task, list_generated_code, out_1, out_2):\n",
    "        if not generated_code:\n",
    "            return TaskResult(\n",
    "                task_id=task_id,\n",
    "                success=False,\n",
    "                score=0.0,\n",
    "                phase1_output=phase1_output,\n",
    "                phase2_output=phase2_output,\n",
    "                error=\"Failed to extract code from response\"\n",
    "            )\n",
    "        try:\n",
    "            score, results = test_program(generated_code, task)\n",
    "\n",
    "            success = score == 1.0\n",
    "            result = TaskResult(\n",
    "                task_id=task_id,\n",
    "                success=success,\n",
    "                score=score,\n",
    "                program=generated_code,\n",
    "                phase1_output=phase1_output,\n",
    "                phase2_output=phase2_output\n",
    "            )\n",
    "        except Exception as e:\n",
    "            result = TaskResult(\n",
    "                task_id=task_id,\n",
    "                success=False,\n",
    "                score=0.0,\n",
    "                phase1_output=phase1_output,\n",
    "                phase2_output=phase2_output,\n",
    "                error=str(e)\n",
    "            )\n",
    "        results.append(result)\n",
    "        if result.success:\n",
    "            successful += 1\n",
    "        \n",
    "        total_score += result.score\n",
    "        \n",
    "        status = \"âœ“\" if result.success else \"âœ—\"\n",
    "        print(f\"{status} [{i}/{len(json_files)}] {task_id}: {result.score:.2f}\", flush=True)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\", flush=True)\n",
    "    print(f\"SUMMARY\", flush=True)\n",
    "    print(f\"{'='*80}\", flush=True)\n",
    "    print(f\"Total tasks: {len(json_files)}\", flush=True)\n",
    "    print(f\"Successful: {successful}/{len(json_files)} ({100*successful/len(json_files):.1f}%)\", flush=True)\n",
    "    print(f\"Average score: {total_score/len(json_files):.2f}\", flush=True)\n",
    "    print(f\"{'='*80}\\n\", flush=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results: List[TaskResult], output_dir: str = 'results') -> None:\n",
    "    \"\"\"Save results to JSON and CSV files.\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # JSON\n",
    "    json_file = output_path / 'results.json'\n",
    "    with open(json_file, 'w') as f:\n",
    "        json_data = [\n",
    "            {\n",
    "                'task_id': r.task_id,\n",
    "                'success': r.success,\n",
    "                'score': r.score,\n",
    "                'error': r.error,\n",
    "                'program': r.program,\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    print(f\"Saved detailed results to {json_file}\", flush=True)\n",
    "    \n",
    "    # CSV\n",
    "    csv_file = output_path / 'summary.csv'\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['task_id', 'success', 'score', 'error'])\n",
    "        for r in results:\n",
    "            writer.writerow([r.task_id, r.success, f'{r.score:.2f}', r.error or ''])\n",
    "    print(f\"Saved summary to {csv_file}\", flush=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    from dotenv import load_dotenv\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    # print(\"Initializing components...\", flush=True)\n",
    "    load_dotenv()\n",
    "    PROVIDER = \"grok\"\n",
    "    if PROVIDER == \"grok\":\n",
    "        api_key = os.getenv('GROK_API_KEY')\n",
    "        api_base = \"https://api.x.ai/v1\"\n",
    "        model = \"grok-4-fast\"\n",
    "    elif PROVIDER == \"qwen\":\n",
    "        api_key = None\n",
    "        api_base = \"http://localhost:8000/v1\"\n",
    "        model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    elif PROVIDER == \"gemini\":\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        api_base = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
    "        model = \"gemini-2.5-pro\"\n",
    "        \n",
    "    vlm_config_phase1 = VLMConfig(\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        api_base=api_base,\n",
    "        max_tokens=16384,  # Longer for analysis\n",
    "        save_prompts=False,\n",
    "        prompt_log_dir=\"prompts_testing\"\n",
    "    )\n",
    "    vlm_config_phase2 = VLMConfig(\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        api_base=api_base,\n",
    "        max_tokens=8192   # Shorter for code gen\n",
    "    )\n",
    "    \n",
    "    vlm_client_phase1 = create_client(PROVIDER, config=vlm_config_phase1)\n",
    "    # print(\"VLM client created\", flush=True)\n",
    "    \n",
    "    vlm_client_phase2 = create_client(PROVIDER, config=vlm_config_phase2)\n",
    "    prompter = VLMPrompter()\n",
    "    # print(\"Prompter created\", flush=True)\n",
    "    \n",
    "    library = ProgramLibrary()  # Auto-loads from solvers.py\n",
    "    # print(\"Loaded library...\", flush=True)\n",
    "    #sanity check\n",
    "    print(f\"Loaded {len(library)} programs from library\", flush=True)\n",
    "    if len(library) > 0:\n",
    "        print(f\"First program: {library.programs[0]['task_id']}\", flush=True)\n",
    "    \n",
    "    # Configure parallelization\n",
    "    results = process_directory(\n",
    "        data_dir='data_v1/eval_size_10',\n",
    "        vlm_client_phase1=vlm_client_phase1,\n",
    "        vlm_client_phase2=vlm_client_phase2,\n",
    "        prompter=prompter,\n",
    "        library=library,\n",
    "        verbose=True,\n",
    "        n_workers=None,  # Auto-detect CPUs (recommended)\n",
    "        timeout=2        # 2 second timeout per program\n",
    "    )\n",
    "    \n",
    "    save_results(results, output_dir='results/images')#TODO change output dir\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # print(\"Starting main...\", flush=True)\n",
    "#     sys.stdout.flush()\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0e47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = None\n",
    "api_base = \"http://localhost:8000/v1\"\n",
    "model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "PROVIDER = \"qwen\"\n",
    "vlm_config_phase1 = VLMConfig(\n",
    "    api_key=api_key,\n",
    "    model=model,\n",
    "    api_base=api_base,\n",
    "    max_tokens=16384,  # Longer for analysis\n",
    "    save_prompts=False,\n",
    "    prompt_log_dir=\"prompts_testing\"\n",
    ")\n",
    "vlm_config_phase2 = VLMConfig(\n",
    "    api_key=api_key,\n",
    "    model=model,\n",
    "    api_base=api_base,\n",
    "    max_tokens=8192   # Shorter for code gen\n",
    ")\n",
    "vlm_client_phase1 = create_client(PROVIDER, config=vlm_config_phase1)\n",
    "# print(\"VLM client created\", flush=True)\n",
    "\n",
    "vlm_client_phase2 = create_client(PROVIDER, config=vlm_config_phase2)\n",
    "prompter = VLMPrompter(use_vision=False)\n",
    "\n",
    "library = ProgramLibrary()  # Auto-loads from solvers.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168172a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## DSL Primitives\n",
      "\n",
      "**Type Definitions:**\n",
      "Grid: Tuple[Tuple[int, ...], ...] - Immutable 2D array (tuple of tuples)\n",
      "Object: FrozenSet[(int, (int, int))] - Set of (color, location) pairs\n",
      "Patch: FrozenSet[(int, int)] or Object - Set of indices or colored object\n",
      "Indices: FrozenSet[(int, int)] - Set of (row, col) positions\n",
      "Objects: FrozenSet[Object] - Set of objects\n",
      "Container: Tuple or FrozenSet - Generic container type\n",
      "IntegerTuple: (int, int) - Tuple of integers (usually coordinates or dimensions)\n",
      "Callable: Function type\n",
      "\n",
      "**Functional Programming:**\n",
      "```python\n",
      "compose(outer, inner) -> Callable                    # outer(inner(x))\n",
      "chain(h, g, f) -> Callable                           # h(g(f(x)))\n",
      "combine_two_function_results(outer, f1, f2) -> Callable  # outer(f1(x), f2(x))\n",
      "transform(f, container) -> Container                 # map\n",
      "transform_and_flatten(f, container) -> FrozenSet     # map + flatten\n",
      "transform_both(f, a, b) -> Tuple                     # pairwise map over tuples\n",
      "transform_both_and_flatten(f, a, b) -> Tuple         # pairwise map + flatten\n",
      "apply_each_function(funcs, value) -> Container       # apply each function to the same value\n",
      "apply_function_on_cartesian_product(f, a, b) -> FrozenSet\n",
      "fix_first_argument(f, arg) -> Callable                # left partial\n",
      "fix_last_argument(f, arg) -> Callable                 # right partial\n",
      "equals(f, target) -> Callable                         # x -> f(x) == target\n",
      "extract_first_matching(container, pred) -> Any        # first element satisfying pred\n",
      "power(f, n) -> Callable                               # f applied n times\n",
      "identity(x) -> Any                                    # returns x unchanged\n",
      "```\n",
      "\n",
      "**Grid Transforms:**\n",
      "```python\n",
      "horizontal_mirror(grid) -> Grid/Patch                # flip up-down\n",
      "vertical_mirror(grid) -> Grid/Patch                  # flip left-right\n",
      "diagonal_mirror(grid) -> Grid/Patch                  # diagonal \\ mirror\n",
      "counterdiagonal_mirror(grid) -> Grid/Patch           # diagonal / mirror\n",
      "rot90(grid) -> Grid                                  # rotate 90Â° clockwise\n",
      "rot180(grid) -> Grid                                 # rotate 180Â°\n",
      "rot270(grid) -> Grid                                 # rotate 270Â° clockwise\n",
      "vertical_concat(a, b) -> Grid                        # stack vertically [a; b]\n",
      "horizontal_concat(a, b) -> Grid                      # stack horizontally [a, b]\n",
      "crop(grid, (i,j), (h,w)) -> Grid                     # extract subgrid\n",
      "upscale(element, n) -> Grid/Object                   # enlarge by factor n\n",
      "downscale(grid, n) -> Grid                           # shrink by factor n\n",
      "horizontal_upscale(grid, n) -> Grid                  # widen by n\n",
      "vertical_upscale(grid, n) -> Grid                    # height by n\n",
      "horizontal_split(grid, n) -> Tuple                   # split into n vertical slabs\n",
      "vertical_split(grid, n) -> Tuple                     # split into n horizontal slabs\n",
      "top_half/bottom_half/left_half/right_half -> Grid    # halves\n",
      "trim_border(grid) -> Grid                            # remove 1-cell border\n",
      "smallest_subgrid_containing(patch, grid) -> Grid     # tight crop around patch\n",
      "```\n",
      "\n",
      "**Objects & Indices:**\n",
      "```python\n",
      "as_objects(grid, each_object_single_color, include_diagonal_neighbors, discard_background) -> FrozenSet[Object]\n",
      "partition(grid) -> Objects                           # objects by color\n",
      "partition_only_foreground(grid) -> Objects           # objects by color w/o background\n",
      "\n",
      "color_filter(objects, color) -> Objects              # filter by color\n",
      "size_filter(objects, n) -> FrozenSet                 # keep of size n\n",
      "of_color(grid, color) -> Indices                     # indices of a color\n",
      "to_object(patch, grid) -> Object                     # patch -> colored object\n",
      "shift_to_origin(obj) -> Patch                        # move object to (0,0)\n",
      "to_indices(patch_or_obj) -> Indices                  # indices only\n",
      "as_indices(grid) -> Indices                          # all grid indices\n",
      "as_object(grid) -> Object                            # grid -> object\n",
      "recolor(color, patch) -> Object                      # recolor patch\n",
      "shift_by_vector(patch, (di, dj)) -> Patch            # translate\n",
      "paint_onto_grid(grid, obj) -> Grid                   # draw object\n",
      "paint_onto_grid_background(grid, obj) -> Grid        # draw only onto background\n",
      "fill(grid, color, patch) -> Grid                     # color cells in indices/patch\n",
      "erase_patch(grid, patch) -> Grid                     # fill patch with background\n",
      "move_object(grid, obj, offset) -> Grid               # move obj by offset\n",
      "```\n",
      "\n",
      "**Spatial Queries:**\n",
      "```python\n",
      "upper_left_corner/upper_right_corner/lower_left_corner/lower_right_corner(obj) -> IntegerTuple\n",
      "center(patch) -> IntegerTuple\n",
      "corner_indices(patch) -> Indices\n",
      "position(a, b) -> IntegerTuple                       # relative (-1/0/1, -1/0/1)\n",
      "box(patch) -> Indices                                # bounding-box outline\n",
      "inbox(patch) -> Indices                              # outline 1 inside\n",
      "outbox(patch) -> Indices                             # outline 1 outside\n",
      "bounding_box_indices(patch) -> Indices               # all indices in box\n",
      "bounding_box_delta(patch) -> Indices                 # box minus patch\n",
      "vertical_line(loc) -> Indices                        # vertical line through loc\n",
      "horizontal_line(loc) -> Indices                      # horizontal line through loc\n",
      "shoot(start, direction) -> Indices                   # ray from point in direction\n",
      "line_between(a, b) -> Indices                        # line between two points\n",
      "adjacent(a, b) -> Boolean                            # patches touch\n",
      "horizontal_matching(a, b) -> Boolean                 # share a row\n",
      "vertical_matching(a, b) -> Boolean                   # share a column\n",
      "manhattan_distance(a, b) -> Integer                  # min Manhattan distance\n",
      "move_until_touching(src, dst) -> IntegerTuple        # offset to touch\n",
      "```\n",
      "\n",
      "**Shape/Geometry Predicates:**\n",
      "```python\n",
      "is_portrait(piece) -> Boolean\n",
      "is_square(piece) -> Boolean\n",
      "is_vertical_line(patch) -> Boolean\n",
      "is_horizontal_line(patch) -> Boolean\n",
      "```\n",
      "\n",
      "**Properties:**\n",
      "```python\n",
      "get_height(grid_or_patch) -> Integer\n",
      "get_width(grid_or_patch) -> Integer\n",
      "get_shape(grid_or_patch) -> IntegerTuple\n",
      "size(container) -> Integer\n",
      "palette(element) -> FrozenSet[int]\n",
      "count_colors(element) -> Integer\n",
      "most_common_color(element) -> Integer\n",
      "least_common_color(element) -> Integer\n",
      "get_color(obj) -> Integer\n",
      "color_at_location(grid, (i,j)) -> Integer/None\n",
      "occurrences(grid, obj) -> Indices\n",
      "cellwise(a, b, fallback) -> Grid\n",
      "horizontal_periodicity(obj) -> Integer\n",
      "vertical_periodicity(obj) -> Integer\n",
      "solid_color_strips_in_grid(grid) -> Objects          # formerly frontiers\n",
      "remove_solid_color_strips_from_grid(grid) -> Grid    # formerly compress\n",
      "```\n",
      "\n",
      "**Set Operations:**\n",
      "```python\n",
      "union(a, b) -> Container                             # concatenate/union\n",
      "intersection(a, b) -> FrozenSet\n",
      "difference(a, b) -> FrozenSet\n",
      "flatten(containers) -> Container                     # flatten nested\n",
      "remove_duplicates(t) -> Tuple\n",
      "keep_if_condition(container, pred) -> Container\n",
      "keep_if_condition_and_flatten(container, pred) -> FrozenSet\n",
      "contains(value, container) -> Boolean\n",
      "initset(value) -> FrozenSet\n",
      "insert(value, frozenset) -> FrozenSet\n",
      "remove(value, container) -> Container\n",
      "to_tuple(frozenset) -> Tuple\n",
      "cartesian_product(a, b) -> FrozenSet\n",
      "pairwise(a, b) -> TupleTuple\n",
      "as_tuple(a, b) -> IntegerTuple\n",
      "as_generic_tuple(a, b) -> Tuple\n",
      "make_cell(color, (i,j)) -> Tuple                     # (color, (i,j))\n",
      "interval(start, stop, step) -> Tuple\n",
      "```\n",
      "\n",
      "**Aggregation:**\n",
      "```python\n",
      "argmax(container, f) -> Any\n",
      "argmin(container, f) -> Any\n",
      "valmax(container, f) -> Integer\n",
      "valmin(container, f) -> Integer\n",
      "maximum(container) -> Integer\n",
      "minimum(container) -> Integer\n",
      "most_common(container) -> Any\n",
      "least_common(container) -> Any\n",
      "sort(container, key_fn) -> Tuple\n",
      "```\n",
      "\n",
      "**Arithmetic & Vectors:**\n",
      "```python\n",
      "add(a, b) -> Numerical\n",
      "subtract(a, b) -> Numerical\n",
      "multiply(a, b) -> Numerical\n",
      "divide(a, b) -> Numerical\n",
      "negate(n) -> Numerical\n",
      "double(n) -> Numerical\n",
      "halve(n) -> Numerical\n",
      "increment(x) -> Numerical\n",
      "decrement(x) -> Numerical\n",
      "sign(x) -> Numerical or IntegerTuple\n",
      "to_vertical_vec(i) -> IntegerTuple\n",
      "to_horizontal_vec(j) -> IntegerTuple\n",
      "```\n",
      "\n",
      "**Boolean:**\n",
      "```python\n",
      "is_equal(a, b) -> Boolean\n",
      "logical_and(a, b) -> Boolean\n",
      "logical_or(a, b) -> Boolean\n",
      "logical_not(b) -> Boolean\n",
      "contains(value, container) -> Boolean\n",
      "is_positive(n) -> Boolean\n",
      "is_even(n) -> Boolean\n",
      "greater_than(a, b) -> Boolean\n",
      "```\n",
      "\n",
      "**Constants:**\n",
      "```python\n",
      "# Colors: ZERO=0, ONE=1, TWO=2, THREE=3, FOUR=4, FIVE=5, SIX=6, SEVEN=7, EIGHT=8, NINE=9\n",
      "# Directions: UP=(-1,0), DOWN=(1,0), LEFT=(0,-1), RIGHT=(0,1)\n",
      "# Diagonals: UNITY=(1,1), NEG_UNITY=(-1,-1), UP_RIGHT=(-1,1), DOWN_LEFT=(1,-1)\n",
      "# Special: ORIGIN=(0,0), True=True, False=False\n",
      "```\n",
      "\n",
      "## Control Flow Allowed\n",
      "- Single-level `for` loops (no nesting)\n",
      "- `if/else` conditionals\n",
      "- List comprehensions\n",
      "\n",
      "## Requirements\n",
      "- Function signature: `def solve(I):`\n",
      "- Input `I` is tuple of tuples of ints (Grid)\n",
      "- Return same format\n",
      "- Use ONLY the DSL primitives listed above (arc-dsl-llm names)\n",
      "- Add brief comments for clarity\n",
      "- Prefer functional patterns (compose, chain, combine_two_function_results, fix_first_argument/fix_last_argument, transform, transform_and_flatten)\n",
      "- You can adapt patterns from similar programs but adjust them to match the pattern description\n",
      "\n",
      "## Output Format\n",
      "```python\n",
      "def solve(I):\n",
      "    # [comment explaining step]\n",
      "    x1 = some_dsl_function(I)\n",
      "\n",
      "    # [comment]\n",
      "    x2 = another_function(x1, args)\n",
      "\n",
      "    # [final step]\n",
      "    return O  # O is the output grid\n",
      "```\n",
      "\n",
      "Generate the solve function now:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompter._get_phase2_dsl_section())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dsl = prompter._get_phase2_dsl_section()\n",
    "\"\"\"You are an expert at solving ARC puzzles using program synthesis given a DSL.\n",
    "Your goal is given a task, generate a Python `solve(I)` function using ONLY the DSL primitives below.\\n\\n\n",
    "{prompt_dsl}\n",
    "\n",
    "Here are some examples of ARC tasks and their corresponding `solve(I)` functions:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# content_blocks.extend(self._format_training_examples(task['train']))\n",
    "# content_blocks.extend(self._format_test_examples(task['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6030c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 programs from library\n",
      "First program: 007bbfb7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(library)} programs from library\", flush=True)\n",
    "if len(library) > 0:\n",
    "    print(f\"First program: {library.programs[0]['task_id']}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb71e66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/0934a4d8.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/135a2760.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/136b0064.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/13e47133.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/142ca369.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/16b78196.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/16de56c4.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/1818057f.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/195c6913.json'),\n",
       " PosixPath('/home/flowers/work/llms_ftw/tasks/evaluation/1ae2feb7.json')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/flowers/work/llms_ftw/tasks/evaluation/\"\n",
    "data_path = Path(data_path)\n",
    "json_files = sorted(data_path.glob('*.json'))\n",
    "json_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c595fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 120 tasks in /home/flowers/work/llms_ftw/tasks/evaluation/\n",
      "\n",
      "Evaluating on first 1 tasks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/flowers/work/llms_ftw/tasks/evaluation/\"\n",
    "verbose = True\n",
    "n_workers = None\n",
    "timeout = 2\n",
    "evaluate_on_n_pb = 1\n",
    "\n",
    "\n",
    "data_path = Path(data_dir)\n",
    "\n",
    "\n",
    "json_files = sorted(data_path.glob('*.json'))\n",
    "\n",
    "\n",
    "print(f\"\\nFound {len(json_files)} tasks in {data_dir}\\n\", flush=True)\n",
    "if evaluate_on_n_pb != -1:\n",
    "    json_files = json_files[:evaluate_on_n_pb]\n",
    "    print (f\"Evaluating on first {evaluate_on_n_pb} tasks\\n\", flush=True)\n",
    "results = []\n",
    "successful = 0\n",
    "total_score = 0.0\n",
    "\n",
    "# for loop here\n",
    "i=1\n",
    "task_file = json_files[0]\n",
    "task_id = task_file.stem\n",
    "with open(task_file, 'r') as f:\n",
    "    task = json.load(f)\n",
    "\n",
    "task=task\n",
    "task_id=task_id\n",
    "# vlm_client_phase1=vlm_client_phase1\n",
    "# vlm_client_phase2=vlm_client_phase2\n",
    "# prompter=prompter\n",
    "# library=library\n",
    "verbose=verbose\n",
    "n_workers=n_workers\n",
    "timeout=timeout\n",
    "log_dir=\"logs\"#TODO change log dir\n",
    "\n",
    "# result = solve_task(\n",
    "# task=task,\n",
    "# task_id=task_id,\n",
    "# vlm_client_phase1=vlm_client_phase1,\n",
    "# vlm_client_phase2=vlm_client_phase2,\n",
    "# prompter=prompter,\n",
    "# library=library,\n",
    "# verbose=verbose,\n",
    "# n_workers=n_workers,\n",
    "# timeout=timeout,\n",
    "# log_dir=\"logs_images\"#TODO change log dir\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a645f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG find_similar ===\n",
      "Number of train examples: 4\n",
      "Number of programs in library: 400\n",
      "min_similarity threshold: 0.1\n",
      "top_k: 5\n",
      "Using 28 parallel workers (timeout: 2s per program)\n",
      "Progress: 50/400 programs (443.9/s, 0.1s elapsed)\n",
      "Progress: 100/400 programs (398.8/s, 0.3s elapsed)\n",
      "Progress: 150/400 programs (490.4/s, 0.3s elapsed)\n",
      "Progress: 200/400 programs (554.7/s, 0.4s elapsed)\n",
      "Progress: 250/400 programs (591.1/s, 0.4s elapsed)\n",
      "Progress: 300/400 programs (628.6/s, 0.5s elapsed)\n",
      "Progress: 350/400 programs (599.6/s, 0.6s elapsed)\n",
      "Progress: 400/400 programs (46.8/s, 8.5s elapsed)\n",
      "\n",
      "Evaluation complete: 400/400 programs in 8.6s\n",
      "\n",
      "=== Execution Statistics ===\n",
      "Programs with errors: 29/400\n",
      "Max similarity achieved: 0.110\n",
      "Average similarity: 0.007\n",
      "\n",
      "Sample errors from first 3 programs:\n",
      "  228f6490: StopIteration: \n",
      "  272f95fa: StopIteration: \n",
      "  3de23699: StopIteration: \n",
      "\n",
      "Top 5 programs by similarity:\n",
      "  1cf80156: 0.110 (errors: 0/4)\n",
      "  ce4f8723: 0.103 (errors: 0/4)\n",
      "  f25fbde4: 0.101 (errors: 0/4)\n",
      "  28bf18c6: 0.088 (errors: 0/4)\n",
      "  1c786137: 0.083 (errors: 0/4)\n",
      "\n",
      "=== Results ===\n",
      "Programs with perfect examples: 0\n",
      "Other programs above threshold: 3\n",
      "Returning 3 programs\n",
      "Top program: 1cf80156 (sim: 0.110)\n",
      "Second program: ce4f8723 (sim: 0.103)\n"
     ]
    }
   ],
   "source": [
    "similar_programs = library.find_similar(\n",
    "    train_examples=task['train'],\n",
    "    top_k=5,\n",
    "    min_similarity=0.1,\n",
    "    n_workers=n_workers,\n",
    "    timeout=timeout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8823479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Best library match: Task 1cf80156 (0.11)\n"
     ]
    }
   ],
   "source": [
    "best_library_score = 0.0\n",
    "best_library_program = None\n",
    "\n",
    "if similar_programs:\n",
    "    best_match = similar_programs[0]\n",
    "    best_library_score = best_match['similarity']\n",
    "    best_library_program = best_match['program']\n",
    "    if verbose:\n",
    "        print(f\"\\nâœ“ Best library match: Task {best_match['task_id']} ({best_library_score:.2f})\", flush=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d5eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_prompt = prompter.build_phase1_prompt(task, similar_programs)\n",
    "# phase1_output = vlm_client_phase1.query(\n",
    "#     phase1_prompt,\n",
    "#     system_prompt=\"\"\"\"You are an expert at analyzing ARC puzzles and discovering transformation patterns.\n",
    "\n",
    "# Remember: Your first hypothesis is sticky and excessively convincing to you.\n",
    "# Combat this by evolving your hypothesis and actively seeking evidence against your initial guess to avoid halluciantion.\n",
    "# \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14436064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training Examples\n",
      "Below are 4 training examples follwed by the test example(s) you have to generalize to:\n",
      " for each example, the input grid is shown first, followed by the output grid. \n",
      ".\n",
      "Example 1:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "3|5|3|3|6|6|5|4|1|4|9|9|4|3|9|9|9|9|3|4|9|9|4|1|4|5|6|6|3|3\n",
      "5|3|3|3|6|6|4|5|4|1|9|9|3|4|9|1|1|9|4|3|9|9|1|4|5|4|6|6|3|3\n",
      "1|1|3|5|5|4|6|6|9|1|1|4|9|9|4|5|5|4|9|9|4|1|1|9|6|6|4|5|5|3\n",
      "1|1|5|3|4|5|6|6|1|9|4|1|9|1|4|4|4|4|1|9|1|4|9|1|6|6|5|4|3|5\n",
      "6|9|9|9|3|5|3|3|4|3|9|9|9|2|6|9|9|6|2|9|9|9|3|4|3|3|5|3|9|9\n",
      "9|6|9|9|5|3|3|3|3|4|9|1|9|9|9|6|6|9|9|9|1|9|4|3|3|3|3|5|9|9\n",
      "9|9|6|9|1|1|3|5|9|9|4|4|6|9|9|2|2|9|9|6|4|4|9|9|5|3|1|1|9|6\n",
      "9|9|9|6|1|1|5|3|9|1|5|4|9|6|9|9|9|9|6|9|4|5|1|9|3|5|1|1|6|9\n",
      "1|4|9|1|4|3|9|9|5|5|7|2|4|3|2|4|4|2|3|4|2|7|5|5|9|9|3|4|1|9\n",
      "4|1|1|9|3|4|9|1|4|5|2|7|3|4|4|2|2|4|4|3|7|2|5|4|1|9|4|3|9|1\n",
      "9|9|1|4|9|9|4|5|6|4|5|5|2|4|4|3|3|4|4|2|5|5|4|6|5|4|9|9|4|1\n",
      "9|9|4|1|9|1|4|4|4|5|4|5|4|2|3|4|4|3|2|4|5|4|5|4|4|4|1|9|1|4\n",
      "4|3|9|9|9|9|6|9|5|9|7|7|5|5|7|2|2|7|5|5|7|7|9|5|9|6|9|9|9|9\n",
      "3|4|9|1|2|9|9|6|9|5|7|7|4|5|2|7|7|2|5|4|7|7|5|9|6|9|9|2|1|9\n",
      "9|9|4|4|6|9|9|9|7|7|5|9|5|4|5|5|5|5|4|5|9|5|7|7|9|8|8|8|8|4\n",
      "9|1|5|4|9|6|2|9|7|7|9|5|4|6|4|5|5|4|6|4|5|9|7|7|9|8|8|8|8|5\n",
      "9|1|5|4|9|6|2|9|7|7|9|5|4|6|4|5|5|4|6|4|5|9|7|7|9|8|8|8|8|5\n",
      "9|9|4|4|6|9|9|9|7|7|5|9|5|4|5|5|5|5|4|5|9|5|7|7|9|8|8|8|8|4\n",
      "3|4|9|1|2|9|9|6|9|5|7|7|4|5|2|7|7|2|5|4|7|7|5|9|6|8|8|8|8|9\n",
      "4|3|9|9|9|9|6|9|5|9|7|7|5|5|7|2|2|7|5|5|7|7|9|5|9|8|8|8|8|9\n",
      "9|9|4|1|9|1|4|4|4|5|4|5|4|2|3|4|4|3|2|4|5|4|5|4|4|8|8|8|8|4\n",
      "9|9|1|4|9|9|4|5|6|4|5|5|2|4|4|3|3|4|4|2|5|5|4|6|5|8|8|8|8|1\n",
      "4|1|1|9|3|4|9|1|4|5|2|7|3|4|4|2|2|4|4|3|7|2|5|4|1|8|8|8|8|1\n",
      "1|4|9|1|4|3|9|9|5|5|7|2|4|3|2|4|4|2|3|4|2|7|5|5|9|9|3|4|1|9\n",
      "9|9|9|6|1|1|5|3|9|1|5|4|9|6|9|9|9|9|6|9|4|5|1|9|3|5|1|1|6|9\n",
      "9|9|6|9|1|1|3|5|9|9|4|4|6|9|9|2|2|9|9|6|4|4|9|9|5|3|1|1|9|6\n",
      "9|6|9|9|5|3|3|3|3|4|9|1|9|9|9|6|6|9|9|9|1|9|4|3|3|3|3|5|9|9\n",
      "6|9|9|9|3|5|3|3|4|3|9|9|9|2|6|9|9|6|2|9|9|9|3|4|3|3|5|3|9|9\n",
      "1|1|5|3|4|5|6|6|1|9|4|1|9|1|4|4|4|4|1|9|1|4|9|1|6|6|5|4|3|5\n",
      "1|1|3|5|5|4|6|6|9|1|1|4|9|9|4|5|5|4|9|9|4|1|1|9|6|6|4|5|5|3\n",
      "\n",
      "Output:\n",
      "\n",
      "ASCII representation:\n",
      "9|9|6|4\n",
      "2|6|9|4\n",
      "2|6|9|4\n",
      "9|9|6|4\n",
      "9|9|2|1\n",
      "6|9|9|9\n",
      "4|1|9|1\n",
      "4|9|9|4\n",
      "9|4|3|9\n",
      "\n",
      "Example 2:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "9|9|2|3|4|4|7|5|3|3|6|6|3|5|6|4|4|6|5|3|6|6|3|3|5|7|4|4|3|2\n",
      "7|9|3|5|4|4|5|7|3|3|6|6|6|3|4|6|6|4|3|6|6|6|3|3|7|5|4|4|5|3\n",
      "3|2|9|9|7|5|4|4|4|1|3|3|6|4|4|7|7|4|4|6|3|8|8|8|8|8|5|7|9|9\n",
      "2|3|7|9|5|7|4|4|1|4|3|3|4|6|7|4|4|7|6|4|3|8|8|8|8|8|7|5|9|7\n",
      "7|7|9|3|9|9|5|3|3|6|6|4|6|7|9|9|9|9|7|6|4|8|8|8|8|8|9|9|3|9\n",
      "7|7|3|9|7|9|3|2|5|3|4|6|2|6|9|9|9|9|6|2|6|8|8|8|8|8|9|7|9|3\n",
      "9|3|7|7|3|2|9|9|6|4|4|7|9|2|6|7|7|6|2|9|7|4|4|6|9|9|2|3|7|7\n",
      "3|9|7|7|2|3|7|9|4|6|7|4|2|9|2|6|6|2|9|2|4|7|6|4|9|7|3|2|7|7\n",
      "3|3|4|1|3|5|6|4|2|4|7|7|1|6|7|2|2|7|6|1|7|7|4|2|4|6|5|3|1|4\n",
      "3|3|1|4|6|3|4|6|2|2|7|1|6|1|2|7|7|2|1|6|1|7|2|2|6|4|3|6|4|1\n",
      "6|6|3|3|6|4|4|7|1|1|2|4|7|2|1|6|6|1|2|7|4|2|1|1|7|4|4|6|3|3\n",
      "6|6|3|3|4|6|7|4|1|3|2|2|2|7|6|1|1|6|7|2|2|2|3|1|4|7|6|4|3|3\n",
      "3|6|6|4|6|2|9|2|9|9|9|7|2|4|1|7|7|1|4|2|7|9|9|9|2|9|2|6|4|6\n",
      "5|3|4|6|7|6|2|9|9|9|7|9|2|2|7|7|7|7|2|2|9|7|9|9|9|2|6|7|6|4\n",
      "6|4|4|7|9|9|6|2|9|7|9|9|3|1|2|4|4|2|1|3|9|9|7|9|2|6|9|9|7|4\n",
      "4|6|7|4|9|9|7|6|7|9|9|9|1|1|2|2|2|2|1|1|9|9|9|7|6|7|9|9|4|7\n",
      "4|6|7|4|9|9|7|6|7|9|9|9|1|1|2|2|2|2|1|1|9|9|9|7|6|7|9|9|4|7\n",
      "6|4|4|7|9|9|6|2|9|7|9|9|3|1|2|4|4|2|1|3|9|9|7|9|2|6|9|9|7|4\n",
      "5|3|4|6|7|6|2|9|9|9|7|9|2|2|7|7|7|7|2|2|9|7|9|9|9|2|6|7|6|4\n",
      "3|6|6|4|6|2|9|2|9|9|9|7|2|4|1|7|7|1|4|2|7|9|9|9|2|9|2|6|4|6\n",
      "6|6|3|3|4|6|7|4|1|3|2|2|2|7|6|1|1|6|7|2|2|2|3|1|4|7|6|4|3|3\n",
      "6|6|3|3|6|4|4|7|1|1|2|4|7|2|1|6|6|1|2|7|4|2|1|1|7|4|4|6|3|3\n",
      "3|3|1|4|6|3|4|6|2|2|7|1|6|1|2|7|7|2|1|6|1|7|2|2|6|4|3|6|4|1\n",
      "3|3|4|1|3|5|6|4|2|4|7|7|1|6|7|2|2|7|6|1|7|7|4|2|4|6|5|3|1|4\n",
      "3|9|7|7|2|3|7|9|4|6|7|4|2|9|2|6|6|2|9|2|4|7|6|4|9|7|3|2|7|7\n",
      "9|3|7|7|3|2|9|9|6|4|4|7|9|2|6|7|7|6|2|9|7|4|4|6|9|9|2|3|7|7\n",
      "7|7|3|9|7|9|3|2|5|3|4|6|2|6|9|9|9|9|6|2|6|4|3|5|2|3|9|7|9|3\n",
      "7|7|9|3|9|9|5|3|3|6|6|4|6|7|9|9|9|9|7|6|4|6|6|3|3|5|9|9|3|9\n",
      "2|3|7|9|5|7|4|4|1|4|3|3|4|6|7|4|4|7|6|4|3|3|4|1|4|4|7|5|9|7\n",
      "3|2|9|9|7|5|4|4|4|1|3|3|6|4|4|7|7|4|4|6|3|3|1|4|4|4|5|7|9|9\n",
      "\n",
      "Output:\n",
      "\n",
      "ASCII representation:\n",
      "3|1|4|4|4\n",
      "3|4|1|4|4\n",
      "6|6|3|3|5\n",
      "4|3|5|2|3\n",
      "\n",
      "Example 3:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "1|9|4|4|9|9|2|7|6|6|9|9|7|6|7|2|2|7|6|7|9|9|6|6|7|2|9|9|4|4\n",
      "7|1|4|4|9|9|7|2|6|6|9|9|6|7|2|7|7|2|7|6|9|9|6|6|2|7|9|9|4|4\n",
      "2|7|1|9|2|7|9|9|4|4|6|6|7|2|5|1|1|5|2|7|6|6|4|4|9|9|7|2|9|1\n",
      "7|2|7|1|7|2|9|9|4|4|6|6|2|7|5|5|5|5|7|2|6|6|4|4|9|9|2|7|1|7\n",
      "9|6|7|2|1|9|4|4|7|6|7|2|9|2|6|4|4|6|2|9|2|7|6|7|4|4|9|1|2|7\n",
      "6|9|2|7|7|1|4|4|6|7|2|7|9|9|4|6|6|4|9|9|7|2|7|6|4|4|1|7|7|2\n",
      "7|2|9|6|2|7|1|9|7|2|5|5|4|5|9|2|2|9|5|4|5|5|2|7|9|1|7|2|6|9\n",
      "2|7|6|9|7|2|7|1|2|7|1|5|5|4|9|9|9|9|4|5|5|1|7|2|1|7|2|7|9|6\n",
      "6|6|4|4|7|6|7|2|3|7|1|4|9|7|7|6|6|7|7|9|4|1|7|3|2|7|6|7|4|4\n",
      "6|6|4|4|6|7|2|7|4|3|4|4|7|9|6|7|7|6|9|7|4|4|3|4|7|2|7|6|4|4\n",
      "9|9|6|6|7|2|5|1|3|7|3|7|7|6|9|7|7|9|6|7|7|3|7|3|1|5|2|7|6|6\n",
      "9|9|6|6|2|7|5|5|7|7|4|3|6|7|7|9|9|7|7|6|3|4|7|7|5|5|7|2|6|6\n",
      "7|6|7|2|9|9|4|5|6|6|5|9|3|7|4|4|4|4|7|3|9|5|6|6|5|4|9|9|2|7\n",
      "6|7|2|7|2|9|5|4|6|6|9|5|4|3|4|1|1|4|3|4|5|9|6|6|4|5|9|2|7|2\n",
      "7|2|5|5|6|4|9|9|5|9|6|6|7|7|3|7|7|3|7|7|6|6|9|5|9|9|4|6|5|5\n",
      "2|7|1|5|4|6|2|9|9|5|6|6|7|3|4|3|3|4|3|7|6|6|5|9|9|2|6|4|5|1\n",
      "2|7|1|5|4|6|2|9|9|5|6|6|7|3|4|3|3|4|3|7|6|6|5|9|9|2|6|4|5|1\n",
      "7|2|5|5|6|4|9|9|5|9|6|6|7|7|3|7|7|3|7|7|6|6|9|5|9|9|4|6|5|5\n",
      "6|7|2|7|2|9|5|4|6|6|9|5|4|3|4|1|1|4|3|4|5|9|6|6|4|5|9|2|7|2\n",
      "7|6|7|2|9|9|4|5|6|6|5|9|8|8|8|8|8|8|8|3|9|5|6|6|5|4|9|9|2|7\n",
      "9|9|6|6|2|7|5|5|7|7|4|3|8|8|8|8|8|8|8|6|3|4|7|7|5|5|7|2|6|6\n",
      "9|9|6|6|7|2|5|1|3|7|3|7|8|8|8|8|8|8|8|7|7|3|7|3|1|5|2|7|6|6\n",
      "6|6|4|4|6|7|2|7|4|3|4|4|7|9|6|7|7|6|9|7|4|4|3|4|7|2|7|6|4|4\n",
      "6|6|4|4|7|6|7|2|3|7|1|4|9|7|7|6|6|7|7|9|4|1|7|3|2|7|6|7|4|4\n",
      "2|7|6|9|7|2|7|1|2|7|1|5|5|4|9|9|9|9|4|5|5|1|7|2|1|7|2|7|9|6\n",
      "7|2|9|6|2|7|1|9|7|2|5|5|4|5|9|2|2|9|5|4|5|5|2|7|9|1|7|2|6|9\n",
      "6|9|2|7|7|1|4|4|6|7|2|7|9|9|4|6|6|4|9|9|7|2|7|6|4|4|1|7|7|2\n",
      "9|6|7|2|1|9|4|4|7|6|7|2|9|2|6|4|4|6|2|9|2|7|6|7|4|4|9|1|2|7\n",
      "7|2|7|1|7|2|9|9|4|4|6|6|2|7|5|5|5|5|7|2|6|6|4|4|9|9|2|7|1|7\n",
      "2|7|1|9|2|7|9|9|4|4|6|6|7|2|5|1|1|5|2|7|6|6|4|4|9|9|7|2|9|1\n",
      "\n",
      "Output:\n",
      "\n",
      "ASCII representation:\n",
      "3|7|4|4|4|4|7\n",
      "6|7|7|9|9|7|7\n",
      "7|6|9|7|7|9|6\n",
      "\n",
      "Example 4:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "3|1|1|9|5|6|7|1|1|4|5|7|3|9|9|1|1|9|9|3|7|5|4|1|1|7|6|5|9|1\n",
      "1|3|9|5|6|5|1|7|4|1|7|5|4|3|1|3|3|1|3|4|5|7|1|4|7|1|5|6|5|9\n",
      "6|9|3|1|7|1|5|6|9|9|1|4|9|1|1|4|4|1|1|9|4|1|9|9|6|5|1|7|1|3\n",
      "9|1|1|3|1|7|6|5|9|9|4|1|1|3|4|1|1|4|3|1|1|4|9|9|5|6|7|1|3|1\n",
      "6|6|6|7|3|1|5|9|3|4|9|1|6|7|2|5|5|2|7|6|1|9|4|3|9|5|1|3|7|6\n",
      "6|6|7|6|1|3|9|1|9|3|1|3|7|6|5|2|2|5|6|7|3|1|3|9|1|9|3|1|6|7\n",
      "6|7|6|6|1|9|3|1|9|1|1|4|6|9|6|7|7|6|9|6|4|1|1|9|1|3|9|1|6|6\n",
      "7|6|6|6|9|6|1|3|1|3|4|1|9|6|7|6|6|7|6|9|1|4|3|1|3|1|8|8|8|8\n",
      "1|4|9|9|3|9|9|1|1|1|6|1|5|2|5|5|5|5|2|5|1|6|1|1|1|9|8|8|8|8\n",
      "4|1|9|9|4|3|1|3|1|1|1|6|2|5|5|5|5|5|5|2|6|1|1|1|3|1|8|8|8|8\n",
      "5|7|1|4|9|1|1|4|2|2|1|1|5|5|5|2|2|5|5|5|1|1|2|2|4|1|8|8|8|8\n",
      "7|5|4|1|1|3|4|1|2|1|1|1|5|5|2|5|5|2|5|5|1|1|1|2|1|4|3|1|1|4\n",
      "3|4|9|1|6|7|6|9|7|6|3|3|1|1|6|1|1|6|1|1|3|3|6|7|9|6|7|6|1|9\n",
      "9|3|1|3|7|6|9|6|6|7|3|3|1|1|1|6|6|1|1|1|3|3|7|6|6|9|6|7|3|1\n",
      "9|1|1|4|2|5|6|7|3|3|7|6|1|2|1|1|1|1|2|1|6|7|3|3|7|6|5|2|4|1\n",
      "1|3|4|1|5|2|7|6|3|3|6|7|2|2|1|1|1|1|2|2|7|6|3|3|6|7|2|5|1|4\n",
      "1|3|4|1|5|2|7|6|3|3|6|7|2|2|1|1|1|1|2|2|7|6|3|3|6|7|2|5|1|4\n",
      "9|1|1|4|2|5|6|7|3|3|7|6|1|2|1|1|1|1|2|1|6|7|3|3|7|6|5|2|4|1\n",
      "9|3|1|3|7|6|9|6|6|7|3|3|1|1|1|6|6|1|1|1|3|3|7|6|6|9|6|7|3|1\n",
      "3|4|9|1|6|7|6|9|7|6|3|3|1|1|6|1|1|6|1|1|3|3|6|7|9|6|7|6|1|9\n",
      "7|5|4|1|1|3|4|1|2|1|1|1|5|5|2|5|5|2|5|5|1|1|1|2|1|4|3|1|1|4\n",
      "5|7|1|4|9|1|1|4|2|2|1|1|5|5|5|2|2|5|5|5|1|1|2|2|4|1|1|9|4|1\n",
      "4|1|9|9|4|3|1|3|1|1|1|6|2|5|5|5|5|5|5|2|6|1|1|1|3|1|3|4|9|9\n",
      "1|4|9|9|3|9|9|1|1|1|6|1|5|2|5|5|5|5|2|5|1|6|1|1|1|9|9|3|9|9\n",
      "7|6|6|6|9|6|1|3|1|3|4|1|9|6|7|6|6|7|6|9|1|4|3|1|3|1|6|9|6|6\n",
      "6|7|6|6|1|9|3|1|9|1|1|4|6|9|6|7|7|6|9|6|4|1|1|9|1|3|9|1|6|6\n",
      "6|6|7|6|1|3|9|1|9|3|1|3|7|6|5|2|2|5|6|7|3|1|3|9|1|9|3|1|6|7\n",
      "6|6|6|7|3|1|5|9|3|4|9|1|6|7|2|5|5|2|7|6|1|9|4|3|9|5|1|3|7|6\n",
      "9|1|1|3|1|7|6|5|9|9|4|1|1|3|4|1|1|4|3|1|1|4|9|9|5|6|7|1|3|1\n",
      "6|9|3|1|7|1|5|6|9|9|1|4|9|1|1|4|4|1|1|9|4|1|9|9|6|5|1|7|1|3\n",
      "\n",
      "Output:\n",
      "\n",
      "ASCII representation:\n",
      "6|9|6|6\n",
      "9|3|9|9\n",
      "3|4|9|9\n",
      "1|9|4|1\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLES (to solve)\n",
      "============================================================\n",
      "Below are 1 test example(s) you need to solve:\n",
      "For each test example, only the input grid is provided. You must determine the output.\n",
      "\n",
      "Test Example 1:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "4|4|1|3|5|7|7|9|6|1|6|6|4|4|7|7|7|7|4|4|6|6|1|6|9|7|7|5|3|1\n",
      "4|4|3|3|7|5|9|7|6|6|6|6|4|4|7|2|2|7|4|4|6|6|6|6|7|9|5|7|3|3\n",
      "3|4|4|4|7|9|5|7|5|1|6|1|7|7|9|9|9|9|7|7|1|6|1|5|7|5|9|7|4|4\n",
      "4|3|4|4|9|7|7|5|1|5|6|6|7|2|1|9|9|1|2|7|6|6|5|1|5|7|7|9|4|4\n",
      "9|7|7|4|4|4|3|3|4|4|7|7|9|7|3|2|2|3|7|9|7|7|4|4|3|3|4|4|4|7\n",
      "7|9|4|7|4|4|3|1|4|4|7|2|7|9|2|3|3|2|9|7|2|7|4|4|1|3|4|4|7|4\n",
      "7|4|9|7|3|4|4|4|7|7|9|1|7|4|9|7|7|9|4|7|1|9|7|7|4|4|4|3|7|9\n",
      "4|7|7|9|4|3|4|4|7|2|9|9|4|7|7|9|9|7|7|4|9|9|2|7|4|4|3|4|9|7\n",
      "6|6|5|1|4|4|7|7|7|2|2|6|4|6|2|2|2|2|6|4|6|2|2|7|7|7|4|4|1|5\n",
      "1|6|1|5|4|4|7|2|3|7|6|6|6|4|2|2|2|2|4|6|6|6|7|3|2|7|4|4|5|1\n",
      "6|6|6|6|7|7|9|9|9|1|7|2|2|2|4|6|6|4|2|2|2|7|1|9|9|9|7|7|6|6\n",
      "6|6|1|6|7|2|1|9|1|5|3|7|2|2|6|4|4|6|2|2|7|3|5|1|9|1|2|7|6|1\n",
      "4|4|7|7|9|7|7|4|9|9|1|6|7|2|6|6|6|6|2|7|6|1|9|9|4|7|7|9|7|7\n",
      "4|4|7|2|7|9|4|7|9|9|6|1|3|7|6|2|2|6|7|3|1|6|9|9|7|4|9|7|2|7\n",
      "8|8|8|1|3|2|9|7|1|6|9|9|5|1|7|2|2|7|1|5|9|9|6|1|7|9|2|3|1|9\n",
      "8|8|8|9|2|3|7|9|6|1|9|9|1|9|3|7|7|3|9|1|9|9|1|6|9|7|3|2|9|9\n",
      "8|8|8|9|2|3|7|9|6|1|9|9|1|9|3|7|7|3|9|1|9|9|1|6|9|7|3|2|9|9\n",
      "8|8|8|1|3|2|9|7|1|6|9|9|5|1|7|2|2|7|1|5|9|9|6|1|7|9|2|3|1|9\n",
      "8|8|8|2|7|9|4|7|9|9|6|1|3|7|6|2|2|6|7|3|1|6|9|9|7|4|9|7|2|7\n",
      "8|8|8|7|9|7|7|4|9|9|1|6|7|2|6|6|6|6|2|7|6|1|9|9|4|7|7|9|7|7\n",
      "8|8|8|6|7|2|1|9|1|5|3|7|2|2|6|4|4|6|2|2|7|3|5|1|9|1|2|7|6|1\n",
      "8|8|8|6|7|7|9|9|9|1|7|2|2|2|4|6|6|4|2|2|2|7|1|9|9|9|7|7|6|6\n",
      "8|8|8|5|4|4|7|2|3|7|6|6|6|4|2|2|2|2|4|6|6|6|7|3|2|7|4|4|5|1\n",
      "6|6|5|1|4|4|7|7|7|2|2|6|4|6|2|2|2|2|6|4|6|2|2|7|7|7|4|4|1|5\n",
      "4|7|7|9|4|3|4|4|7|2|9|9|4|7|7|9|9|7|7|4|9|9|2|7|4|4|3|4|9|7\n",
      "7|4|9|7|3|4|4|4|7|7|9|1|7|4|9|7|7|9|4|7|1|9|7|7|4|4|4|3|7|9\n",
      "7|9|4|7|4|4|3|1|4|4|7|2|7|9|2|3|3|2|9|7|2|7|4|4|1|3|4|4|7|4\n",
      "9|7|7|4|4|4|3|3|4|4|7|7|9|7|3|2|2|3|7|9|7|7|4|4|3|3|4|4|4|7\n",
      "4|3|4|4|9|7|7|5|1|5|6|6|7|2|1|9|9|1|2|7|6|6|5|1|5|7|7|9|4|4\n",
      "3|4|4|4|7|9|5|7|5|1|6|1|7|7|9|9|9|9|7|7|1|6|1|5|7|5|9|7|4|4\n",
      "\n",
      "Output: [TO BE DETERMINED]\n",
      "\n",
      "## Similar Programs from Library\n",
      "The following programs may be useful to solve the current tasks, feel free to use parts of each program, and/or combine them. Most importanly, use them as guidance:\n",
      "\n",
      "Similar Program 1 (similarity: 0.11, task: 1cf80156):\n",
      "```python\n",
      "def solve_1cf80156(I):\n",
      "    x1 = as_objects(I, True, True, True)\n",
      "    x2 = get_first(x1)\n",
      "    O = smallest_subgrid_containing(x2, I)\n",
      "    return O\n",
      "\n",
      "```\n",
      "\n",
      "Similar Program 2 (similarity: 0.10, task: ce4f8723):\n",
      "```python\n",
      "def solve_ce4f8723(I):\n",
      "    x1 = top_half(I)\n",
      "    x2 = bottom_half(I)\n",
      "    x3 = of_color(x1, COLOR_ZERO)\n",
      "    x4 = of_color(x2, COLOR_ZERO)\n",
      "    x5 = intersection(x3, x4)\n",
      "    x6 = as_tuple(4, 4)\n",
      "    x7 = create_grid(COLOR_THREE, x6)\n",
      "    O = fill(x7, COLOR_ZERO, x5)\n",
      "    return O\n",
      "\n",
      "```\n",
      "\n",
      "Similar Program 3 (similarity: 0.10, task: f25fbde4):\n",
      "```python\n",
      "def solve_f25fbde4(I):\n",
      "    x1 = as_objects(I, True, True, True)\n",
      "    x2 = get_first(x1)\n",
      "    x3 = smallest_subgrid_containing(x2, I)\n",
      "    O = upscale(x3, 2)\n",
      "    return O\n",
      "\n",
      "```\n",
      "\n",
      "## Analysis Protocol\n",
      "\n",
      "You will analyze these examples and the test sample systematically, allowing your hypothesis to evolve \n",
      "naturally as you see more data - like a human solving a puzzle.\n",
      "\n",
      "**Core principle:** Look for DIFFERENCES within each example (inputâ†’output changes)\n",
      "and SIMILARITIES across all examples (the consistent pattern).\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Work through examples sequentially, reasoning in plain English about what you observe.\n",
      "\n",
      "Step 1.1: First Example Analysis\n",
      "Given: Input 1 â†’ Output 1\n",
      "\n",
      "<observation_1>\n",
      "Describe what you see:\n",
      "- What's the size/shape change?\n",
      "- What colors changed? \n",
      "- What geometric transformations occurred?\n",
      "- What patterns do you notice?\n",
      "</observation_1>\n",
      "\n",
      "<hypothesis_1>\n",
      "State your initial guess about the transformation rule in natural language.\n",
      "Example: \"The grid appears to be flipped horizontally\"\n",
      "</hypothesis_1>\n",
      "\n",
      "Step 1.2: Second Example Validation\n",
      "Given: Input 2 â†’ Output 2\n",
      "\n",
      "<observation_2>\n",
      "- Does your hypothesis from Example 1 still hold?\n",
      "- What's similar to Example 1? \n",
      "- What's different from Example 1?\n",
      "</observation_2>\n",
      "\n",
      "<hypothesis_2>\n",
      "Refine your hypothesis:\n",
      "- If it still works: Confirm and strengthen\n",
      "- If it breaks: Revise with a more general pattern\n",
      "Example: \"Actually, it's mirrored horizontally, THEN the original is stacked on top\"\n",
      "</hypothesis_2>\n",
      "\n",
      "Step 1.3: Third Example Confirmation\n",
      "Given: Input 3 â†’ Output 3\n",
      "\n",
      "<observation_3>\n",
      "- Does hypothesis_2 work here?\n",
      "- Any new edge cases or variations?\n",
      "</observation_3>\n",
      "\n",
      "<hypothesis_3>\n",
      "Final refined hypothesis in natural language.\n",
      "</hypothesis_3>\n",
      "\n",
      "Step 1.N: Additional Examples\n",
      "Continue for all remaining examples...\n",
      "\n",
      "Step 2.1: First Test Example Confirmation\n",
      "Given: Test Input 1\n",
      "<observation_test1>\n",
      "- Does hypothesis_training work here?\n",
      "- Any new edge cases or variations?\n",
      "</observation_test1>\n",
      "\n",
      "<hypothesis_test1>\n",
      "Test hypothesis after seeing test example 1.\n",
      "</hypothesis_test1>\n",
      "\n",
      "Step 2.N: Additional Test Examples, if present\n",
      "Continue for all remaining test cases...\n",
      "\n",
      "Step 2.Final: Pattern Synthesis\n",
      "<pattern_summary>\n",
      "In plain English, the transformation rule is:\n",
      "- [First operation in natural language]\n",
      "- [Second operation in natural language]\n",
      "- [Any conditions or special cases]\n",
      "\n",
      "Edge cases to consider:\n",
      "- [Any variations you noticed]\n",
      "\n",
      "Why this works:\n",
      "- [Brief explanation about WHY this pattern makes sense]\n",
      "</pattern_summary>\n",
      "\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Begin your analysis:\n"
     ]
    }
   ],
   "source": [
    "prompt_str = \"\"\n",
    "for i in phase1_prompt:\n",
    "    prompt_str += i[\"text\"]\n",
    "print(prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9385d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af080730",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train= \"/home/flowers/work/llms_ftw/tasks/training/\"\n",
    "list_task_train_id = [\"29c11459\",\n",
    "\"4612dd53\",\n",
    "\"b782dc8a\"]\n",
    "\n",
    "list_task_train = []\n",
    "for task_id in list_task_train_id:\n",
    "    with open(path_train + task_id + \".json\", \"r\") as f:\n",
    "        task_data = json.load(f)\n",
    "        list_task_train.append(task_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0207fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are 2 training examples follwed by the test example(s) you have to generalize to:\n",
      " for each example, the input grid is shown first, followed by the output grid. \n",
      ".\n",
      "Example 1:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "8|0|0|0|0|0|8|8|8|8|8|8|0|8|8|8|0|8|8|0|8|8|8|0\n",
      "0|0|8|8|8|0|0|0|0|0|0|8|0|0|0|8|0|8|0|0|8|0|8|0\n",
      "8|8|8|0|8|0|8|8|8|8|0|8|8|8|0|8|0|8|8|8|8|0|8|0\n",
      "8|0|0|0|8|0|8|0|0|8|0|0|0|8|0|8|0|0|0|0|0|0|8|0\n",
      "8|0|8|8|8|0|8|8|0|8|0|8|8|8|0|8|8|0|8|8|8|8|8|0\n",
      "8|0|8|0|0|0|0|8|0|8|0|8|0|0|0|0|8|0|8|0|0|0|0|0\n",
      "8|0|8|8|8|8|8|8|0|8|0|8|8|8|8|8|8|3|8|8|8|8|8|0\n",
      "8|0|0|0|0|0|0|0|0|8|0|0|0|0|0|0|3|2|3|0|0|0|8|0\n",
      "8|8|0|8|8|8|0|8|8|8|0|8|8|8|8|8|8|3|8|8|8|0|8|0\n",
      "0|8|0|8|0|8|0|8|0|0|0|8|0|0|0|0|8|0|8|0|8|0|8|0\n",
      "0|8|8|8|0|8|8|8|0|8|8|8|0|8|8|0|8|8|8|0|8|8|8|0\n",
      "\n",
      "Output:\n",
      "\n",
      "ASCII representation:\n",
      "8|3|2|3|2|3|8|8|8|8|8|8|0|8|8|8|2|8|8|0|8|8|8|0\n",
      "3|2|8|8|8|2|3|2|3|2|3|8|0|0|0|8|3|8|0|0|8|2|8|0\n",
      "8|8|8|0|8|3|8|8|8|8|2|8|8|8|0|8|2|8|8|8|8|3|8|0\n",
      "8|0|0|0|8|2|8|0|0|8|3|2|3|8|0|8|3|2|3|2|3|2|8|0\n",
      "8|0|8|8|8|3|8|8|0|8|2|8|8|8|0|8|8|3|8|8|8|8|8|0\n",
      "8|0|8|2|3|2|3|8|0|8|3|8|0|0|0|0|8|2|8|0|0|0|0|0\n",
      "8|0|8|8|8|8|8|8|0|8|2|8|8|8|8|8|8|3|8|8|8|8|8|0\n",
      "8|0|0|0|0|0|0|0|0|8|3|2|3|2|3|2|3|2|3|2|3|2|8|0\n",
      "8|8|0|8|8|8|0|8|8|8|2|8|8|8|8|8|8|3|8|8|8|3|8|0\n",
      "0|8|0|8|0|8|0|8|3|2|3|8|0|0|0|0|8|2|8|0|8|2|8|0\n",
      "0|8|8|8|0|8|8|8|2|8|8|8|0|8|8|0|8|8|8|0|8|8|8|0\n",
      "\n",
      "Example 2:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "0|0|0|8|0|0|0|8|0|0|0|0|0|8\n",
      "8|8|0|8|8|8|0|8|0|8|8|8|0|8\n",
      "0|8|0|0|0|8|0|8|0|8|0|8|8|8\n",
      "0|8|8|8|8|8|0|8|0|8|0|0|0|0\n",
      "0|0|0|0|0|0|0|8|0|8|8|8|0|8\n",
      "8|8|8|8|8|8|0|8|0|0|0|8|0|8\n",
      "8|0|0|0|0|8|0|8|8|8|0|8|0|8\n",
      "8|8|8|8|0|8|0|0|0|8|0|8|0|0\n",
      "0|0|0|8|1|8|8|8|8|8|0|8|8|0\n",
      "8|8|0|8|4|1|0|0|0|0|0|0|8|0\n",
      "0|8|0|8|1|8|8|8|8|8|8|8|8|0\n",
      "0|8|8|8|0|8|0|0|0|0|0|0|0|0\n",
      "0|0|0|0|0|8|0|8|8|8|8|8|8|8\n",
      "\n",
      "Output:\n",
      "\n",
      "ASCII representation:\n",
      "0|0|0|8|0|0|0|8|1|4|1|4|1|8\n",
      "8|8|0|8|8|8|0|8|4|8|8|8|4|8\n",
      "0|8|0|0|0|8|0|8|1|8|0|8|8|8\n",
      "0|8|8|8|8|8|0|8|4|8|0|0|0|0\n",
      "0|0|0|0|0|0|0|8|1|8|8|8|0|8\n",
      "8|8|8|8|8|8|0|8|4|1|4|8|0|8\n",
      "8|4|1|4|1|8|0|8|8|8|1|8|0|8\n",
      "8|8|8|8|4|8|0|0|0|8|4|8|0|0\n",
      "0|0|0|8|1|8|8|8|8|8|1|8|8|0\n",
      "8|8|0|8|4|1|4|1|4|1|4|1|8|0\n",
      "1|8|0|8|1|8|8|8|8|8|8|8|8|0\n",
      "4|8|8|8|4|8|0|0|0|0|0|0|0|0\n",
      "1|4|1|4|1|8|0|8|8|8|8|8|8|8\n",
      "\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLES (to solve)\n",
      "============================================================\n",
      "Below are 1 test example(s) you need to solve:\n",
      "For each test example, only the input grid is provided. You must determine the output.\n",
      "\n",
      "Test Example 1:\n",
      "Input:\n",
      "\n",
      "ASCII representation:\n",
      "8|8|0|8|0|0|8|0|0|0|0|0|0|0|0\n",
      "0|8|0|8|8|8|8|4|8|8|8|8|8|8|8\n",
      "0|8|0|0|0|0|4|3|8|0|0|0|0|0|8\n",
      "0|8|8|8|8|8|8|4|8|8|8|0|8|8|8\n",
      "0|0|0|0|0|0|8|0|0|0|8|0|8|0|0\n",
      "8|8|8|8|8|0|8|8|8|0|8|0|8|0|8\n",
      "0|0|0|0|8|0|0|0|8|0|8|0|8|0|8\n",
      "8|8|8|0|8|8|8|0|8|0|8|0|8|8|8\n",
      "0|0|8|0|0|0|8|0|8|0|8|0|0|0|0\n",
      "8|0|8|8|8|0|8|8|8|0|8|8|8|0|8\n",
      "8|0|0|0|8|0|0|0|0|0|0|0|8|0|8\n",
      "8|8|8|0|8|0|8|8|8|8|8|8|8|0|8\n",
      "0|0|8|0|8|0|8|0|0|0|0|0|0|0|8\n",
      "8|0|8|8|8|0|8|0|8|8|8|8|8|8|8\n",
      "8|0|0|0|0|0|8|0|8|0|0|0|0|0|0\n",
      "\n",
      "Output: [TO BE DETERMINED]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_prompt_str(prompter._format_training_examples(list_task_train[2]['train'])))\n",
    "print(get_prompt_str(prompter._format_test_examples(list_task_train[2]['test'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ex=\"\"\"\n",
    "---------------------------\n",
    "# Task 1:\n",
    "\n",
    "Below are 2 training examples follwed by the test example(s) you have to generalize to, for each example, the input grid is shown first, followed by the output grid. \n",
    "\n",
    "Example 1:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "1|0|0|0|0|0|0|0|0|0|2\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "1|1|1|1|1|5|2|2|2|2|2\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Example 2:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "3|0|0|0|0|0|0|0|0|0|7\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "3|3|3|3|3|5|7|7|7|7|7\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "\n",
    "============================================================\n",
    "TEST EXAMPLES (to solve)\n",
    "============================================================\n",
    "Below are 1 test example(s) you need to solve:\n",
    "For each test example, only the input grid is provided. You must determine the output.\n",
    "\n",
    "Test Example 1:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "4|0|0|0|0|0|0|0|0|0|8\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0\n",
    "6|0|0|0|0|0|0|0|0|0|9\n",
    "\n",
    "Output: [TO BE DETERMINED]\n",
    "\n",
    "Solution:\n",
    "```python\n",
    "def solve(I):\n",
    "    x1 = left_half(I)\n",
    "    x2 = right_half(I)\n",
    "    x3 = as_objects(x2, True, False, True)\n",
    "    x4 = as_objects(x1, True, False, True)\n",
    "    x5 = compose(horizontal_line, center)\n",
    "    x6 = combine_two_function_results(recolor, get_color, x5)\n",
    "    x7 = transform_and_flatten(x6, x4)\n",
    "    x8 = paint_onto_grid(x1, x7)\n",
    "    x9 = transform_and_flatten(x6, x3)\n",
    "    x10 = paint_onto_grid(I, x9)\n",
    "    x11 = as_objects(x8, True, False, True)\n",
    "    x12 = transform(upper_right_corner, x11)\n",
    "    x13 = shift_by_vector(x12, RIGHT)\n",
    "    x14 = flatten(x11)\n",
    "    x15 = paint_onto_grid(x10, x14)\n",
    "    O = fill(x15, COLOR_FIVE, x13)\n",
    "    return O\n",
    "```\n",
    "\n",
    "---------------------------\n",
    "# Task 2:\n",
    "\n",
    "Below are 3 training examples follwed by the test example(s) you have to generalize to, for each example, the input grid is shown first, followed by the output grid. \n",
    "\n",
    "Example 1:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|0|1|0|0|1|1|0|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|1|0|0|1|1|0|1|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|2|1|2|2|1|1|2|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|2|0|0\n",
    "0|0|2|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|2|0|0|0|0|0|0|0|2|0|0\n",
    "0|0|2|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|1|2|2|1|1|2|1|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Example 2:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|1|1|0|0|1|1|0|0|0|0\n",
    "0|0|1|0|0|0|0|0|1|0|0|0|0\n",
    "0|0|0|0|1|0|0|0|0|0|0|0|0\n",
    "0|0|1|0|1|0|0|0|1|0|0|0|0\n",
    "0|0|1|0|0|0|0|0|1|0|0|0|0\n",
    "0|0|0|0|1|0|0|0|1|0|0|0|0\n",
    "0|0|1|1|1|1|0|1|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|1|1|2|2|1|1|0|0|0|0\n",
    "0|0|1|0|2|0|0|0|1|0|0|0|0\n",
    "0|0|2|0|1|0|0|0|2|0|0|0|0\n",
    "0|0|1|0|1|0|0|0|1|0|0|0|0\n",
    "0|0|1|0|2|0|0|0|1|0|0|0|0\n",
    "0|0|2|0|1|0|0|0|1|0|0|0|0\n",
    "0|0|1|1|1|1|2|1|2|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Example 3:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|1|0|1|1|0|1|1|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|1|0|1|0|1|1|0|0|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|1|0|1|1|0|0|1|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|1|2|1|1|2|1|1|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|2|0|0|0|0|0|0|0|2|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|1|2|1|2|1|1|2|2|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|2|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|1|2|1|1|2|2|1|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "\n",
    "============================================================\n",
    "TEST EXAMPLES (to solve)\n",
    "============================================================\n",
    "Below are 1 test example(s) you need to solve:\n",
    "For each test example, only the input grid is provided. You must determine the output.\n",
    "\n",
    "Test Example 1:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|0|1|1|0|1|0|1|1|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|0|1|0|1|0|0|1|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|1|0|0|0|0|0|0|0|1|0|0\n",
    "0|0|1|0|1|1|0|1|0|1|1|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "\n",
    "Output: [TO BE DETERMINED]\n",
    "\n",
    "Solution:\n",
    "```python\n",
    "def solve(I):\n",
    "    x1 = of_color(I, COLOR_ONE)\n",
    "    x2 = box(x1)\n",
    "    x3 = fill(I, COLOR_TWO, x2)\n",
    "    x4 = smallest_subgrid_containing(x1, x3)\n",
    "    x5 = of_color(x4, COLOR_ONE)\n",
    "    x6 = transform_and_flatten(vertical_line, x5)\n",
    "    x7 = transform_and_flatten(horizontal_line, x5)\n",
    "    x8 = size(x6)\n",
    "    x9 = size(x7)\n",
    "    x10 = greater_than(x8, x9)\n",
    "    x11 = condition_if_else(x10, x7, x6)\n",
    "    x12 = fill(x4, COLOR_TWO, x11)\n",
    "    x13 = of_color(x12, COLOR_TWO)\n",
    "    x14 = upper_left_corner(x1)\n",
    "    x15 = shift_by_vector(x13, x14)\n",
    "    O = fill_background(I, COLOR_TWO, x15)\n",
    "    return O\n",
    "```\n",
    "\n",
    "---------------------------\n",
    "# Task 3:\n",
    "\n",
    "Below are 2 training examples follwed by the test example(s) you have to generalize to:\n",
    " for each example, the input grid is shown first, followed by the output grid. \n",
    ".\n",
    "Example 1:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "8|0|0|0|0|0|8|8|8|8|8|8|0|8|8|8|0|8|8|0|8|8|8|0\n",
    "0|0|8|8|8|0|0|0|0|0|0|8|0|0|0|8|0|8|0|0|8|0|8|0\n",
    "8|8|8|0|8|0|8|8|8|8|0|8|8|8|0|8|0|8|8|8|8|0|8|0\n",
    "8|0|0|0|8|0|8|0|0|8|0|0|0|8|0|8|0|0|0|0|0|0|8|0\n",
    "8|0|8|8|8|0|8|8|0|8|0|8|8|8|0|8|8|0|8|8|8|8|8|0\n",
    "8|0|8|0|0|0|0|8|0|8|0|8|0|0|0|0|8|0|8|0|0|0|0|0\n",
    "8|0|8|8|8|8|8|8|0|8|0|8|8|8|8|8|8|3|8|8|8|8|8|0\n",
    "8|0|0|0|0|0|0|0|0|8|0|0|0|0|0|0|3|2|3|0|0|0|8|0\n",
    "8|8|0|8|8|8|0|8|8|8|0|8|8|8|8|8|8|3|8|8|8|0|8|0\n",
    "0|8|0|8|0|8|0|8|0|0|0|8|0|0|0|0|8|0|8|0|8|0|8|0\n",
    "0|8|8|8|0|8|8|8|0|8|8|8|0|8|8|0|8|8|8|0|8|8|8|0\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "8|3|2|3|2|3|8|8|8|8|8|8|0|8|8|8|2|8|8|0|8|8|8|0\n",
    "3|2|8|8|8|2|3|2|3|2|3|8|0|0|0|8|3|8|0|0|8|2|8|0\n",
    "8|8|8|0|8|3|8|8|8|8|2|8|8|8|0|8|2|8|8|8|8|3|8|0\n",
    "8|0|0|0|8|2|8|0|0|8|3|2|3|8|0|8|3|2|3|2|3|2|8|0\n",
    "8|0|8|8|8|3|8|8|0|8|2|8|8|8|0|8|8|3|8|8|8|8|8|0\n",
    "8|0|8|2|3|2|3|8|0|8|3|8|0|0|0|0|8|2|8|0|0|0|0|0\n",
    "8|0|8|8|8|8|8|8|0|8|2|8|8|8|8|8|8|3|8|8|8|8|8|0\n",
    "8|0|0|0|0|0|0|0|0|8|3|2|3|2|3|2|3|2|3|2|3|2|8|0\n",
    "8|8|0|8|8|8|0|8|8|8|2|8|8|8|8|8|8|3|8|8|8|3|8|0\n",
    "0|8|0|8|0|8|0|8|3|2|3|8|0|0|0|0|8|2|8|0|8|2|8|0\n",
    "0|8|8|8|0|8|8|8|2|8|8|8|0|8|8|0|8|8|8|0|8|8|8|0\n",
    "\n",
    "Example 2:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|8|0|0|0|8|0|0|0|0|0|8\n",
    "8|8|0|8|8|8|0|8|0|8|8|8|0|8\n",
    "0|8|0|0|0|8|0|8|0|8|0|8|8|8\n",
    "0|8|8|8|8|8|0|8|0|8|0|0|0|0\n",
    "0|0|0|0|0|0|0|8|0|8|8|8|0|8\n",
    "8|8|8|8|8|8|0|8|0|0|0|8|0|8\n",
    "8|0|0|0|0|8|0|8|8|8|0|8|0|8\n",
    "8|8|8|8|0|8|0|0|0|8|0|8|0|0\n",
    "0|0|0|8|1|8|8|8|8|8|0|8|8|0\n",
    "8|8|0|8|4|1|0|0|0|0|0|0|8|0\n",
    "0|8|0|8|1|8|8|8|8|8|8|8|8|0\n",
    "0|8|8|8|0|8|0|0|0|0|0|0|0|0\n",
    "0|0|0|0|0|8|0|8|8|8|8|8|8|8\n",
    "\n",
    "Output:\n",
    "\n",
    "ASCII representation:\n",
    "0|0|0|8|0|0|0|8|1|4|1|4|1|8\n",
    "8|8|0|8|8|8|0|8|4|8|8|8|4|8\n",
    "0|8|0|0|0|8|0|8|1|8|0|8|8|8\n",
    "0|8|8|8|8|8|0|8|4|8|0|0|0|0\n",
    "0|0|0|0|0|0|0|8|1|8|8|8|0|8\n",
    "8|8|8|8|8|8|0|8|4|1|4|8|0|8\n",
    "8|4|1|4|1|8|0|8|8|8|1|8|0|8\n",
    "8|8|8|8|4|8|0|0|0|8|4|8|0|0\n",
    "0|0|0|8|1|8|8|8|8|8|1|8|8|0\n",
    "8|8|0|8|4|1|4|1|4|1|4|1|8|0\n",
    "1|8|0|8|1|8|8|8|8|8|8|8|8|0\n",
    "4|8|8|8|4|8|0|0|0|0|0|0|0|0\n",
    "1|4|1|4|1|8|0|8|8|8|8|8|8|8\n",
    "\n",
    "\n",
    "============================================================\n",
    "TEST EXAMPLES (to solve)\n",
    "============================================================\n",
    "Below are 1 test example(s) you need to solve:\n",
    "For each test example, only the input grid is provided. You must determine the output.\n",
    "\n",
    "Test Example 1:\n",
    "Input:\n",
    "\n",
    "ASCII representation:\n",
    "8|8|0|8|0|0|8|0|0|0|0|0|0|0|0\n",
    "0|8|0|8|8|8|8|4|8|8|8|8|8|8|8\n",
    "0|8|0|0|0|0|4|3|8|0|0|0|0|0|8\n",
    "0|8|8|8|8|8|8|4|8|8|8|0|8|8|8\n",
    "0|0|0|0|0|0|8|0|0|0|8|0|8|0|0\n",
    "8|8|8|8|8|0|8|8|8|0|8|0|8|0|8\n",
    "0|0|0|0|8|0|0|0|8|0|8|0|8|0|8\n",
    "8|8|8|0|8|8|8|0|8|0|8|0|8|8|8\n",
    "0|0|8|0|0|0|8|0|8|0|8|0|0|0|0\n",
    "8|0|8|8|8|0|8|8|8|0|8|8|8|0|8\n",
    "8|0|0|0|8|0|0|0|0|0|0|0|8|0|8\n",
    "8|8|8|0|8|0|8|8|8|8|8|8|8|0|8\n",
    "0|0|8|0|8|0|8|0|0|0|0|0|0|0|8\n",
    "8|0|8|8|8|0|8|0|8|8|8|8|8|8|8\n",
    "8|0|0|0|0|0|8|0|8|0|0|0|0|0|0\n",
    "\n",
    "Output: [TO BE DETERMINED]\n",
    "\n",
    "Solution:\n",
    "```python\n",
    "def solve_b782dc8a(I):\n",
    "    x1 = least_common_color(I)\n",
    "    x2 = as_objects(I, True, False, False)\n",
    "    x3 = of_color(I, x1)\n",
    "    x4 = get_first(x3)\n",
    "    x5 = direct_neighbors(x4)\n",
    "    x6 = to_object(x5, I)\n",
    "    x7 = most_common_color(x6)\n",
    "    x8 = of_color(I, x7)\n",
    "    x9 = color_filter(x2, COLOR_ZERO)\n",
    "    x10 = fix_last_argument(adjacent, x8)\n",
    "    x11 = keep_if_condition_and_flatten(x9, x10)\n",
    "    x12 = to_indices(x11)\n",
    "    x13 = fix_last_argument(manhattan_distance, x3)\n",
    "    x14 = chain(is_even, x13, initset)\n",
    "    x15 = keep_if_condition(x12, x14)\n",
    "    x16 = difference(x12, x15)\n",
    "    x17 = fill(I, x1, x15)\n",
    "    O = fill(x17, x7, x16)\n",
    "    return O\n",
    "```\n",
    "\n",
    "---------------------------\n",
    "Now you need to solve the following Task:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ceb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_29c11459(I):\n",
    "    x1 = left_half(I)\n",
    "    x2 = right_half(I)\n",
    "    x3 = as_objects(x2, True, False, True)\n",
    "    x4 = as_objects(x1, True, False, True)\n",
    "    x5 = compose(horizontal_line, center)\n",
    "    x6 = combine_two_function_results(recolor, get_color, x5)\n",
    "    x7 = transform_and_flatten(x6, x4)\n",
    "    x8 = paint_onto_grid(x1, x7)\n",
    "    x9 = transform_and_flatten(x6, x3)\n",
    "    x10 = paint_onto_grid(I, x9)\n",
    "    x11 = as_objects(x8, True, False, True)\n",
    "    x12 = transform(upper_right_corner, x11)\n",
    "    x13 = shift_by_vector(x12, RIGHT)\n",
    "    x14 = flatten(x11)\n",
    "    x15 = paint_onto_grid(x10, x14)\n",
    "    O = fill(x15, COLOR_FIVE, x13)\n",
    "    return O\n",
    "\n",
    "\n",
    "def solve_4612dd53(I):\n",
    "    x1 = of_color(I, COLOR_ONE)\n",
    "    x2 = box(x1)\n",
    "    x3 = fill(I, COLOR_TWO, x2)\n",
    "    x4 = smallest_subgrid_containing(x1, x3)\n",
    "    x5 = of_color(x4, COLOR_ONE)\n",
    "    x6 = transform_and_flatten(vertical_line, x5)\n",
    "    x7 = transform_and_flatten(horizontal_line, x5)\n",
    "    x8 = size(x6)\n",
    "    x9 = size(x7)\n",
    "    x10 = greater_than(x8, x9)\n",
    "    x11 = condition_if_else(x10, x7, x6)\n",
    "    x12 = fill(x4, COLOR_TWO, x11)\n",
    "    x13 = of_color(x12, COLOR_TWO)\n",
    "    x14 = upper_left_corner(x1)\n",
    "    x15 = shift_by_vector(x13, x14)\n",
    "    O = fill_background(I, COLOR_TWO, x15)\n",
    "    return O\n",
    "\n",
    "\n",
    "def solve_b782dc8a(I):\n",
    "    x1 = least_common_color(I)\n",
    "    x2 = as_objects(I, True, False, False)\n",
    "    x3 = of_color(I, x1)\n",
    "    x4 = get_first(x3)\n",
    "    x5 = direct_neighbors(x4)\n",
    "    x6 = to_object(x5, I)\n",
    "    x7 = most_common_color(x6)\n",
    "    x8 = of_color(I, x7)\n",
    "    x9 = color_filter(x2, COLOR_ZERO)\n",
    "    x10 = fix_last_argument(adjacent, x8)\n",
    "    x11 = keep_if_condition_and_flatten(x9, x10)\n",
    "    x12 = to_indices(x11)\n",
    "    x13 = fix_last_argument(manhattan_distance, x3)\n",
    "    x14 = chain(is_even, x13, initset)\n",
    "    x15 = keep_if_condition(x12, x14)\n",
    "    x16 = difference(x12, x15)\n",
    "    x17 = fill(I, x1, x15)\n",
    "    O = fill(x17, x7, x16)\n",
    "    return O\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d735f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send 1 / 1 messages\n",
      "[2025-11-07 15:45:35] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 15:45:35] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 15:45:35] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.06, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 15:45:35] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.09, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 15:45:36] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.12, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 15:45:36] Prefill batch, #new-seq: 1, #new-token: 64, #cached-token: 0, token usage: 0.15, #running-req: 0, #queue-req: 0, \n",
      "[2025-11-07 15:45:37] Decode batch, #running-req: 1, #token: 10337, token usage: 0.16, cuda graph: True, gen throughput (token/s): 0.36, #queue-req: 0, \n",
      "[2025-11-07 15:45:38] Decode batch, #running-req: 1, #token: 10377, token usage: 0.16, cuda graph: True, gen throughput (token/s): 63.06, #queue-req: 0, \n",
      "[2025-11-07 15:45:38] Decode batch, #running-req: 1, #token: 10417, token usage: 0.16, cuda graph: True, gen throughput (token/s): 64.52, #queue-req: 0, \n",
      "[2025-11-07 15:45:39] Decode batch, #running-req: 1, #token: 10457, token usage: 0.16, cuda graph: True, gen throughput (token/s): 63.80, #queue-req: 0, \n",
      "[2025-11-07 15:45:39] Decode batch, #running-req: 1, #token: 10497, token usage: 0.16, cuda graph: True, gen throughput (token/s): 64.59, #queue-req: 0, \n",
      "[2025-11-07 15:45:40] Decode batch, #running-req: 1, #token: 10537, token usage: 0.16, cuda graph: True, gen throughput (token/s): 63.25, #queue-req: 0, \n",
      "[2025-11-07 15:45:41] Decode batch, #running-req: 1, #token: 10577, token usage: 0.16, cuda graph: True, gen throughput (token/s): 63.79, #queue-req: 0, \n",
      "[2025-11-07 15:45:41] Decode batch, #running-req: 1, #token: 10617, token usage: 0.16, cuda graph: True, gen throughput (token/s): 64.19, #queue-req: 0, \n",
      "[2025-11-07 15:45:42] Decode batch, #running-req: 1, #token: 10657, token usage: 0.16, cuda graph: True, gen throughput (token/s): 62.66, #queue-req: 0, \n",
      "[2025-11-07 15:45:43] Decode batch, #running-req: 1, #token: 10697, token usage: 0.16, cuda graph: True, gen throughput (token/s): 63.11, #queue-req: 0, \n",
      "[2025-11-07 15:45:43] Decode batch, #running-req: 1, #token: 10737, token usage: 0.16, cuda graph: True, gen throughput (token/s): 66.77, #queue-req: 0, \n",
      "[2025-11-07 15:45:44] Decode batch, #running-req: 1, #token: 10777, token usage: 0.16, cuda graph: True, gen throughput (token/s): 69.31, #queue-req: 0, \n",
      "[2025-11-07 15:45:44] Decode batch, #running-req: 1, #token: 10817, token usage: 0.16, cuda graph: True, gen throughput (token/s): 73.52, #queue-req: 0, \n",
      "[2025-11-07 15:45:45] Decode batch, #running-req: 1, #token: 10857, token usage: 0.16, cuda graph: True, gen throughput (token/s): 74.52, #queue-req: 0, \n",
      "[2025-11-07 15:45:45] Decode batch, #running-req: 1, #token: 10897, token usage: 0.16, cuda graph: True, gen throughput (token/s): 74.17, #queue-req: 0, \n",
      "[2025-11-07 15:45:46] Decode batch, #running-req: 1, #token: 10937, token usage: 0.16, cuda graph: True, gen throughput (token/s): 74.90, #queue-req: 0, \n",
      "[2025-11-07 15:45:46] Decode batch, #running-req: 1, #token: 10977, token usage: 0.16, cuda graph: True, gen throughput (token/s): 74.25, #queue-req: 0, \n",
      "[2025-11-07 15:45:47] Decode batch, #running-req: 1, #token: 11017, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.91, #queue-req: 0, \n",
      "[2025-11-07 15:45:48] Decode batch, #running-req: 1, #token: 11057, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.71, #queue-req: 0, \n",
      "[2025-11-07 15:45:48] Decode batch, #running-req: 1, #token: 11097, token usage: 0.17, cuda graph: True, gen throughput (token/s): 75.26, #queue-req: 0, \n",
      "[2025-11-07 15:45:49] Decode batch, #running-req: 1, #token: 11137, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.91, #queue-req: 0, \n",
      "[2025-11-07 15:45:49] Decode batch, #running-req: 1, #token: 11177, token usage: 0.17, cuda graph: True, gen throughput (token/s): 75.26, #queue-req: 0, \n",
      "[2025-11-07 15:45:50] Decode batch, #running-req: 1, #token: 11217, token usage: 0.17, cuda graph: True, gen throughput (token/s): 75.01, #queue-req: 0, \n",
      "[2025-11-07 15:45:50] Decode batch, #running-req: 1, #token: 11257, token usage: 0.17, cuda graph: True, gen throughput (token/s): 75.22, #queue-req: 0, \n",
      "[2025-11-07 15:45:51] Decode batch, #running-req: 1, #token: 11297, token usage: 0.17, cuda graph: True, gen throughput (token/s): 75.15, #queue-req: 0, \n",
      "[2025-11-07 15:45:51] Decode batch, #running-req: 1, #token: 11337, token usage: 0.17, cuda graph: True, gen throughput (token/s): 75.08, #queue-req: 0, \n",
      "[2025-11-07 15:45:52] Decode batch, #running-req: 1, #token: 11377, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.80, #queue-req: 0, \n",
      "[2025-11-07 15:45:52] Decode batch, #running-req: 1, #token: 11417, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.78, #queue-req: 0, \n",
      "[2025-11-07 15:45:53] Decode batch, #running-req: 1, #token: 11457, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.94, #queue-req: 0, \n",
      "[2025-11-07 15:45:53] Decode batch, #running-req: 1, #token: 11497, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.69, #queue-req: 0, \n",
      "[2025-11-07 15:45:54] Decode batch, #running-req: 1, #token: 11537, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.79, #queue-req: 0, \n",
      "[2025-11-07 15:45:54] Decode batch, #running-req: 1, #token: 11577, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.07, #queue-req: 0, \n",
      "[2025-11-07 15:45:55] Decode batch, #running-req: 1, #token: 11617, token usage: 0.17, cuda graph: True, gen throughput (token/s): 74.16, #queue-req: 0, \n",
      "[2025-11-07 15:45:56] Decode batch, #running-req: 1, #token: 11657, token usage: 0.17, cuda graph: True, gen throughput (token/s): 73.91, #queue-req: 0, \n",
      "[2025-11-07 15:45:56] Decode batch, #running-req: 1, #token: 11697, token usage: 0.18, cuda graph: True, gen throughput (token/s): 74.17, #queue-req: 0, \n",
      "[2025-11-07 15:45:57] Decode batch, #running-req: 1, #token: 11737, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.88, #queue-req: 0, \n",
      "[2025-11-07 15:45:57] Decode batch, #running-req: 1, #token: 11777, token usage: 0.18, cuda graph: True, gen throughput (token/s): 74.28, #queue-req: 0, \n",
      "[2025-11-07 15:45:58] Decode batch, #running-req: 1, #token: 11817, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.88, #queue-req: 0, \n",
      "[2025-11-07 15:45:58] Decode batch, #running-req: 1, #token: 11857, token usage: 0.18, cuda graph: True, gen throughput (token/s): 74.12, #queue-req: 0, \n",
      "[2025-11-07 15:45:59] Decode batch, #running-req: 1, #token: 11897, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.89, #queue-req: 0, \n",
      "[2025-11-07 15:45:59] Decode batch, #running-req: 1, #token: 11937, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.97, #queue-req: 0, \n",
      "[2025-11-07 15:46:00] Decode batch, #running-req: 1, #token: 11977, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.83, #queue-req: 0, \n",
      "[2025-11-07 15:46:00] Decode batch, #running-req: 1, #token: 12017, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.41, #queue-req: 0, \n",
      "[2025-11-07 15:46:01] Decode batch, #running-req: 1, #token: 12057, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.27, #queue-req: 0, \n",
      "[2025-11-07 15:46:02] Decode batch, #running-req: 1, #token: 12097, token usage: 0.18, cuda graph: True, gen throughput (token/s): 72.89, #queue-req: 0, \n",
      "[2025-11-07 15:46:02] Decode batch, #running-req: 1, #token: 12137, token usage: 0.18, cuda graph: True, gen throughput (token/s): 73.00, #queue-req: 0, \n",
      "[2025-11-07 15:46:03] Decode batch, #running-req: 1, #token: 12177, token usage: 0.18, cuda graph: True, gen throughput (token/s): 72.95, #queue-req: 0, \n",
      "[2025-11-07 15:46:03] Decode batch, #running-req: 1, #token: 12217, token usage: 0.18, cuda graph: True, gen throughput (token/s): 72.87, #queue-req: 0, \n",
      "[2025-11-07 15:46:04] Decode batch, #running-req: 1, #token: 12257, token usage: 0.18, cuda graph: True, gen throughput (token/s): 72.92, #queue-req: 0, \n",
      "[2025-11-07 15:46:04] Decode batch, #running-req: 1, #token: 12297, token usage: 0.18, cuda graph: True, gen throughput (token/s): 72.84, #queue-req: 0, \n",
      "[2025-11-07 15:46:05] Decode batch, #running-req: 1, #token: 12337, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.83, #queue-req: 0, \n",
      "[2025-11-07 15:46:05] Decode batch, #running-req: 1, #token: 12377, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.88, #queue-req: 0, \n",
      "[2025-11-07 15:46:06] Decode batch, #running-req: 1, #token: 12417, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.96, #queue-req: 0, \n",
      "[2025-11-07 15:46:06] Decode batch, #running-req: 1, #token: 12457, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.90, #queue-req: 0, \n",
      "[2025-11-07 15:46:07] Decode batch, #running-req: 1, #token: 12497, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.73, #queue-req: 0, \n",
      "[2025-11-07 15:46:08] Decode batch, #running-req: 1, #token: 12537, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.75, #queue-req: 0, \n",
      "[2025-11-07 15:46:08] Decode batch, #running-req: 1, #token: 12577, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.85, #queue-req: 0, \n",
      "[2025-11-07 15:46:09] Decode batch, #running-req: 1, #token: 12617, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.80, #queue-req: 0, \n",
      "[2025-11-07 15:46:09] Decode batch, #running-req: 1, #token: 12657, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.73, #queue-req: 0, \n",
      "[2025-11-07 15:46:10] Decode batch, #running-req: 1, #token: 12697, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.80, #queue-req: 0, \n",
      "[2025-11-07 15:46:10] Decode batch, #running-req: 1, #token: 12737, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.69, #queue-req: 0, \n",
      "[2025-11-07 15:46:11] Decode batch, #running-req: 1, #token: 12777, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.53, #queue-req: 0, \n",
      "[2025-11-07 15:46:11] Decode batch, #running-req: 1, #token: 12817, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.53, #queue-req: 0, \n",
      "[2025-11-07 15:46:12] Decode batch, #running-req: 1, #token: 12857, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.57, #queue-req: 0, \n",
      "[2025-11-07 15:46:12] Decode batch, #running-req: 1, #token: 12897, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.43, #queue-req: 0, \n",
      "[2025-11-07 15:46:13] Decode batch, #running-req: 1, #token: 12937, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.19, #queue-req: 0, \n",
      "[2025-11-07 15:46:14] Decode batch, #running-req: 1, #token: 12977, token usage: 0.19, cuda graph: True, gen throughput (token/s): 72.23, #queue-req: 0, \n",
      "[2025-11-07 15:46:14] Decode batch, #running-req: 1, #token: 13017, token usage: 0.20, cuda graph: True, gen throughput (token/s): 72.15, #queue-req: 0, \n",
      "[2025-11-07 15:46:15] Decode batch, #running-req: 1, #token: 13057, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.85, #queue-req: 0, \n",
      "[2025-11-07 15:46:15] Decode batch, #running-req: 1, #token: 13097, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.81, #queue-req: 0, \n",
      "[2025-11-07 15:46:16] Decode batch, #running-req: 1, #token: 13137, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.85, #queue-req: 0, \n",
      "[2025-11-07 15:46:16] Decode batch, #running-req: 1, #token: 13177, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.97, #queue-req: 0, \n",
      "[2025-11-07 15:46:17] Decode batch, #running-req: 1, #token: 13217, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.88, #queue-req: 0, \n",
      "[2025-11-07 15:46:18] Decode batch, #running-req: 1, #token: 13257, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.67, #queue-req: 0, \n",
      "[2025-11-07 15:46:18] Decode batch, #running-req: 1, #token: 13297, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.63, #queue-req: 0, \n",
      "[2025-11-07 15:46:19] Decode batch, #running-req: 1, #token: 13337, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.55, #queue-req: 0, \n",
      "[2025-11-07 15:46:19] Decode batch, #running-req: 1, #token: 13377, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.46, #queue-req: 0, \n",
      "[2025-11-07 15:46:20] Decode batch, #running-req: 1, #token: 13417, token usage: 0.20, cuda graph: True, gen throughput (token/s): 71.03, #queue-req: 0, \n",
      "[2025-11-07 15:46:20] Decode batch, #running-req: 1, #token: 13457, token usage: 0.20, cuda graph: True, gen throughput (token/s): 70.97, #queue-req: 0, \n",
      "[2025-11-07 15:46:21] Decode batch, #running-req: 1, #token: 13497, token usage: 0.20, cuda graph: True, gen throughput (token/s): 70.93, #queue-req: 0, \n",
      "[2025-11-07 15:46:21] Decode batch, #running-req: 1, #token: 13537, token usage: 0.20, cuda graph: True, gen throughput (token/s): 70.89, #queue-req: 0, \n",
      "[2025-11-07 15:46:22] Decode batch, #running-req: 1, #token: 13577, token usage: 0.20, cuda graph: True, gen throughput (token/s): 70.79, #queue-req: 0, \n",
      "[2025-11-07 15:46:23] Decode batch, #running-req: 1, #token: 13617, token usage: 0.20, cuda graph: True, gen throughput (token/s): 70.79, #queue-req: 0, \n",
      "[2025-11-07 15:46:23] Decode batch, #running-req: 1, #token: 13657, token usage: 0.20, cuda graph: True, gen throughput (token/s): 70.87, #queue-req: 0, \n",
      "[2025-11-07 15:46:24] Decode batch, #running-req: 1, #token: 13697, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.65, #queue-req: 0, \n",
      "[2025-11-07 15:46:24] Decode batch, #running-req: 1, #token: 13737, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.45, #queue-req: 0, \n",
      "[2025-11-07 15:46:25] Decode batch, #running-req: 1, #token: 13777, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.42, #queue-req: 0, \n",
      "[2025-11-07 15:46:25] Decode batch, #running-req: 1, #token: 13817, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.40, #queue-req: 0, \n",
      "[2025-11-07 15:46:26] Decode batch, #running-req: 1, #token: 13857, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.35, #queue-req: 0, \n",
      "[2025-11-07 15:46:27] Decode batch, #running-req: 1, #token: 13897, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.55, #queue-req: 0, \n",
      "[2025-11-07 15:46:27] Decode batch, #running-req: 1, #token: 13937, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.41, #queue-req: 0, \n",
      "[2025-11-07 15:46:28] Decode batch, #running-req: 1, #token: 13977, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.51, #queue-req: 0, \n",
      "[2025-11-07 15:46:28] Decode batch, #running-req: 1, #token: 14017, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.18, #queue-req: 0, \n",
      "[2025-11-07 15:46:29] Decode batch, #running-req: 1, #token: 14057, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.08, #queue-req: 0, \n",
      "[2025-11-07 15:46:29] Decode batch, #running-req: 1, #token: 14097, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.09, #queue-req: 0, \n",
      "[2025-11-07 15:46:30] Decode batch, #running-req: 1, #token: 14137, token usage: 0.21, cuda graph: True, gen throughput (token/s): 69.98, #queue-req: 0, \n",
      "[2025-11-07 15:46:31] Decode batch, #running-req: 1, #token: 14177, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.26, #queue-req: 0, \n",
      "[2025-11-07 15:46:31] Decode batch, #running-req: 1, #token: 14217, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.13, #queue-req: 0, \n",
      "[2025-11-07 15:46:32] Decode batch, #running-req: 1, #token: 14257, token usage: 0.21, cuda graph: True, gen throughput (token/s): 70.10, #queue-req: 0, \n",
      "[2025-11-07 15:46:32] Decode batch, #running-req: 1, #token: 14297, token usage: 0.21, cuda graph: True, gen throughput (token/s): 69.85, #queue-req: 0, \n",
      "[2025-11-07 15:46:33] Decode batch, #running-req: 1, #token: 14337, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.90, #queue-req: 0, \n",
      "[2025-11-07 15:46:33] Decode batch, #running-req: 1, #token: 14377, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.95, #queue-req: 0, \n",
      "[2025-11-07 15:46:34] Decode batch, #running-req: 1, #token: 14417, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.72, #queue-req: 0, \n",
      "[2025-11-07 15:46:35] Decode batch, #running-req: 1, #token: 14457, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.73, #queue-req: 0, \n",
      "[2025-11-07 15:46:35] Decode batch, #running-req: 1, #token: 14497, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.43, #queue-req: 0, \n",
      "[2025-11-07 15:46:36] Decode batch, #running-req: 1, #token: 14537, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.53, #queue-req: 0, \n",
      "[2025-11-07 15:46:36] Decode batch, #running-req: 1, #token: 14577, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.33, #queue-req: 0, \n",
      "[2025-11-07 15:46:37] Decode batch, #running-req: 1, #token: 14617, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.06, #queue-req: 0, \n",
      "[2025-11-07 15:46:37] Decode batch, #running-req: 1, #token: 14657, token usage: 0.22, cuda graph: True, gen throughput (token/s): 69.11, #queue-req: 0, \n",
      "[2025-11-07 15:46:38] Decode batch, #running-req: 1, #token: 14697, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.54, #queue-req: 0, \n",
      "[2025-11-07 15:46:39] Decode batch, #running-req: 1, #token: 14737, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.82, #queue-req: 0, \n",
      "[2025-11-07 15:46:39] Decode batch, #running-req: 1, #token: 14777, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, \n",
      "[2025-11-07 15:46:40] Decode batch, #running-req: 1, #token: 14817, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.80, #queue-req: 0, \n",
      "[2025-11-07 15:46:40] Decode batch, #running-req: 1, #token: 14857, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, \n",
      "[2025-11-07 15:46:41] Decode batch, #running-req: 1, #token: 14897, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.54, #queue-req: 0, \n",
      "[2025-11-07 15:46:41] Decode batch, #running-req: 1, #token: 14937, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, \n",
      "[2025-11-07 15:46:42] Decode batch, #running-req: 1, #token: 14977, token usage: 0.22, cuda graph: True, gen throughput (token/s): 68.27, #queue-req: 0, \n",
      "[2025-11-07 15:46:43] Decode batch, #running-req: 1, #token: 15017, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.53, #queue-req: 0, \n",
      "[2025-11-07 15:46:43] Decode batch, #running-req: 1, #token: 15057, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, \n",
      "[2025-11-07 15:46:44] Decode batch, #running-req: 1, #token: 15097, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.40, #queue-req: 0, \n",
      "[2025-11-07 15:46:44] Decode batch, #running-req: 1, #token: 15137, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.37, #queue-req: 0, \n",
      "[2025-11-07 15:46:45] Decode batch, #running-req: 1, #token: 15177, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.42, #queue-req: 0, \n",
      "[2025-11-07 15:46:46] Decode batch, #running-req: 1, #token: 15217, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, \n",
      "[2025-11-07 15:46:46] Decode batch, #running-req: 1, #token: 15257, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.19, #queue-req: 0, \n",
      "[2025-11-07 15:46:47] Decode batch, #running-req: 1, #token: 15297, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.29, #queue-req: 0, \n",
      "[2025-11-07 15:46:47] Decode batch, #running-req: 1, #token: 15337, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.19, #queue-req: 0, \n",
      "[2025-11-07 15:46:48] Decode batch, #running-req: 1, #token: 15377, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.03, #queue-req: 0, \n",
      "[2025-11-07 15:46:49] Decode batch, #running-req: 1, #token: 15417, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.08, #queue-req: 0, \n",
      "[2025-11-07 15:46:49] Decode batch, #running-req: 1, #token: 15457, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.09, #queue-req: 0, \n",
      "[2025-11-07 15:46:50] Decode batch, #running-req: 1, #token: 15497, token usage: 0.23, cuda graph: True, gen throughput (token/s): 68.02, #queue-req: 0, \n",
      "[2025-11-07 15:46:50] Decode batch, #running-req: 1, #token: 15537, token usage: 0.23, cuda graph: True, gen throughput (token/s): 67.98, #queue-req: 0, \n",
      "[2025-11-07 15:46:51] Decode batch, #running-req: 1, #token: 15577, token usage: 0.23, cuda graph: True, gen throughput (token/s): 67.92, #queue-req: 0, \n",
      "[2025-11-07 15:46:51] Decode batch, #running-req: 1, #token: 15617, token usage: 0.23, cuda graph: True, gen throughput (token/s): 67.63, #queue-req: 0, \n",
      "[2025-11-07 15:46:52] Decode batch, #running-req: 1, #token: 15657, token usage: 0.23, cuda graph: True, gen throughput (token/s): 67.66, #queue-req: 0, \n",
      "[2025-11-07 15:46:53] Decode batch, #running-req: 1, #token: 15697, token usage: 0.24, cuda graph: True, gen throughput (token/s): 67.41, #queue-req: 0, \n",
      "[2025-11-07 15:46:53] Decode batch, #running-req: 1, #token: 15737, token usage: 0.24, cuda graph: True, gen throughput (token/s): 67.40, #queue-req: 0, \n",
      "[2025-11-07 15:46:54] Decode batch, #running-req: 1, #token: 15777, token usage: 0.24, cuda graph: True, gen throughput (token/s): 67.27, #queue-req: 0, \n",
      "[2025-11-07 15:46:54] Decode batch, #running-req: 1, #token: 15817, token usage: 0.24, cuda graph: True, gen throughput (token/s): 67.10, #queue-req: 0, \n",
      "[2025-11-07 15:46:55] Decode batch, #running-req: 1, #token: 15857, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.84, #queue-req: 0, \n",
      "[2025-11-07 15:46:56] Decode batch, #running-req: 1, #token: 15897, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.81, #queue-req: 0, \n",
      "[2025-11-07 15:46:56] Decode batch, #running-req: 1, #token: 15937, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.82, #queue-req: 0, \n",
      "[2025-11-07 15:46:57] Decode batch, #running-req: 1, #token: 15977, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.80, #queue-req: 0, \n",
      "[2025-11-07 15:46:57] Decode batch, #running-req: 1, #token: 16017, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.73, #queue-req: 0, \n",
      "[2025-11-07 15:46:58] Decode batch, #running-req: 1, #token: 16057, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.62, #queue-req: 0, \n",
      "[2025-11-07 15:46:59] Decode batch, #running-req: 1, #token: 16097, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.67, #queue-req: 0, \n",
      "[2025-11-07 15:46:59] Decode batch, #running-req: 1, #token: 16137, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.79, #queue-req: 0, \n",
      "[2025-11-07 15:47:00] Decode batch, #running-req: 1, #token: 16177, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.53, #queue-req: 0, \n",
      "[2025-11-07 15:47:00] Decode batch, #running-req: 1, #token: 16217, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.72, #queue-req: 0, \n",
      "[2025-11-07 15:47:01] Decode batch, #running-req: 1, #token: 16257, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.57, #queue-req: 0, \n",
      "[2025-11-07 15:47:02] Decode batch, #running-req: 1, #token: 16297, token usage: 0.24, cuda graph: True, gen throughput (token/s): 66.62, #queue-req: 0, \n",
      "[2025-11-07 15:47:02] Decode batch, #running-req: 1, #token: 16337, token usage: 0.25, cuda graph: True, gen throughput (token/s): 66.53, #queue-req: 0, \n",
      "[2025-11-07 15:47:03] Decode batch, #running-req: 1, #token: 16377, token usage: 0.25, cuda graph: True, gen throughput (token/s): 66.41, #queue-req: 0, \n",
      "[2025-11-07 15:47:03] Decode batch, #running-req: 1, #token: 16417, token usage: 0.25, cuda graph: True, gen throughput (token/s): 66.42, #queue-req: 0, \n",
      "[2025-11-07 15:47:04] Decode batch, #running-req: 1, #token: 16457, token usage: 0.25, cuda graph: True, gen throughput (token/s): 66.34, #queue-req: 0, \n",
      "[2025-11-07 15:47:05] Decode batch, #running-req: 1, #token: 16497, token usage: 0.25, cuda graph: True, gen throughput (token/s): 66.25, #queue-req: 0, \n",
      "[2025-11-07 15:47:05] Decode batch, #running-req: 1, #token: 16537, token usage: 0.25, cuda graph: True, gen throughput (token/s): 66.10, #queue-req: 0, \n",
      "[2025-11-07 15:47:06] Decode batch, #running-req: 1, #token: 16577, token usage: 0.25, cuda graph: True, gen throughput (token/s): 65.93, #queue-req: 0, \n",
      "[2025-11-07 15:47:06] Decode batch, #running-req: 1, #token: 16617, token usage: 0.25, cuda graph: True, gen throughput (token/s): 65.92, #queue-req: 0, \n",
      "[2025-11-07 15:47:07] Decode batch, #running-req: 1, #token: 16657, token usage: 0.25, cuda graph: True, gen throughput (token/s): 65.35, #queue-req: 0, \n",
      "[2025-11-07 15:47:08] Decode batch, #running-req: 1, #token: 16697, token usage: 0.25, cuda graph: True, gen throughput (token/s): 65.29, #queue-req: 0, \n",
      "[2025-11-07 15:47:08] Decode batch, #running-req: 1, #token: 16737, token usage: 0.25, cuda graph: True, gen throughput (token/s): 63.70, #queue-req: 0, \n",
      "[2025-11-07 15:47:09] Decode batch, #running-req: 1, #token: 16777, token usage: 0.25, cuda graph: True, gen throughput (token/s): 62.81, #queue-req: 0, \n",
      "[2025-11-07 15:47:10] Decode batch, #running-req: 1, #token: 16817, token usage: 0.25, cuda graph: True, gen throughput (token/s): 63.35, #queue-req: 0, \n",
      "[2025-11-07 15:47:10] Decode batch, #running-req: 1, #token: 16857, token usage: 0.25, cuda graph: True, gen throughput (token/s): 64.43, #queue-req: 0, \n",
      "[2025-11-07 15:47:11] Decode batch, #running-req: 1, #token: 16897, token usage: 0.25, cuda graph: True, gen throughput (token/s): 64.80, #queue-req: 0, \n",
      "[2025-11-07 15:47:11] Decode batch, #running-req: 1, #token: 16937, token usage: 0.25, cuda graph: True, gen throughput (token/s): 64.83, #queue-req: 0, \n",
      "[2025-11-07 15:47:12] Decode batch, #running-req: 1, #token: 16977, token usage: 0.25, cuda graph: True, gen throughput (token/s): 64.93, #queue-req: 0, \n",
      "[2025-11-07 15:47:13] Decode batch, #running-req: 1, #token: 17017, token usage: 0.26, cuda graph: True, gen throughput (token/s): 65.15, #queue-req: 0, \n",
      "[2025-11-07 15:47:13] Decode batch, #running-req: 1, #token: 17057, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.93, #queue-req: 0, \n",
      "[2025-11-07 15:47:14] Decode batch, #running-req: 1, #token: 17097, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.80, #queue-req: 0, \n",
      "[2025-11-07 15:47:15] Decode batch, #running-req: 1, #token: 17137, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.89, #queue-req: 0, \n",
      "[2025-11-07 15:47:15] Decode batch, #running-req: 1, #token: 17177, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.57, #queue-req: 0, \n",
      "[2025-11-07 15:47:16] Decode batch, #running-req: 1, #token: 17217, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.69, #queue-req: 0, \n",
      "[2025-11-07 15:47:16] Decode batch, #running-req: 1, #token: 17257, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.68, #queue-req: 0, \n",
      "[2025-11-07 15:47:17] Decode batch, #running-req: 1, #token: 17297, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.76, #queue-req: 0, \n",
      "[2025-11-07 15:47:18] Decode batch, #running-req: 1, #token: 17337, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.99, #queue-req: 0, \n",
      "[2025-11-07 15:47:18] Decode batch, #running-req: 1, #token: 17377, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.86, #queue-req: 0, \n",
      "[2025-11-07 15:47:19] Decode batch, #running-req: 1, #token: 17417, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.90, #queue-req: 0, \n",
      "[2025-11-07 15:47:19] Decode batch, #running-req: 1, #token: 17457, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.72, #queue-req: 0, \n",
      "[2025-11-07 15:47:20] Decode batch, #running-req: 1, #token: 17497, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.61, #queue-req: 0, \n",
      "[2025-11-07 15:47:21] Decode batch, #running-req: 1, #token: 17537, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.71, #queue-req: 0, \n",
      "[2025-11-07 15:47:21] Decode batch, #running-req: 1, #token: 17577, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.79, #queue-req: 0, \n",
      "[2025-11-07 15:47:22] Decode batch, #running-req: 1, #token: 17617, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.61, #queue-req: 0, \n",
      "[2025-11-07 15:47:23] Decode batch, #running-req: 1, #token: 17657, token usage: 0.26, cuda graph: True, gen throughput (token/s): 64.65, #queue-req: 0, \n",
      "[2025-11-07 15:47:23] Decode batch, #running-req: 1, #token: 17697, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.74, #queue-req: 0, \n",
      "[2025-11-07 15:47:24] Decode batch, #running-req: 1, #token: 17737, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.57, #queue-req: 0, \n",
      "[2025-11-07 15:47:24] Decode batch, #running-req: 1, #token: 17777, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.45, #queue-req: 0, \n",
      "[2025-11-07 15:47:25] Decode batch, #running-req: 1, #token: 17817, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.21, #queue-req: 0, \n",
      "[2025-11-07 15:47:26] Decode batch, #running-req: 1, #token: 17857, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.52, #queue-req: 0, \n",
      "[2025-11-07 15:47:26] Decode batch, #running-req: 1, #token: 17897, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.37, #queue-req: 0, \n",
      "[2025-11-07 15:47:27] Decode batch, #running-req: 1, #token: 17937, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.24, #queue-req: 0, \n",
      "[2025-11-07 15:47:28] Decode batch, #running-req: 1, #token: 17977, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.07, #queue-req: 0, \n",
      "[2025-11-07 15:47:28] Decode batch, #running-req: 1, #token: 18017, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.14, #queue-req: 0, \n",
      "[2025-11-07 15:47:29] Decode batch, #running-req: 1, #token: 18057, token usage: 0.27, cuda graph: True, gen throughput (token/s): 64.14, #queue-req: 0, \n",
      "[2025-11-07 15:47:29] Decode batch, #running-req: 1, #token: 18097, token usage: 0.27, cuda graph: True, gen throughput (token/s): 63.02, #queue-req: 0, \n",
      "[2025-11-07 15:47:30] Decode batch, #running-req: 1, #token: 18137, token usage: 0.27, cuda graph: True, gen throughput (token/s): 61.14, #queue-req: 0, \n",
      "[2025-11-07 15:47:31] Decode batch, #running-req: 1, #token: 18177, token usage: 0.27, cuda graph: True, gen throughput (token/s): 58.16, #queue-req: 0, \n",
      "[2025-11-07 15:47:31] Decode batch, #running-req: 1, #token: 18217, token usage: 0.27, cuda graph: True, gen throughput (token/s): 58.20, #queue-req: 0, \n",
      "[2025-11-07 15:47:32] Decode batch, #running-req: 1, #token: 18257, token usage: 0.27, cuda graph: True, gen throughput (token/s): 58.37, #queue-req: 0, \n",
      "[2025-11-07 15:47:33] Decode batch, #running-req: 1, #token: 18297, token usage: 0.27, cuda graph: True, gen throughput (token/s): 58.48, #queue-req: 0, \n",
      "[2025-11-07 15:47:33] Decode batch, #running-req: 1, #token: 18337, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.63, #queue-req: 0, \n",
      "[2025-11-07 15:47:34] Decode batch, #running-req: 1, #token: 18377, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.86, #queue-req: 0, \n",
      "[2025-11-07 15:47:35] Decode batch, #running-req: 1, #token: 18417, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.90, #queue-req: 0, \n",
      "[2025-11-07 15:47:36] Decode batch, #running-req: 1, #token: 18457, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.90, #queue-req: 0, \n",
      "[2025-11-07 15:47:36] Decode batch, #running-req: 1, #token: 18497, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.83, #queue-req: 0, \n",
      "[2025-11-07 15:47:37] Decode batch, #running-req: 1, #token: 18537, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.69, #queue-req: 0, \n",
      "[2025-11-07 15:47:38] Decode batch, #running-req: 1, #token: 18577, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.55, #queue-req: 0, \n",
      "[2025-11-07 15:47:38] Decode batch, #running-req: 1, #token: 18617, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.27, #queue-req: 0, \n",
      "[2025-11-07 15:47:39] Decode batch, #running-req: 1, #token: 18657, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.09, #queue-req: 0, \n",
      "[2025-11-07 15:47:40] Decode batch, #running-req: 1, #token: 18697, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.31, #queue-req: 0, \n",
      "[2025-11-07 15:47:40] Decode batch, #running-req: 1, #token: 18737, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.54, #queue-req: 0, \n",
      "[2025-11-07 15:47:41] Decode batch, #running-req: 1, #token: 18777, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.20, #queue-req: 0, \n",
      "[2025-11-07 15:47:42] Decode batch, #running-req: 1, #token: 18817, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.22, #queue-req: 0, \n",
      "[2025-11-07 15:47:42] Decode batch, #running-req: 1, #token: 18857, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.35, #queue-req: 0, \n",
      "[2025-11-07 15:47:43] Decode batch, #running-req: 1, #token: 18897, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.43, #queue-req: 0, \n",
      "[2025-11-07 15:47:44] Decode batch, #running-req: 1, #token: 18937, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.38, #queue-req: 0, \n",
      "[2025-11-07 15:47:44] Decode batch, #running-req: 1, #token: 18977, token usage: 0.28, cuda graph: True, gen throughput (token/s): 58.39, #queue-req: 0, \n",
      "[2025-11-07 15:47:45] Decode batch, #running-req: 1, #token: 19017, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.21, #queue-req: 0, \n",
      "[2025-11-07 15:47:46] Decode batch, #running-req: 1, #token: 19057, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.16, #queue-req: 0, \n",
      "[2025-11-07 15:47:46] Decode batch, #running-req: 1, #token: 19097, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.35, #queue-req: 0, \n",
      "[2025-11-07 15:47:47] Decode batch, #running-req: 1, #token: 19137, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.27, #queue-req: 0, \n",
      "[2025-11-07 15:47:48] Decode batch, #running-req: 1, #token: 19177, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.28, #queue-req: 0, \n",
      "[2025-11-07 15:47:49] Decode batch, #running-req: 1, #token: 19217, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.44, #queue-req: 0, \n",
      "[2025-11-07 15:47:49] Decode batch, #running-req: 1, #token: 19257, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.91, #queue-req: 0, \n",
      "[2025-11-07 15:47:50] Decode batch, #running-req: 1, #token: 19297, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.95, #queue-req: 0, \n",
      "[2025-11-07 15:47:51] Decode batch, #running-req: 1, #token: 19337, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.94, #queue-req: 0, \n",
      "[2025-11-07 15:47:51] Decode batch, #running-req: 1, #token: 19377, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.00, #queue-req: 0, \n",
      "[2025-11-07 15:47:52] Decode batch, #running-req: 1, #token: 19417, token usage: 0.29, cuda graph: True, gen throughput (token/s): 58.05, #queue-req: 0, \n",
      "[2025-11-07 15:47:53] Decode batch, #running-req: 1, #token: 19457, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.94, #queue-req: 0, \n",
      "[2025-11-07 15:47:53] Decode batch, #running-req: 1, #token: 19497, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.73, #queue-req: 0, \n",
      "[2025-11-07 15:47:54] Decode batch, #running-req: 1, #token: 19537, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.82, #queue-req: 0, \n",
      "[2025-11-07 15:47:55] Decode batch, #running-req: 1, #token: 19577, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.71, #queue-req: 0, \n",
      "[2025-11-07 15:47:55] Decode batch, #running-req: 1, #token: 19617, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.68, #queue-req: 0, \n",
      "[2025-11-07 15:47:56] Decode batch, #running-req: 1, #token: 19657, token usage: 0.29, cuda graph: True, gen throughput (token/s): 57.62, #queue-req: 0, \n",
      "[2025-11-07 15:47:57] Decode batch, #running-req: 1, #token: 19697, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.35, #queue-req: 0, \n",
      "[2025-11-07 15:47:58] Decode batch, #running-req: 1, #token: 19737, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.44, #queue-req: 0, \n",
      "[2025-11-07 15:47:58] Decode batch, #running-req: 1, #token: 19777, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.52, #queue-req: 0, \n",
      "[2025-11-07 15:47:59] Decode batch, #running-req: 1, #token: 19817, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.42, #queue-req: 0, \n",
      "[2025-11-07 15:48:00] Decode batch, #running-req: 1, #token: 19857, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.56, #queue-req: 0, \n",
      "[2025-11-07 15:48:00] Decode batch, #running-req: 1, #token: 19897, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.47, #queue-req: 0, \n",
      "[2025-11-07 15:48:01] Decode batch, #running-req: 1, #token: 19937, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.22, #queue-req: 0, \n",
      "[2025-11-07 15:48:02] Decode batch, #running-req: 1, #token: 19977, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.32, #queue-req: 0, \n",
      "[2025-11-07 15:48:02] Decode batch, #running-req: 1, #token: 20017, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.28, #queue-req: 0, \n",
      "[2025-11-07 15:48:03] Decode batch, #running-req: 1, #token: 20057, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.27, #queue-req: 0, \n",
      "[2025-11-07 15:48:04] Decode batch, #running-req: 1, #token: 20097, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.33, #queue-req: 0, \n",
      "[2025-11-07 15:48:05] Decode batch, #running-req: 1, #token: 20137, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.33, #queue-req: 0, \n",
      "[2025-11-07 15:48:05] Decode batch, #running-req: 1, #token: 20177, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.33, #queue-req: 0, \n",
      "[2025-11-07 15:48:06] Decode batch, #running-req: 1, #token: 20217, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.19, #queue-req: 0, \n",
      "[2025-11-07 15:48:07] Decode batch, #running-req: 1, #token: 20257, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.00, #queue-req: 0, \n",
      "[2025-11-07 15:48:07] Decode batch, #running-req: 1, #token: 20297, token usage: 0.30, cuda graph: True, gen throughput (token/s): 57.03, #queue-req: 0, \n",
      "[2025-11-07 15:48:08] Decode batch, #running-req: 1, #token: 20337, token usage: 0.31, cuda graph: True, gen throughput (token/s): 57.05, #queue-req: 0, \n",
      "[2025-11-07 15:48:09] Decode batch, #running-req: 1, #token: 20377, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.98, #queue-req: 0, \n",
      "[2025-11-07 15:48:09] Decode batch, #running-req: 1, #token: 20417, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.86, #queue-req: 0, \n",
      "[2025-11-07 15:48:10] Decode batch, #running-req: 1, #token: 20457, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.31, #queue-req: 0, \n",
      "[2025-11-07 15:48:11] Decode batch, #running-req: 1, #token: 20497, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.21, #queue-req: 0, \n",
      "[2025-11-07 15:48:12] Decode batch, #running-req: 1, #token: 20537, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.01, #queue-req: 0, \n",
      "[2025-11-07 15:48:12] Decode batch, #running-req: 1, #token: 20577, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.11, #queue-req: 0, \n",
      "[2025-11-07 15:48:13] Decode batch, #running-req: 1, #token: 20617, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.25, #queue-req: 0, \n",
      "[2025-11-07 15:48:14] Decode batch, #running-req: 1, #token: 20657, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.46, #queue-req: 0, \n",
      "[2025-11-07 15:48:14] Decode batch, #running-req: 1, #token: 20697, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.40, #queue-req: 0, \n",
      "[2025-11-07 15:48:15] Decode batch, #running-req: 1, #token: 20737, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.08, #queue-req: 0, \n",
      "[2025-11-07 15:48:16] Decode batch, #running-req: 1, #token: 20777, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.10, #queue-req: 0, \n",
      "[2025-11-07 15:48:17] Decode batch, #running-req: 1, #token: 20817, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.05, #queue-req: 0, \n",
      "[2025-11-07 15:48:17] Decode batch, #running-req: 1, #token: 20857, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.29, #queue-req: 0, \n",
      "[2025-11-07 15:48:18] Decode batch, #running-req: 1, #token: 20897, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.33, #queue-req: 0, \n",
      "[2025-11-07 15:48:19] Decode batch, #running-req: 1, #token: 20937, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.39, #queue-req: 0, \n",
      "[2025-11-07 15:48:19] Decode batch, #running-req: 1, #token: 20977, token usage: 0.31, cuda graph: True, gen throughput (token/s): 56.45, #queue-req: 0, \n",
      "[2025-11-07 15:48:20] Decode batch, #running-req: 1, #token: 21017, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.39, #queue-req: 0, \n",
      "[2025-11-07 15:48:21] Decode batch, #running-req: 1, #token: 21057, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.39, #queue-req: 0, \n",
      "[2025-11-07 15:48:22] Decode batch, #running-req: 1, #token: 21097, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.34, #queue-req: 0, \n",
      "[2025-11-07 15:48:22] Decode batch, #running-req: 1, #token: 21137, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.34, #queue-req: 0, \n",
      "[2025-11-07 15:48:23] Decode batch, #running-req: 1, #token: 21177, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.39, #queue-req: 0, \n",
      "[2025-11-07 15:48:24] Decode batch, #running-req: 1, #token: 21217, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.26, #queue-req: 0, \n",
      "[2025-11-07 15:48:24] Decode batch, #running-req: 1, #token: 21257, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.29, #queue-req: 0, \n",
      "[2025-11-07 15:48:25] Decode batch, #running-req: 1, #token: 21297, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.31, #queue-req: 0, \n",
      "[2025-11-07 15:48:26] Decode batch, #running-req: 1, #token: 21337, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.22, #queue-req: 0, \n",
      "[2025-11-07 15:48:26] Decode batch, #running-req: 1, #token: 21377, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.12, #queue-req: 0, \n",
      "[2025-11-07 15:48:27] Decode batch, #running-req: 1, #token: 21417, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.13, #queue-req: 0, \n",
      "[2025-11-07 15:48:28] Decode batch, #running-req: 1, #token: 21457, token usage: 0.32, cuda graph: True, gen throughput (token/s): 56.01, #queue-req: 0, \n",
      "[2025-11-07 15:48:29] Decode batch, #running-req: 1, #token: 21497, token usage: 0.32, cuda graph: True, gen throughput (token/s): 55.87, #queue-req: 0, \n",
      "[2025-11-07 15:48:29] Decode batch, #running-req: 1, #token: 21537, token usage: 0.32, cuda graph: True, gen throughput (token/s): 55.87, #queue-req: 0, \n",
      "[2025-11-07 15:48:30] Decode batch, #running-req: 1, #token: 21577, token usage: 0.32, cuda graph: True, gen throughput (token/s): 55.91, #queue-req: 0, \n",
      "[2025-11-07 15:48:31] Decode batch, #running-req: 1, #token: 21617, token usage: 0.32, cuda graph: True, gen throughput (token/s): 55.87, #queue-req: 0, \n",
      "[2025-11-07 15:48:31] Decode batch, #running-req: 1, #token: 21657, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.76, #queue-req: 0, \n",
      "[2025-11-07 15:48:32] Decode batch, #running-req: 1, #token: 21697, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.62, #queue-req: 0, \n",
      "[2025-11-07 15:48:33] Decode batch, #running-req: 1, #token: 21737, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.55, #queue-req: 0, \n",
      "[2025-11-07 15:48:34] Decode batch, #running-req: 1, #token: 21777, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.48, #queue-req: 0, \n",
      "[2025-11-07 15:48:34] Decode batch, #running-req: 1, #token: 21817, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.41, #queue-req: 0, \n",
      "[2025-11-07 15:48:35] Decode batch, #running-req: 1, #token: 21857, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.37, #queue-req: 0, \n",
      "[2025-11-07 15:48:36] Decode batch, #running-req: 1, #token: 21897, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.39, #queue-req: 0, \n",
      "[2025-11-07 15:48:37] Decode batch, #running-req: 1, #token: 21937, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.26, #queue-req: 0, \n",
      "[2025-11-07 15:48:37] Decode batch, #running-req: 1, #token: 21977, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.33, #queue-req: 0, \n",
      "[2025-11-07 15:48:38] Decode batch, #running-req: 1, #token: 22017, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.16, #queue-req: 0, \n",
      "[2025-11-07 15:48:39] Decode batch, #running-req: 1, #token: 22057, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.06, #queue-req: 0, \n",
      "[2025-11-07 15:48:39] Decode batch, #running-req: 1, #token: 22097, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.05, #queue-req: 0, \n",
      "[2025-11-07 15:48:40] Decode batch, #running-req: 1, #token: 22137, token usage: 0.33, cuda graph: True, gen throughput (token/s): 55.00, #queue-req: 0, \n",
      "[2025-11-07 15:48:41] Decode batch, #running-req: 1, #token: 22177, token usage: 0.33, cuda graph: True, gen throughput (token/s): 54.83, #queue-req: 0, \n",
      "[2025-11-07 15:48:42] Decode batch, #running-req: 1, #token: 22217, token usage: 0.33, cuda graph: True, gen throughput (token/s): 54.51, #queue-req: 0, \n",
      "[2025-11-07 15:48:42] Decode batch, #running-req: 1, #token: 22257, token usage: 0.33, cuda graph: True, gen throughput (token/s): 54.71, #queue-req: 0, \n",
      "[2025-11-07 15:48:43] Decode batch, #running-req: 1, #token: 22297, token usage: 0.33, cuda graph: True, gen throughput (token/s): 54.59, #queue-req: 0, \n",
      "[2025-11-07 15:48:44] Decode batch, #running-req: 1, #token: 22337, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.81, #queue-req: 0, \n",
      "[2025-11-07 15:48:45] Decode batch, #running-req: 1, #token: 22377, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.87, #queue-req: 0, \n",
      "[2025-11-07 15:48:45] Decode batch, #running-req: 1, #token: 22417, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.96, #queue-req: 0, \n",
      "[2025-11-07 15:48:46] Decode batch, #running-req: 1, #token: 22457, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.97, #queue-req: 0, \n",
      "[2025-11-07 15:48:47] Decode batch, #running-req: 1, #token: 22497, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.92, #queue-req: 0, \n",
      "[2025-11-07 15:48:47] Decode batch, #running-req: 1, #token: 22537, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.91, #queue-req: 0, \n",
      "[2025-11-07 15:48:48] Decode batch, #running-req: 1, #token: 22577, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.90, #queue-req: 0, \n",
      "[2025-11-07 15:48:49] Decode batch, #running-req: 1, #token: 22617, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.87, #queue-req: 0, \n",
      "[2025-11-07 15:48:50] Decode batch, #running-req: 1, #token: 22657, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.82, #queue-req: 0, \n",
      "[2025-11-07 15:48:50] Decode batch, #running-req: 1, #token: 22697, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.70, #queue-req: 0, \n",
      "[2025-11-07 15:48:51] Decode batch, #running-req: 1, #token: 22737, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.48, #queue-req: 0, \n",
      "[2025-11-07 15:48:52] Decode batch, #running-req: 1, #token: 22777, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.42, #queue-req: 0, \n",
      "[2025-11-07 15:48:53] Decode batch, #running-req: 1, #token: 22817, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.47, #queue-req: 0, \n",
      "[2025-11-07 15:48:53] Decode batch, #running-req: 1, #token: 22857, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.54, #queue-req: 0, \n",
      "[2025-11-07 15:48:54] Decode batch, #running-req: 1, #token: 22897, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.55, #queue-req: 0, \n",
      "[2025-11-07 15:48:55] Decode batch, #running-req: 1, #token: 22937, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.49, #queue-req: 0, \n",
      "[2025-11-07 15:48:56] Decode batch, #running-req: 1, #token: 22977, token usage: 0.34, cuda graph: True, gen throughput (token/s): 54.46, #queue-req: 0, \n",
      "[2025-11-07 15:48:56] Decode batch, #running-req: 1, #token: 23017, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.38, #queue-req: 0, \n",
      "[2025-11-07 15:48:57] Decode batch, #running-req: 1, #token: 23057, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.35, #queue-req: 0, \n",
      "[2025-11-07 15:48:58] Decode batch, #running-req: 1, #token: 23097, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.28, #queue-req: 0, \n",
      "[2025-11-07 15:48:58] Decode batch, #running-req: 1, #token: 23137, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.19, #queue-req: 0, \n",
      "[2025-11-07 15:48:59] Decode batch, #running-req: 1, #token: 23177, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.14, #queue-req: 0, \n",
      "[2025-11-07 15:49:00] Decode batch, #running-req: 1, #token: 23217, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.11, #queue-req: 0, \n",
      "[2025-11-07 15:49:01] Decode batch, #running-req: 1, #token: 23257, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.14, #queue-req: 0, \n",
      "[2025-11-07 15:49:01] Decode batch, #running-req: 1, #token: 23297, token usage: 0.35, cuda graph: True, gen throughput (token/s): 54.08, #queue-req: 0, \n",
      "[2025-11-07 15:49:02] Decode batch, #running-req: 1, #token: 23337, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.89, #queue-req: 0, \n",
      "[2025-11-07 15:49:03] Decode batch, #running-req: 1, #token: 23377, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.88, #queue-req: 0, \n",
      "[2025-11-07 15:49:04] Decode batch, #running-req: 1, #token: 23417, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.88, #queue-req: 0, \n",
      "[2025-11-07 15:49:04] Decode batch, #running-req: 1, #token: 23457, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.90, #queue-req: 0, \n",
      "[2025-11-07 15:49:05] Decode batch, #running-req: 1, #token: 23497, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.95, #queue-req: 0, \n",
      "[2025-11-07 15:49:06] Decode batch, #running-req: 1, #token: 23537, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.95, #queue-req: 0, \n",
      "[2025-11-07 15:49:07] Decode batch, #running-req: 1, #token: 23577, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.94, #queue-req: 0, \n",
      "[2025-11-07 15:49:07] Decode batch, #running-req: 1, #token: 23617, token usage: 0.35, cuda graph: True, gen throughput (token/s): 53.95, #queue-req: 0, \n",
      "[2025-11-07 15:49:08] Decode batch, #running-req: 1, #token: 23657, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.92, #queue-req: 0, \n",
      "[2025-11-07 15:49:09] Decode batch, #running-req: 1, #token: 23697, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.96, #queue-req: 0, \n",
      "[2025-11-07 15:49:10] Decode batch, #running-req: 1, #token: 23737, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.92, #queue-req: 0, \n",
      "[2025-11-07 15:49:10] Decode batch, #running-req: 1, #token: 23777, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.69, #queue-req: 0, \n",
      "[2025-11-07 15:49:11] Decode batch, #running-req: 1, #token: 23817, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.77, #queue-req: 0, \n",
      "[2025-11-07 15:49:12] Decode batch, #running-req: 1, #token: 23857, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.68, #queue-req: 0, \n",
      "[2025-11-07 15:49:13] Decode batch, #running-req: 1, #token: 23897, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.76, #queue-req: 0, \n",
      "[2025-11-07 15:49:13] Decode batch, #running-req: 1, #token: 23937, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.70, #queue-req: 0, \n",
      "[2025-11-07 15:49:14] Decode batch, #running-req: 1, #token: 23977, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.66, #queue-req: 0, \n",
      "[2025-11-07 15:49:15] Decode batch, #running-req: 1, #token: 24017, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.60, #queue-req: 0, \n",
      "[2025-11-07 15:49:16] Decode batch, #running-req: 1, #token: 24057, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.55, #queue-req: 0, \n",
      "[2025-11-07 15:49:16] Decode batch, #running-req: 1, #token: 24097, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.57, #queue-req: 0, \n",
      "[2025-11-07 15:49:17] Decode batch, #running-req: 1, #token: 24137, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.55, #queue-req: 0, \n",
      "[2025-11-07 15:49:18] Decode batch, #running-req: 1, #token: 24177, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.49, #queue-req: 0, \n",
      "[2025-11-07 15:49:19] Decode batch, #running-req: 1, #token: 24217, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.47, #queue-req: 0, \n",
      "[2025-11-07 15:49:19] Decode batch, #running-req: 1, #token: 24257, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.37, #queue-req: 0, \n",
      "[2025-11-07 15:49:20] Decode batch, #running-req: 1, #token: 24297, token usage: 0.36, cuda graph: True, gen throughput (token/s): 53.34, #queue-req: 0, \n",
      "[2025-11-07 15:49:21] Decode batch, #running-req: 1, #token: 24337, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.26, #queue-req: 0, \n",
      "[2025-11-07 15:49:22] Decode batch, #running-req: 1, #token: 24377, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.16, #queue-req: 0, \n",
      "[2025-11-07 15:49:22] Decode batch, #running-req: 1, #token: 24417, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.08, #queue-req: 0, \n",
      "[2025-11-07 15:49:23] Decode batch, #running-req: 1, #token: 24457, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.07, #queue-req: 0, \n",
      "[2025-11-07 15:49:24] Decode batch, #running-req: 1, #token: 24497, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.03, #queue-req: 0, \n",
      "[2025-11-07 15:49:25] Decode batch, #running-req: 1, #token: 24537, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.06, #queue-req: 0, \n",
      "[2025-11-07 15:49:25] Decode batch, #running-req: 1, #token: 24577, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.00, #queue-req: 0, \n",
      "[2025-11-07 15:49:26] Decode batch, #running-req: 1, #token: 24617, token usage: 0.37, cuda graph: True, gen throughput (token/s): 53.05, #queue-req: 0, \n",
      "[2025-11-07 15:49:27] Decode batch, #running-req: 1, #token: 24657, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.96, #queue-req: 0, \n",
      "[2025-11-07 15:49:28] Decode batch, #running-req: 1, #token: 24697, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.76, #queue-req: 0, \n",
      "[2025-11-07 15:49:28] Decode batch, #running-req: 1, #token: 24737, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.70, #queue-req: 0, \n",
      "[2025-11-07 15:49:29] Decode batch, #running-req: 1, #token: 24777, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.79, #queue-req: 0, \n",
      "[2025-11-07 15:49:30] Decode batch, #running-req: 1, #token: 24817, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.83, #queue-req: 0, \n",
      "[2025-11-07 15:49:31] Decode batch, #running-req: 1, #token: 24857, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.76, #queue-req: 0, \n",
      "[2025-11-07 15:49:31] Decode batch, #running-req: 1, #token: 24897, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.82, #queue-req: 0, \n",
      "[2025-11-07 15:49:32] Decode batch, #running-req: 1, #token: 24937, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.83, #queue-req: 0, \n",
      "[2025-11-07 15:49:33] Decode batch, #running-req: 1, #token: 24977, token usage: 0.37, cuda graph: True, gen throughput (token/s): 52.82, #queue-req: 0, \n",
      "[2025-11-07 15:49:34] Decode batch, #running-req: 1, #token: 25017, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.45, #queue-req: 0, \n",
      "[2025-11-07 15:49:34] Decode batch, #running-req: 1, #token: 25057, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.12, #queue-req: 0, \n",
      "[2025-11-07 15:49:35] Decode batch, #running-req: 1, #token: 25097, token usage: 0.38, cuda graph: True, gen throughput (token/s): 51.93, #queue-req: 0, \n",
      "[2025-11-07 15:49:36] Decode batch, #running-req: 1, #token: 25137, token usage: 0.38, cuda graph: True, gen throughput (token/s): 51.81, #queue-req: 0, \n",
      "[2025-11-07 15:49:37] Decode batch, #running-req: 1, #token: 25177, token usage: 0.38, cuda graph: True, gen throughput (token/s): 51.77, #queue-req: 0, \n",
      "[2025-11-07 15:49:37] Decode batch, #running-req: 1, #token: 25217, token usage: 0.38, cuda graph: True, gen throughput (token/s): 51.91, #queue-req: 0, \n",
      "[2025-11-07 15:49:38] Decode batch, #running-req: 1, #token: 25257, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.04, #queue-req: 0, \n",
      "[2025-11-07 15:49:39] Decode batch, #running-req: 1, #token: 25297, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.13, #queue-req: 0, \n",
      "[2025-11-07 15:49:40] Decode batch, #running-req: 1, #token: 25337, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.30, #queue-req: 0, \n",
      "[2025-11-07 15:49:41] Decode batch, #running-req: 1, #token: 25377, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.39, #queue-req: 0, \n",
      "[2025-11-07 15:49:41] Decode batch, #running-req: 1, #token: 25417, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.35, #queue-req: 0, \n",
      "[2025-11-07 15:49:42] Decode batch, #running-req: 1, #token: 25457, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.25, #queue-req: 0, \n",
      "[2025-11-07 15:49:43] Decode batch, #running-req: 1, #token: 25497, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.08, #queue-req: 0, \n",
      "[2025-11-07 15:49:44] Decode batch, #running-req: 1, #token: 25537, token usage: 0.38, cuda graph: True, gen throughput (token/s): 52.15, #queue-req: 0, \n",
      "[2025-11-07 15:49:44] Decode batch, #running-req: 1, #token: 25577, token usage: 0.38, cuda graph: True, gen throughput (token/s): 51.82, #queue-req: 0, \n",
      "[2025-11-07 15:49:45] Decode batch, #running-req: 1, #token: 25617, token usage: 0.38, cuda graph: True, gen throughput (token/s): 51.79, #queue-req: 0, \n",
      "[2025-11-07 15:49:46] Decode batch, #running-req: 1, #token: 25657, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.60, #queue-req: 0, \n",
      "[2025-11-07 15:49:47] Decode batch, #running-req: 1, #token: 25697, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.78, #queue-req: 0, \n",
      "[2025-11-07 15:49:47] Decode batch, #running-req: 1, #token: 25737, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.75, #queue-req: 0, \n",
      "[2025-11-07 15:49:48] Decode batch, #running-req: 1, #token: 25777, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.84, #queue-req: 0, \n",
      "[2025-11-07 15:49:49] Decode batch, #running-req: 1, #token: 25817, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.92, #queue-req: 0, \n",
      "[2025-11-07 15:49:50] Decode batch, #running-req: 1, #token: 25857, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.91, #queue-req: 0, \n",
      "[2025-11-07 15:49:51] Decode batch, #running-req: 1, #token: 25897, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.87, #queue-req: 0, \n",
      "[2025-11-07 15:49:51] Decode batch, #running-req: 1, #token: 25937, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.85, #queue-req: 0, \n",
      "[2025-11-07 15:49:52] Decode batch, #running-req: 1, #token: 25977, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.88, #queue-req: 0, \n",
      "[2025-11-07 15:49:53] Decode batch, #running-req: 1, #token: 26017, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.80, #queue-req: 0, \n",
      "[2025-11-07 15:49:54] Decode batch, #running-req: 1, #token: 26057, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.69, #queue-req: 0, \n",
      "[2025-11-07 15:49:54] Decode batch, #running-req: 1, #token: 26097, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.54, #queue-req: 0, \n",
      "[2025-11-07 15:49:55] Decode batch, #running-req: 1, #token: 26137, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.66, #queue-req: 0, \n",
      "[2025-11-07 15:49:56] Decode batch, #running-req: 1, #token: 26177, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.72, #queue-req: 0, \n",
      "[2025-11-07 15:49:57] Decode batch, #running-req: 1, #token: 26217, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.71, #queue-req: 0, \n",
      "[2025-11-07 15:49:58] Decode batch, #running-req: 1, #token: 26257, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.63, #queue-req: 0, \n",
      "[2025-11-07 15:49:58] Decode batch, #running-req: 1, #token: 26297, token usage: 0.39, cuda graph: True, gen throughput (token/s): 51.62, #queue-req: 0, \n",
      "[2025-11-07 15:49:59] Decode batch, #running-req: 1, #token: 26337, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.64, #queue-req: 0, \n",
      "[2025-11-07 15:50:00] Decode batch, #running-req: 1, #token: 26377, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.52, #queue-req: 0, \n",
      "[2025-11-07 15:50:01] Decode batch, #running-req: 1, #token: 26417, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.50, #queue-req: 0, \n",
      "[2025-11-07 15:50:01] Decode batch, #running-req: 1, #token: 26457, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.48, #queue-req: 0, \n",
      "[2025-11-07 15:50:02] Decode batch, #running-req: 1, #token: 26497, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.44, #queue-req: 0, \n",
      "[2025-11-07 15:50:03] Decode batch, #running-req: 1, #token: 26537, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.47, #queue-req: 0, \n",
      "[2025-11-07 15:50:04] Decode batch, #running-req: 1, #token: 26577, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.38, #queue-req: 0, \n",
      "[2025-11-07 15:50:05] Decode batch, #running-req: 1, #token: 26617, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.31, #queue-req: 0, \n",
      "[2025-11-07 15:50:05] Decode batch, #running-req: 1, #token: 26657, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.05, #queue-req: 0, \n",
      "[2025-11-07 15:50:06] Decode batch, #running-req: 1, #token: 26697, token usage: 0.40, cuda graph: True, gen throughput (token/s): 50.07, #queue-req: 0, \n",
      "[2025-11-07 15:50:07] Decode batch, #running-req: 1, #token: 26737, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.23, #queue-req: 0, \n",
      "[2025-11-07 15:50:08] Decode batch, #running-req: 1, #token: 26777, token usage: 0.40, cuda graph: True, gen throughput (token/s): 51.02, #queue-req: 0, \n",
      "[2025-11-07 15:50:08] Decode batch, #running-req: 1, #token: 26817, token usage: 0.40, cuda graph: True, gen throughput (token/s): 50.57, #queue-req: 0, \n",
      "[2025-11-07 15:50:09] Decode batch, #running-req: 1, #token: 26857, token usage: 0.40, cuda graph: True, gen throughput (token/s): 50.59, #queue-req: 0, \n",
      "[2025-11-07 15:50:10] Decode batch, #running-req: 1, #token: 26897, token usage: 0.40, cuda graph: True, gen throughput (token/s): 50.79, #queue-req: 0, \n",
      "[2025-11-07 15:50:11] Decode batch, #running-req: 1, #token: 26937, token usage: 0.40, cuda graph: True, gen throughput (token/s): 50.89, #queue-req: 0, \n",
      "[2025-11-07 15:50:12] Decode batch, #running-req: 1, #token: 26977, token usage: 0.40, cuda graph: True, gen throughput (token/s): 50.76, #queue-req: 0, \n",
      "[2025-11-07 15:50:12] Decode batch, #running-req: 1, #token: 27017, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.77, #queue-req: 0, \n",
      "[2025-11-07 15:50:13] Decode batch, #running-req: 1, #token: 27057, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.70, #queue-req: 0, \n",
      "[2025-11-07 15:50:14] Decode batch, #running-req: 1, #token: 27097, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.75, #queue-req: 0, \n",
      "[2025-11-07 15:50:15] Decode batch, #running-req: 1, #token: 27137, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.71, #queue-req: 0, \n",
      "[2025-11-07 15:50:16] Decode batch, #running-req: 1, #token: 27177, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.57, #queue-req: 0, \n",
      "[2025-11-07 15:50:16] Decode batch, #running-req: 1, #token: 27217, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.60, #queue-req: 0, \n",
      "[2025-11-07 15:50:17] Decode batch, #running-req: 1, #token: 27257, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.63, #queue-req: 0, \n",
      "[2025-11-07 15:50:18] Decode batch, #running-req: 1, #token: 27297, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.63, #queue-req: 0, \n",
      "[2025-11-07 15:50:19] Decode batch, #running-req: 1, #token: 27337, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.70, #queue-req: 0, \n",
      "[2025-11-07 15:50:20] Decode batch, #running-req: 1, #token: 27377, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.65, #queue-req: 0, \n",
      "[2025-11-07 15:50:20] Decode batch, #running-req: 1, #token: 27417, token usage: 0.41, cuda graph: True, gen throughput (token/s): 46.55, #queue-req: 0, \n",
      "[2025-11-07 15:50:21] Decode batch, #running-req: 1, #token: 27457, token usage: 0.41, cuda graph: True, gen throughput (token/s): 48.43, #queue-req: 0, \n",
      "[2025-11-07 15:50:22] Decode batch, #running-req: 1, #token: 27497, token usage: 0.41, cuda graph: True, gen throughput (token/s): 49.22, #queue-req: 0, \n",
      "[2025-11-07 15:50:23] Decode batch, #running-req: 1, #token: 27537, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.03, #queue-req: 0, \n",
      "[2025-11-07 15:50:24] Decode batch, #running-req: 1, #token: 27577, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.28, #queue-req: 0, \n",
      "[2025-11-07 15:50:24] Decode batch, #running-req: 1, #token: 27617, token usage: 0.41, cuda graph: True, gen throughput (token/s): 50.34, #queue-req: 0, \n",
      "[2025-11-07 15:50:25] Decode batch, #running-req: 1, #token: 27657, token usage: 0.42, cuda graph: True, gen throughput (token/s): 50.32, #queue-req: 0, \n",
      "[2025-11-07 15:50:26] Decode batch, #running-req: 1, #token: 27697, token usage: 0.42, cuda graph: True, gen throughput (token/s): 50.34, #queue-req: 0, \n",
      "[2025-11-07 15:50:27] Decode batch, #running-req: 1, #token: 27737, token usage: 0.42, cuda graph: True, gen throughput (token/s): 50.31, #queue-req: 0, \n",
      "[2025-11-07 15:50:28] Decode batch, #running-req: 1, #token: 27777, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.64, #queue-req: 0, \n",
      "[2025-11-07 15:50:28] Decode batch, #running-req: 1, #token: 27817, token usage: 0.42, cuda graph: True, gen throughput (token/s): 46.46, #queue-req: 0, \n",
      "[2025-11-07 15:50:29] Decode batch, #running-req: 1, #token: 27857, token usage: 0.42, cuda graph: True, gen throughput (token/s): 47.30, #queue-req: 0, \n",
      "[2025-11-07 15:50:30] Decode batch, #running-req: 1, #token: 27897, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.24, #queue-req: 0, \n",
      "[2025-11-07 15:50:31] Decode batch, #running-req: 1, #token: 27937, token usage: 0.42, cuda graph: True, gen throughput (token/s): 47.13, #queue-req: 0, \n",
      "[2025-11-07 15:50:32] Decode batch, #running-req: 1, #token: 27977, token usage: 0.42, cuda graph: True, gen throughput (token/s): 48.43, #queue-req: 0, \n",
      "[2025-11-07 15:50:33] Decode batch, #running-req: 1, #token: 28017, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.83, #queue-req: 0, \n",
      "[2025-11-07 15:50:33] Decode batch, #running-req: 1, #token: 28057, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.83, #queue-req: 0, \n",
      "[2025-11-07 15:50:34] Decode batch, #running-req: 1, #token: 28097, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.79, #queue-req: 0, \n",
      "[2025-11-07 15:50:35] Decode batch, #running-req: 1, #token: 28137, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.74, #queue-req: 0, \n",
      "[2025-11-07 15:50:36] Decode batch, #running-req: 1, #token: 28177, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.79, #queue-req: 0, \n",
      "[2025-11-07 15:50:37] Decode batch, #running-req: 1, #token: 28217, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.80, #queue-req: 0, \n",
      "[2025-11-07 15:50:37] Decode batch, #running-req: 1, #token: 28257, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.77, #queue-req: 0, \n",
      "[2025-11-07 15:50:38] Decode batch, #running-req: 1, #token: 28297, token usage: 0.42, cuda graph: True, gen throughput (token/s): 49.88, #queue-req: 0, \n",
      "[2025-11-07 15:50:39] Decode batch, #running-req: 1, #token: 28337, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.81, #queue-req: 0, \n",
      "[2025-11-07 15:50:40] Decode batch, #running-req: 1, #token: 28377, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.81, #queue-req: 0, \n",
      "[2025-11-07 15:50:41] Decode batch, #running-req: 1, #token: 28417, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.77, #queue-req: 0, \n",
      "[2025-11-07 15:50:41] Decode batch, #running-req: 1, #token: 28457, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.74, #queue-req: 0, \n",
      "[2025-11-07 15:50:42] Decode batch, #running-req: 1, #token: 28497, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.05, #queue-req: 0, \n",
      "[2025-11-07 15:50:43] Decode batch, #running-req: 1, #token: 28537, token usage: 0.43, cuda graph: True, gen throughput (token/s): 53.15, #queue-req: 0, \n",
      "[2025-11-07 15:50:44] Decode batch, #running-req: 1, #token: 28577, token usage: 0.43, cuda graph: True, gen throughput (token/s): 50.40, #queue-req: 0, \n",
      "[2025-11-07 15:50:45] Decode batch, #running-req: 1, #token: 28617, token usage: 0.43, cuda graph: True, gen throughput (token/s): 45.29, #queue-req: 0, \n",
      "[2025-11-07 15:50:45] Decode batch, #running-req: 1, #token: 28657, token usage: 0.43, cuda graph: True, gen throughput (token/s): 48.17, #queue-req: 0, \n",
      "[2025-11-07 15:50:46] Decode batch, #running-req: 1, #token: 28697, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.70, #queue-req: 0, \n",
      "[2025-11-07 15:50:47] Decode batch, #running-req: 1, #token: 28737, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.68, #queue-req: 0, \n",
      "[2025-11-07 15:50:48] Decode batch, #running-req: 1, #token: 28777, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.64, #queue-req: 0, \n",
      "[2025-11-07 15:50:49] Decode batch, #running-req: 1, #token: 28817, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.69, #queue-req: 0, \n",
      "[2025-11-07 15:50:50] Decode batch, #running-req: 1, #token: 28857, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.66, #queue-req: 0, \n",
      "[2025-11-07 15:50:50] Decode batch, #running-req: 1, #token: 28897, token usage: 0.43, cuda graph: True, gen throughput (token/s): 49.61, #queue-req: 0, \n",
      "[2025-11-07 15:50:51] Decode batch, #running-req: 1, #token: 28937, token usage: 0.43, cuda graph: True, gen throughput (token/s): 48.18, #queue-req: 0, \n",
      "[2025-11-07 15:50:52] Decode batch, #running-req: 1, #token: 28977, token usage: 0.43, cuda graph: True, gen throughput (token/s): 47.40, #queue-req: 0, \n",
      "[2025-11-07 15:50:53] Decode batch, #running-req: 1, #token: 29017, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.62, #queue-req: 0, \n",
      "[2025-11-07 15:50:54] Decode batch, #running-req: 1, #token: 29057, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.44, #queue-req: 0, \n",
      "[2025-11-07 15:50:54] Decode batch, #running-req: 1, #token: 29097, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.39, #queue-req: 0, \n",
      "[2025-11-07 15:50:55] Decode batch, #running-req: 1, #token: 29137, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.40, #queue-req: 0, \n",
      "[2025-11-07 15:50:56] Decode batch, #running-req: 1, #token: 29177, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.37, #queue-req: 0, \n",
      "[2025-11-07 15:50:57] Decode batch, #running-req: 1, #token: 29217, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.17, #queue-req: 0, \n",
      "[2025-11-07 15:50:58] Decode batch, #running-req: 1, #token: 29257, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, \n",
      "[2025-11-07 15:50:59] Decode batch, #running-req: 1, #token: 29297, token usage: 0.44, cuda graph: True, gen throughput (token/s): 48.40, #queue-req: 0, \n",
      "[2025-11-07 15:50:59] Decode batch, #running-req: 1, #token: 29337, token usage: 0.44, cuda graph: True, gen throughput (token/s): 48.47, #queue-req: 0, \n",
      "[2025-11-07 15:51:00] Decode batch, #running-req: 1, #token: 29377, token usage: 0.44, cuda graph: True, gen throughput (token/s): 48.42, #queue-req: 0, \n",
      "[2025-11-07 15:51:01] Decode batch, #running-req: 1, #token: 29417, token usage: 0.44, cuda graph: True, gen throughput (token/s): 48.40, #queue-req: 0, \n",
      "[2025-11-07 15:51:02] Decode batch, #running-req: 1, #token: 29457, token usage: 0.44, cuda graph: True, gen throughput (token/s): 48.46, #queue-req: 0, \n",
      "[2025-11-07 15:51:03] Decode batch, #running-req: 1, #token: 29497, token usage: 0.44, cuda graph: True, gen throughput (token/s): 45.22, #queue-req: 0, \n",
      "[2025-11-07 15:51:04] Decode batch, #running-req: 1, #token: 29537, token usage: 0.44, cuda graph: True, gen throughput (token/s): 47.44, #queue-req: 0, \n",
      "[2025-11-07 15:51:04] Decode batch, #running-req: 1, #token: 29577, token usage: 0.44, cuda graph: True, gen throughput (token/s): 49.05, #queue-req: 0, \n",
      "[2025-11-07 15:51:05] Decode batch, #running-req: 1, #token: 29617, token usage: 0.44, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, \n",
      "[2025-11-07 15:51:06] Decode batch, #running-req: 1, #token: 29657, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, \n",
      "[2025-11-07 15:51:07] Decode batch, #running-req: 1, #token: 29697, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, \n",
      "[2025-11-07 15:51:08] Decode batch, #running-req: 1, #token: 29737, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, \n",
      "[2025-11-07 15:51:08] Decode batch, #running-req: 1, #token: 29777, token usage: 0.45, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, \n",
      "[2025-11-07 15:51:09] Decode batch, #running-req: 1, #token: 29817, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, \n",
      "[2025-11-07 15:51:10] Decode batch, #running-req: 1, #token: 29857, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, \n",
      "[2025-11-07 15:51:11] Decode batch, #running-req: 1, #token: 29897, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, \n",
      "[2025-11-07 15:51:12] Decode batch, #running-req: 1, #token: 29937, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.18, #queue-req: 0, \n",
      "[2025-11-07 15:51:13] Decode batch, #running-req: 1, #token: 29977, token usage: 0.45, cuda graph: True, gen throughput (token/s): 44.48, #queue-req: 0, \n",
      "[2025-11-07 15:51:13] Decode batch, #running-req: 1, #token: 30017, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.54, #queue-req: 0, \n",
      "[2025-11-07 15:51:14] Decode batch, #running-req: 1, #token: 30057, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, \n",
      "[2025-11-07 15:51:15] Decode batch, #running-req: 1, #token: 30097, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.59, #queue-req: 0, \n",
      "[2025-11-07 15:51:16] Decode batch, #running-req: 1, #token: 30137, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.62, #queue-req: 0, \n",
      "[2025-11-07 15:51:17] Decode batch, #running-req: 1, #token: 30177, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, \n",
      "[2025-11-07 15:51:18] Decode batch, #running-req: 1, #token: 30217, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.56, #queue-req: 0, \n",
      "[2025-11-07 15:51:18] Decode batch, #running-req: 1, #token: 30257, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.55, #queue-req: 0, \n",
      "[2025-11-07 15:51:19] Decode batch, #running-req: 1, #token: 30297, token usage: 0.45, cuda graph: True, gen throughput (token/s): 48.45, #queue-req: 0, \n",
      "[2025-11-07 15:51:20] Decode batch, #running-req: 1, #token: 30337, token usage: 0.46, cuda graph: True, gen throughput (token/s): 50.45, #queue-req: 0, \n",
      "[2025-11-07 15:51:21] Decode batch, #running-req: 1, #token: 30377, token usage: 0.46, cuda graph: True, gen throughput (token/s): 47.71, #queue-req: 0, \n",
      "[2025-11-07 15:51:22] Decode batch, #running-req: 1, #token: 30417, token usage: 0.46, cuda graph: True, gen throughput (token/s): 47.76, #queue-req: 0, \n",
      "[2025-11-07 15:51:23] Decode batch, #running-req: 1, #token: 30457, token usage: 0.46, cuda graph: True, gen throughput (token/s): 47.64, #queue-req: 0, \n",
      "[2025-11-07 15:51:23] Decode batch, #running-req: 1, #token: 30497, token usage: 0.46, cuda graph: True, gen throughput (token/s): 47.60, #queue-req: 0, \n",
      "[2025-11-07 15:51:24] Decode batch, #running-req: 1, #token: 30537, token usage: 0.46, cuda graph: True, gen throughput (token/s): 47.57, #queue-req: 0, \n",
      "[2025-11-07 15:51:25] Decode batch, #running-req: 1, #token: 30577, token usage: 0.46, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, \n",
      "[2025-11-07 15:51:26] Decode batch, #running-req: 1, #token: 30617, token usage: 0.46, cuda graph: True, gen throughput (token/s): 46.23, #queue-req: 0, \n",
      "[2025-11-07 15:51:27] Decode batch, #running-req: 1, #token: 30657, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.04, #queue-req: 0, \n",
      "[2025-11-07 15:51:28] Decode batch, #running-req: 1, #token: 30697, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.05, #queue-req: 0, \n",
      "[2025-11-07 15:51:28] Decode batch, #running-req: 1, #token: 30737, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.19, #queue-req: 0, \n",
      "[2025-11-07 15:51:29] Decode batch, #running-req: 1, #token: 30777, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.19, #queue-req: 0, \n",
      "[2025-11-07 15:51:30] Decode batch, #running-req: 1, #token: 30817, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.20, #queue-req: 0, \n",
      "[2025-11-07 15:51:31] Decode batch, #running-req: 1, #token: 30857, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.11, #queue-req: 0, \n",
      "[2025-11-07 15:51:32] Decode batch, #running-req: 1, #token: 30897, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.02, #queue-req: 0, \n",
      "[2025-11-07 15:51:33] Decode batch, #running-req: 1, #token: 30937, token usage: 0.46, cuda graph: True, gen throughput (token/s): 48.02, #queue-req: 0, \n",
      "[2025-11-07 15:51:33] Decode batch, #running-req: 1, #token: 30977, token usage: 0.46, cuda graph: True, gen throughput (token/s): 47.99, #queue-req: 0, \n",
      "[2025-11-07 15:51:34] Decode batch, #running-req: 1, #token: 31017, token usage: 0.47, cuda graph: True, gen throughput (token/s): 47.59, #queue-req: 0, \n",
      "[2025-11-07 15:51:35] Decode batch, #running-req: 1, #token: 31057, token usage: 0.47, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, \n",
      "[2025-11-07 15:51:36] Decode batch, #running-req: 1, #token: 31097, token usage: 0.47, cuda graph: True, gen throughput (token/s): 44.07, #queue-req: 0, \n",
      "[2025-11-07 15:51:37] Decode batch, #running-req: 1, #token: 31137, token usage: 0.47, cuda graph: True, gen throughput (token/s): 46.11, #queue-req: 0, \n",
      "[2025-11-07 15:51:38] Decode batch, #running-req: 1, #token: 31177, token usage: 0.47, cuda graph: True, gen throughput (token/s): 48.02, #queue-req: 0, \n",
      "[2025-11-07 15:51:39] Decode batch, #running-req: 1, #token: 31217, token usage: 0.47, cuda graph: True, gen throughput (token/s): 47.83, #queue-req: 0, \n",
      "[2025-11-07 15:51:39] Decode batch, #running-req: 1, #token: 31257, token usage: 0.47, cuda graph: True, gen throughput (token/s): 48.24, #queue-req: 0, \n",
      "[2025-11-07 15:51:40] Decode batch, #running-req: 1, #token: 31297, token usage: 0.47, cuda graph: True, gen throughput (token/s): 49.52, #queue-req: 0, \n",
      "[2025-11-07 15:51:41] Decode batch, #running-req: 1, #token: 31337, token usage: 0.47, cuda graph: True, gen throughput (token/s): 51.36, #queue-req: 0, \n",
      "[2025-11-07 15:51:42] Decode batch, #running-req: 1, #token: 31377, token usage: 0.47, cuda graph: True, gen throughput (token/s): 48.18, #queue-req: 0, \n",
      "[2025-11-07 15:51:43] Decode batch, #running-req: 1, #token: 31417, token usage: 0.47, cuda graph: True, gen throughput (token/s): 43.12, #queue-req: 0, \n",
      "[2025-11-07 15:51:44] Decode batch, #running-req: 1, #token: 31457, token usage: 0.47, cuda graph: True, gen throughput (token/s): 46.68, #queue-req: 0, \n",
      "[2025-11-07 15:51:44] Decode batch, #running-req: 1, #token: 31497, token usage: 0.47, cuda graph: True, gen throughput (token/s): 47.80, #queue-req: 0, \n",
      "[2025-11-07 15:51:45] Decode batch, #running-req: 1, #token: 31537, token usage: 0.47, cuda graph: True, gen throughput (token/s): 47.67, #queue-req: 0, \n",
      "[2025-11-07 15:51:46] Decode batch, #running-req: 1, #token: 31577, token usage: 0.47, cuda graph: True, gen throughput (token/s): 47.47, #queue-req: 0, \n",
      "[2025-11-07 15:51:47] Decode batch, #running-req: 1, #token: 31617, token usage: 0.47, cuda graph: True, gen throughput (token/s): 49.19, #queue-req: 0, \n",
      "[2025-11-07 15:51:48] Decode batch, #running-req: 1, #token: 31657, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, \n",
      "[2025-11-07 15:51:49] Decode batch, #running-req: 1, #token: 31697, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, \n",
      "[2025-11-07 15:51:49] Decode batch, #running-req: 1, #token: 31737, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, \n",
      "[2025-11-07 15:51:50] Decode batch, #running-req: 1, #token: 31777, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, \n",
      "[2025-11-07 15:51:51] Decode batch, #running-req: 1, #token: 31817, token usage: 0.48, cuda graph: True, gen throughput (token/s): 44.54, #queue-req: 0, \n",
      "[2025-11-07 15:51:52] Decode batch, #running-req: 1, #token: 31857, token usage: 0.48, cuda graph: True, gen throughput (token/s): 44.84, #queue-req: 0, \n",
      "[2025-11-07 15:51:53] Decode batch, #running-req: 1, #token: 31897, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.32, #queue-req: 0, \n",
      "[2025-11-07 15:51:54] Decode batch, #running-req: 1, #token: 31937, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.31, #queue-req: 0, \n",
      "[2025-11-07 15:51:55] Decode batch, #running-req: 1, #token: 31977, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.32, #queue-req: 0, \n",
      "[2025-11-07 15:51:55] Decode batch, #running-req: 1, #token: 32017, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.42, #queue-req: 0, \n",
      "[2025-11-07 15:51:56] Decode batch, #running-req: 1, #token: 32057, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.34, #queue-req: 0, \n",
      "[2025-11-07 15:51:57] Decode batch, #running-req: 1, #token: 32097, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.24, #queue-req: 0, \n",
      "[2025-11-07 15:51:58] Decode batch, #running-req: 1, #token: 32137, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.11, #queue-req: 0, \n",
      "[2025-11-07 15:51:59] Decode batch, #running-req: 1, #token: 32177, token usage: 0.48, cuda graph: True, gen throughput (token/s): 47.45, #queue-req: 0, \n",
      "[2025-11-07 15:52:00] Decode batch, #running-req: 1, #token: 32217, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, \n",
      "[2025-11-07 15:52:00] Decode batch, #running-req: 1, #token: 32257, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, \n",
      "[2025-11-07 15:52:01] Decode batch, #running-req: 1, #token: 32297, token usage: 0.48, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, \n",
      "[2025-11-07 15:52:02] Decode batch, #running-req: 1, #token: 32337, token usage: 0.49, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, \n",
      "[2025-11-07 15:52:03] Decode batch, #running-req: 1, #token: 32377, token usage: 0.49, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, \n",
      "[2025-11-07 15:52:04] Decode batch, #running-req: 1, #token: 32417, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.83, #queue-req: 0, \n",
      "[2025-11-07 15:52:05] Decode batch, #running-req: 1, #token: 32457, token usage: 0.49, cuda graph: True, gen throughput (token/s): 47.07, #queue-req: 0, \n",
      "[2025-11-07 15:52:06] Decode batch, #running-req: 1, #token: 32497, token usage: 0.49, cuda graph: True, gen throughput (token/s): 47.04, #queue-req: 0, \n",
      "[2025-11-07 15:52:06] Decode batch, #running-req: 1, #token: 32537, token usage: 0.49, cuda graph: True, gen throughput (token/s): 47.06, #queue-req: 0, \n",
      "[2025-11-07 15:52:07] Decode batch, #running-req: 1, #token: 32577, token usage: 0.49, cuda graph: True, gen throughput (token/s): 47.06, #queue-req: 0, \n",
      "[2025-11-07 15:52:08] Decode batch, #running-req: 1, #token: 32617, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.89, #queue-req: 0, \n",
      "[2025-11-07 15:52:09] Decode batch, #running-req: 1, #token: 32657, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.90, #queue-req: 0, \n",
      "[2025-11-07 15:52:10] Decode batch, #running-req: 1, #token: 32697, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.85, #queue-req: 0, \n",
      "[2025-11-07 15:52:11] Decode batch, #running-req: 1, #token: 32737, token usage: 0.49, cuda graph: True, gen throughput (token/s): 42.60, #queue-req: 0, \n",
      "[2025-11-07 15:52:12] Decode batch, #running-req: 1, #token: 32777, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.00, #queue-req: 0, \n",
      "[2025-11-07 15:52:13] Decode batch, #running-req: 1, #token: 32817, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.63, #queue-req: 0, \n",
      "[2025-11-07 15:52:13] Decode batch, #running-req: 1, #token: 32857, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.45, #queue-req: 0, \n",
      "[2025-11-07 15:52:14] Decode batch, #running-req: 1, #token: 32897, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.42, #queue-req: 0, \n",
      "[2025-11-07 15:52:15] Decode batch, #running-req: 1, #token: 32937, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.32, #queue-req: 0, \n",
      "[2025-11-07 15:52:16] Decode batch, #running-req: 1, #token: 32977, token usage: 0.49, cuda graph: True, gen throughput (token/s): 46.21, #queue-req: 0, \n",
      "[2025-11-07 15:52:17] Decode batch, #running-req: 1, #token: 33017, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.27, #queue-req: 0, \n",
      "[2025-11-07 15:52:18] Decode batch, #running-req: 1, #token: 33057, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.28, #queue-req: 0, \n",
      "[2025-11-07 15:52:19] Decode batch, #running-req: 1, #token: 33097, token usage: 0.50, cuda graph: True, gen throughput (token/s): 42.58, #queue-req: 0, \n",
      "[2025-11-07 15:52:20] Decode batch, #running-req: 1, #token: 33137, token usage: 0.50, cuda graph: True, gen throughput (token/s): 40.09, #queue-req: 0, \n",
      "[2025-11-07 15:52:21] Decode batch, #running-req: 1, #token: 33177, token usage: 0.50, cuda graph: True, gen throughput (token/s): 43.19, #queue-req: 0, \n",
      "[2025-11-07 15:52:21] Decode batch, #running-req: 1, #token: 33217, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.16, #queue-req: 0, \n",
      "[2025-11-07 15:52:22] Decode batch, #running-req: 1, #token: 33257, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.33, #queue-req: 0, \n",
      "[2025-11-07 15:52:23] Decode batch, #running-req: 1, #token: 33297, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.03, #queue-req: 0, \n",
      "[2025-11-07 15:52:24] Decode batch, #running-req: 1, #token: 33337, token usage: 0.50, cuda graph: True, gen throughput (token/s): 41.89, #queue-req: 0, \n",
      "[2025-11-07 15:52:25] Decode batch, #running-req: 1, #token: 33377, token usage: 0.50, cuda graph: True, gen throughput (token/s): 45.96, #queue-req: 0, \n",
      "[2025-11-07 15:52:26] Decode batch, #running-req: 1, #token: 33417, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.49, #queue-req: 0, \n",
      "[2025-11-07 15:52:27] Decode batch, #running-req: 1, #token: 33457, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.38, #queue-req: 0, \n",
      "[2025-11-07 15:52:28] Decode batch, #running-req: 1, #token: 33497, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.30, #queue-req: 0, \n",
      "[2025-11-07 15:52:28] Decode batch, #running-req: 1, #token: 33537, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.20, #queue-req: 0, \n",
      "[2025-11-07 15:52:29] Decode batch, #running-req: 1, #token: 33577, token usage: 0.50, cuda graph: True, gen throughput (token/s): 46.07, #queue-req: 0, \n",
      "[2025-11-07 15:52:30] Decode batch, #running-req: 1, #token: 33617, token usage: 0.50, cuda graph: True, gen throughput (token/s): 45.86, #queue-req: 0, \n",
      "[2025-11-07 15:52:31] Decode batch, #running-req: 1, #token: 33657, token usage: 0.51, cuda graph: True, gen throughput (token/s): 46.05, #queue-req: 0, \n",
      "[2025-11-07 15:52:32] Decode batch, #running-req: 1, #token: 33697, token usage: 0.51, cuda graph: True, gen throughput (token/s): 46.06, #queue-req: 0, \n",
      "[2025-11-07 15:52:33] Decode batch, #running-req: 1, #token: 33737, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.99, #queue-req: 0, \n",
      "[2025-11-07 15:52:34] Decode batch, #running-req: 1, #token: 33777, token usage: 0.51, cuda graph: True, gen throughput (token/s): 42.35, #queue-req: 0, \n",
      "[2025-11-07 15:52:35] Decode batch, #running-req: 1, #token: 33817, token usage: 0.51, cuda graph: True, gen throughput (token/s): 44.94, #queue-req: 0, \n",
      "[2025-11-07 15:52:35] Decode batch, #running-req: 1, #token: 33857, token usage: 0.51, cuda graph: True, gen throughput (token/s): 46.09, #queue-req: 0, \n",
      "[2025-11-07 15:52:36] Decode batch, #running-req: 1, #token: 33897, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.83, #queue-req: 0, \n",
      "[2025-11-07 15:52:37] Decode batch, #running-req: 1, #token: 33937, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.73, #queue-req: 0, \n",
      "[2025-11-07 15:52:38] Decode batch, #running-req: 1, #token: 33977, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.66, #queue-req: 0, \n",
      "[2025-11-07 15:52:39] Decode batch, #running-req: 1, #token: 34017, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.76, #queue-req: 0, \n",
      "[2025-11-07 15:52:40] Decode batch, #running-req: 1, #token: 34057, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.82, #queue-req: 0, \n",
      "[2025-11-07 15:52:41] Decode batch, #running-req: 1, #token: 34097, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.80, #queue-req: 0, \n",
      "[2025-11-07 15:52:42] Decode batch, #running-req: 1, #token: 34137, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.84, #queue-req: 0, \n",
      "[2025-11-07 15:52:42] Decode batch, #running-req: 1, #token: 34177, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.78, #queue-req: 0, \n",
      "[2025-11-07 15:52:43] Decode batch, #running-req: 1, #token: 34217, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.60, #queue-req: 0, \n",
      "[2025-11-07 15:52:44] Decode batch, #running-req: 1, #token: 34257, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.56, #queue-req: 0, \n",
      "[2025-11-07 15:52:45] Decode batch, #running-req: 1, #token: 34297, token usage: 0.51, cuda graph: True, gen throughput (token/s): 45.14, #queue-req: 0, \n",
      "[2025-11-07 15:52:46] Decode batch, #running-req: 1, #token: 34337, token usage: 0.52, cuda graph: True, gen throughput (token/s): 41.93, #queue-req: 0, \n",
      "[2025-11-07 15:52:47] Decode batch, #running-req: 1, #token: 34377, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.43, #queue-req: 0, \n",
      "[2025-11-07 15:52:48] Decode batch, #running-req: 1, #token: 34417, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.64, #queue-req: 0, \n",
      "[2025-11-07 15:52:49] Decode batch, #running-req: 1, #token: 34457, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.75, #queue-req: 0, \n",
      "[2025-11-07 15:52:50] Decode batch, #running-req: 1, #token: 34497, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.71, #queue-req: 0, \n",
      "[2025-11-07 15:52:50] Decode batch, #running-req: 1, #token: 34537, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.70, #queue-req: 0, \n",
      "[2025-11-07 15:52:51] Decode batch, #running-req: 1, #token: 34577, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.69, #queue-req: 0, \n",
      "[2025-11-07 15:52:52] Decode batch, #running-req: 1, #token: 34617, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.51, #queue-req: 0, \n",
      "[2025-11-07 15:52:53] Decode batch, #running-req: 1, #token: 34657, token usage: 0.52, cuda graph: True, gen throughput (token/s): 46.76, #queue-req: 0, \n",
      "[2025-11-07 15:52:54] Decode batch, #running-req: 1, #token: 34697, token usage: 0.52, cuda graph: True, gen throughput (token/s): 47.63, #queue-req: 0, \n",
      "[2025-11-07 15:52:55] Decode batch, #running-req: 1, #token: 34737, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.16, #queue-req: 0, \n",
      "[2025-11-07 15:52:56] Decode batch, #running-req: 1, #token: 34777, token usage: 0.52, cuda graph: True, gen throughput (token/s): 41.30, #queue-req: 0, \n",
      "[2025-11-07 15:52:57] Decode batch, #running-req: 1, #token: 34817, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.23, #queue-req: 0, \n",
      "[2025-11-07 15:52:58] Decode batch, #running-req: 1, #token: 34857, token usage: 0.52, cuda graph: True, gen throughput (token/s): 44.59, #queue-req: 0, \n",
      "[2025-11-07 15:52:58] Decode batch, #running-req: 1, #token: 34897, token usage: 0.52, cuda graph: True, gen throughput (token/s): 41.56, #queue-req: 0, \n",
      "[2025-11-07 15:52:59] Decode batch, #running-req: 1, #token: 34937, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.25, #queue-req: 0, \n",
      "[2025-11-07 15:53:00] Decode batch, #running-req: 1, #token: 34977, token usage: 0.52, cuda graph: True, gen throughput (token/s): 45.42, #queue-req: 0, \n",
      "[2025-11-07 15:53:01] Decode batch, #running-req: 1, #token: 35017, token usage: 0.53, cuda graph: True, gen throughput (token/s): 45.48, #queue-req: 0, \n",
      "[2025-11-07 15:53:02] Decode batch, #running-req: 1, #token: 35057, token usage: 0.53, cuda graph: True, gen throughput (token/s): 43.84, #queue-req: 0, \n",
      "[2025-11-07 15:53:03] Decode batch, #running-req: 1, #token: 35097, token usage: 0.53, cuda graph: True, gen throughput (token/s): 42.31, #queue-req: 0, \n",
      "[2025-11-07 15:53:04] Decode batch, #running-req: 1, #token: 35137, token usage: 0.53, cuda graph: True, gen throughput (token/s): 45.20, #queue-req: 0, \n",
      "[2025-11-07 15:53:05] Decode batch, #running-req: 1, #token: 35177, token usage: 0.53, cuda graph: True, gen throughput (token/s): 45.26, #queue-req: 0, \n",
      "[2025-11-07 15:53:06] Decode batch, #running-req: 1, #token: 35217, token usage: 0.53, cuda graph: True, gen throughput (token/s): 45.23, #queue-req: 0, \n",
      "[2025-11-07 15:53:07] Decode batch, #running-req: 1, #token: 35257, token usage: 0.53, cuda graph: True, gen throughput (token/s): 44.46, #queue-req: 0, \n",
      "[2025-11-07 15:53:08] Decode batch, #running-req: 1, #token: 35297, token usage: 0.53, cuda graph: True, gen throughput (token/s): 41.34, #queue-req: 0, \n",
      "[2025-11-07 15:53:08] Decode batch, #running-req: 1, #token: 35337, token usage: 0.53, cuda graph: True, gen throughput (token/s): 44.87, #queue-req: 0, \n",
      "[2025-11-07 15:53:09] Decode batch, #running-req: 1, #token: 35377, token usage: 0.53, cuda graph: True, gen throughput (token/s): 45.06, #queue-req: 0, \n",
      "[2025-11-07 15:53:10] Decode batch, #running-req: 1, #token: 35417, token usage: 0.53, cuda graph: True, gen throughput (token/s): 45.05, #queue-req: 0, \n",
      "[2025-11-07 15:53:11] Decode batch, #running-req: 1, #token: 35457, token usage: 0.53, cuda graph: True, gen throughput (token/s): 44.78, #queue-req: 0, \n",
      "[2025-11-07 15:53:12] Decode batch, #running-req: 1, #token: 35497, token usage: 0.53, cuda graph: True, gen throughput (token/s): 44.49, #queue-req: 0, \n",
      "[2025-11-07 15:53:13] Decode batch, #running-req: 1, #token: 35537, token usage: 0.53, cuda graph: True, gen throughput (token/s): 41.42, #queue-req: 0, \n",
      "[2025-11-07 15:53:14] Decode batch, #running-req: 1, #token: 35577, token usage: 0.53, cuda graph: True, gen throughput (token/s): 42.77, #queue-req: 0, \n",
      "[2025-11-07 15:53:15] Decode batch, #running-req: 1, #token: 35617, token usage: 0.53, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, \n",
      "[2025-11-07 15:53:16] Decode batch, #running-req: 1, #token: 35657, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, \n",
      "[2025-11-07 15:53:17] Decode batch, #running-req: 1, #token: 35697, token usage: 0.54, cuda graph: True, gen throughput (token/s): 43.43, #queue-req: 0, \n",
      "[2025-11-07 15:53:18] Decode batch, #running-req: 1, #token: 35737, token usage: 0.54, cuda graph: True, gen throughput (token/s): 41.77, #queue-req: 0, \n",
      "[2025-11-07 15:53:18] Decode batch, #running-req: 1, #token: 35777, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.48, #queue-req: 0, \n",
      "[2025-11-07 15:53:19] Decode batch, #running-req: 1, #token: 35817, token usage: 0.54, cuda graph: True, gen throughput (token/s): 43.94, #queue-req: 0, \n",
      "[2025-11-07 15:53:20] Decode batch, #running-req: 1, #token: 35857, token usage: 0.54, cuda graph: True, gen throughput (token/s): 40.73, #queue-req: 0, \n",
      "[2025-11-07 15:53:21] Decode batch, #running-req: 1, #token: 35897, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.65, #queue-req: 0, \n",
      "[2025-11-07 15:53:22] Decode batch, #running-req: 1, #token: 35937, token usage: 0.54, cuda graph: True, gen throughput (token/s): 43.70, #queue-req: 0, \n",
      "[2025-11-07 15:53:23] Decode batch, #running-req: 1, #token: 35977, token usage: 0.54, cuda graph: True, gen throughput (token/s): 41.03, #queue-req: 0, \n",
      "[2025-11-07 15:53:24] Decode batch, #running-req: 1, #token: 36017, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.49, #queue-req: 0, \n",
      "[2025-11-07 15:53:25] Decode batch, #running-req: 1, #token: 36057, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.68, #queue-req: 0, \n",
      "[2025-11-07 15:53:26] Decode batch, #running-req: 1, #token: 36097, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.65, #queue-req: 0, \n",
      "[2025-11-07 15:53:27] Decode batch, #running-req: 1, #token: 36137, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.72, #queue-req: 0, \n",
      "[2025-11-07 15:53:28] Decode batch, #running-req: 1, #token: 36177, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.71, #queue-req: 0, \n",
      "[2025-11-07 15:53:29] Decode batch, #running-req: 1, #token: 36217, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.68, #queue-req: 0, \n",
      "[2025-11-07 15:53:29] Decode batch, #running-req: 1, #token: 36257, token usage: 0.54, cuda graph: True, gen throughput (token/s): 44.60, #queue-req: 0, \n",
      "[2025-11-07 15:53:30] Decode batch, #running-req: 1, #token: 36297, token usage: 0.54, cuda graph: True, gen throughput (token/s): 43.66, #queue-req: 0, \n",
      "[2025-11-07 15:53:31] Decode batch, #running-req: 1, #token: 36337, token usage: 0.55, cuda graph: True, gen throughput (token/s): 40.89, #queue-req: 0, \n",
      "[2025-11-07 15:53:32] Decode batch, #running-req: 1, #token: 36377, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, \n",
      "[2025-11-07 15:53:33] Decode batch, #running-req: 1, #token: 36417, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.51, #queue-req: 0, \n",
      "[2025-11-07 15:53:34] Decode batch, #running-req: 1, #token: 36457, token usage: 0.55, cuda graph: True, gen throughput (token/s): 43.26, #queue-req: 0, \n",
      "[2025-11-07 15:53:35] Decode batch, #running-req: 1, #token: 36497, token usage: 0.55, cuda graph: True, gen throughput (token/s): 41.55, #queue-req: 0, \n",
      "[2025-11-07 15:53:36] Decode batch, #running-req: 1, #token: 36537, token usage: 0.55, cuda graph: True, gen throughput (token/s): 43.60, #queue-req: 0, \n",
      "[2025-11-07 15:53:37] Decode batch, #running-req: 1, #token: 36577, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, \n",
      "[2025-11-07 15:53:38] Decode batch, #running-req: 1, #token: 36617, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.14, #queue-req: 0, \n",
      "[2025-11-07 15:53:39] Decode batch, #running-req: 1, #token: 36657, token usage: 0.55, cuda graph: True, gen throughput (token/s): 41.28, #queue-req: 0, \n",
      "[2025-11-07 15:53:40] Decode batch, #running-req: 1, #token: 36697, token usage: 0.55, cuda graph: True, gen throughput (token/s): 42.52, #queue-req: 0, \n",
      "[2025-11-07 15:53:41] Decode batch, #running-req: 1, #token: 36737, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, \n",
      "[2025-11-07 15:53:41] Decode batch, #running-req: 1, #token: 36777, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, \n",
      "[2025-11-07 15:53:42] Decode batch, #running-req: 1, #token: 36817, token usage: 0.55, cuda graph: True, gen throughput (token/s): 43.37, #queue-req: 0, \n",
      "[2025-11-07 15:53:43] Decode batch, #running-req: 1, #token: 36857, token usage: 0.55, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, \n",
      "[2025-11-07 15:53:44] Decode batch, #running-req: 1, #token: 36897, token usage: 0.55, cuda graph: True, gen throughput (token/s): 43.55, #queue-req: 0, \n",
      "[2025-11-07 15:53:45] Decode batch, #running-req: 1, #token: 36937, token usage: 0.55, cuda graph: True, gen throughput (token/s): 44.04, #queue-req: 0, \n",
      "[2025-11-07 15:53:46] Decode batch, #running-req: 1, #token: 36977, token usage: 0.55, cuda graph: True, gen throughput (token/s): 43.96, #queue-req: 0, \n",
      "[2025-11-07 15:53:47] Decode batch, #running-req: 1, #token: 37017, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.79, #queue-req: 0, \n",
      "[2025-11-07 15:53:48] Decode batch, #running-req: 1, #token: 37057, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.73, #queue-req: 0, \n",
      "[2025-11-07 15:53:49] Decode batch, #running-req: 1, #token: 37097, token usage: 0.56, cuda graph: True, gen throughput (token/s): 42.16, #queue-req: 0, \n",
      "[2025-11-07 15:53:50] Decode batch, #running-req: 1, #token: 37137, token usage: 0.56, cuda graph: True, gen throughput (token/s): 40.99, #queue-req: 0, \n",
      "[2025-11-07 15:53:51] Decode batch, #running-req: 1, #token: 37177, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.87, #queue-req: 0, \n",
      "[2025-11-07 15:53:52] Decode batch, #running-req: 1, #token: 37217, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.84, #queue-req: 0, \n",
      "[2025-11-07 15:53:53] Decode batch, #running-req: 1, #token: 37257, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.78, #queue-req: 0, \n",
      "[2025-11-07 15:53:53] Decode batch, #running-req: 1, #token: 37297, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.47, #queue-req: 0, \n",
      "[2025-11-07 15:53:54] Decode batch, #running-req: 1, #token: 37337, token usage: 0.56, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, \n",
      "[2025-11-07 15:53:55] Decode batch, #running-req: 1, #token: 37377, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.55, #queue-req: 0, \n",
      "[2025-11-07 15:53:56] Decode batch, #running-req: 1, #token: 37417, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.06, #queue-req: 0, \n",
      "[2025-11-07 15:53:57] Decode batch, #running-req: 1, #token: 37457, token usage: 0.56, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, \n",
      "[2025-11-07 15:53:58] Decode batch, #running-req: 1, #token: 37497, token usage: 0.56, cuda graph: True, gen throughput (token/s): 42.90, #queue-req: 0, \n",
      "[2025-11-07 15:53:59] Decode batch, #running-req: 1, #token: 37537, token usage: 0.56, cuda graph: True, gen throughput (token/s): 42.50, #queue-req: 0, \n",
      "[2025-11-07 15:54:00] Decode batch, #running-req: 1, #token: 37577, token usage: 0.56, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, \n",
      "[2025-11-07 15:54:01] Decode batch, #running-req: 1, #token: 37617, token usage: 0.56, cuda graph: True, gen throughput (token/s): 43.60, #queue-req: 0, \n",
      "[2025-11-07 15:54:02] Decode batch, #running-req: 1, #token: 37657, token usage: 0.57, cuda graph: True, gen throughput (token/s): 43.71, #queue-req: 0, \n",
      "[2025-11-07 15:54:03] Decode batch, #running-req: 1, #token: 37697, token usage: 0.57, cuda graph: True, gen throughput (token/s): 43.78, #queue-req: 0, \n",
      "[2025-11-07 15:54:04] Decode batch, #running-req: 1, #token: 37737, token usage: 0.57, cuda graph: True, gen throughput (token/s): 43.43, #queue-req: 0, \n",
      "[2025-11-07 15:54:05] Decode batch, #running-req: 1, #token: 37777, token usage: 0.57, cuda graph: True, gen throughput (token/s): 43.48, #queue-req: 0, \n",
      "[2025-11-07 15:54:06] Decode batch, #running-req: 1, #token: 37817, token usage: 0.57, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, \n",
      "[2025-11-07 15:54:07] Decode batch, #running-req: 1, #token: 37857, token usage: 0.57, cuda graph: True, gen throughput (token/s): 42.17, #queue-req: 0, \n",
      "[2025-11-07 15:54:08] Decode batch, #running-req: 1, #token: 37897, token usage: 0.57, cuda graph: True, gen throughput (token/s): 43.57, #queue-req: 0, \n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/sglang/srt/managers/detokenizer_manager.py\", line 336, in run_detokenizer_process\n",
      "    manager.event_loop()\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/sglang/srt/managers/detokenizer_manager.py\", line 116, in event_loop\n",
      "    recv_obj = self.recv_from_scheduler.recv_pyobj()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 989, in recv_pyobj\n",
      "    msg = self.recv(flags)\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 1218, in zmq.backend.cython._zmq.Socket.recv\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 1253, in zmq.backend.cython._zmq.Socket.recv\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 1408, in zmq.backend.cython._zmq._recv_copy\n",
      "  File \"zmq/backend/cython/_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 2826, in run_scheduler_process\n",
      "    scheduler.event_loop_overlap()\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 996, in event_loop_overlap\n",
      "    self.process_batch_result(tmp_batch, tmp_result)\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 2090, in process_batch_result\n",
      "    self.process_batch_result_decode(batch, result)\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/sglang/srt/managers/scheduler_output_processor_mixin.py\", line 288, in process_batch_result_decode\n",
      "    result.copy_done.synchronize()\n",
      "  File \"/home/flowers/miniconda3/envs/arcn/lib/python3.12/site-packages/torch/cuda/streams.py\", line 231, in synchronize\n",
      "    super().synchronize()\n",
      "KeyboardInterrupt\n",
      "[2025-11-07 15:54:08] INFO:     Shutting down\n",
      "[2025-11-07 15:54:08] INFO:     Waiting for connections to close. (CTRL+C to force quit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flowers/work/llms_ftw/src/llm_client.py:517: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(' /!\\ Server is running /!\\ ')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_str\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llms_ftw/src/llm_client.py:592\u001b[39m, in \u001b[36mLLMClient.generate\u001b[39m\u001b[34m(self, prompts, n, temperature)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts,n=\u001b[32m1\u001b[39m,temperature=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmultiple_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llms_ftw/src/llm_client.py:589\u001b[39m, in \u001b[36mLLMClient.multiple_completion\u001b[39m\u001b[34m(self, batch_prompt, n, temperature)\u001b[39m\n\u001b[32m    587\u001b[39m     batch_prompt = \u001b[38;5;28mself\u001b[39m.formating_chat_prompt(batch_prompt)\n\u001b[32m    588\u001b[39m batch_prompt = \u001b[38;5;28mself\u001b[39m.add_reasoning_system_prompt(batch_prompt)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_multiple_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_generation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llms_ftw/src/llm_client.py:760\u001b[39m, in \u001b[36mget_multiple_completions\u001b[39m\u001b[34m(client, batch_prompt, cfg_generation, max_workers, temperature, n)\u001b[39m\n\u001b[32m    758\u001b[39m completions = []\n\u001b[32m    759\u001b[39m count=\u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub_batch\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/arcn/lib/python3.12/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/arcn/lib/python3.12/concurrent/futures/thread.py:239\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/arcn/lib/python3.12/threading.py:1149\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1151\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/arcn/lib/python3.12/threading.py:1169\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1170\u001b[39m         lock.release()\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1107 15:54:09.071132603 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "nanobind: leaked 1 instances!\n",
      " - leaked instance 0x7f38b31ad4b8 of type \"xgrammar.xgrammar_bindings.GrammarCompiler\"\n",
      "nanobind: leaked 1 types!\n",
      " - leaked type \"xgrammar.xgrammar_bindings.GrammarCompiler\"\n",
      "nanobind: leaked 9 functions!\n",
      " - leaked function \"compile_regex\"\n",
      " - leaked function \"\"\n",
      " - leaked function \"compile_builtin_json_grammar\"\n",
      " - leaked function \"compile_grammar\"\n",
      " - leaked function \"compile_json_schema\"\n",
      " - leaked function \"clear_cache\"\n",
      " - leaked function \"get_cache_size_bytes\"\n",
      " - leaked function \"__init__\"\n",
      " - leaked function \"compile_structural_tag\"\n",
      "nanobind: this is likely caused by a reference counting issue in the binding code.\n"
     ]
    }
   ],
   "source": [
    "out = llm_client.generate([prompt_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c56818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text', 'text': '## Training Examples\\n'},\n",
       " {'type': 'text',\n",
       "  'text': 'Below are 4 training examples:\\n for each example, the input grid is shown first, followed by the output grid.\\n.'},\n",
       " {'type': 'text', 'text': '\\nExample 1:\\n'},\n",
       " {'type': 'text', 'text': 'Input:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n3|5|3|3|6|6|5|4|1|4|9|9|4|3|9|9|9|9|3|4|9|9|4|1|4|5|6|6|3|3\\n5|3|3|3|6|6|4|5|4|1|9|9|3|4|9|1|1|9|4|3|9|9|1|4|5|4|6|6|3|3\\n1|1|3|5|5|4|6|6|9|1|1|4|9|9|4|5|5|4|9|9|4|1|1|9|6|6|4|5|5|3\\n1|1|5|3|4|5|6|6|1|9|4|1|9|1|4|4|4|4|1|9|1|4|9|1|6|6|5|4|3|5\\n6|9|9|9|3|5|3|3|4|3|9|9|9|2|6|9|9|6|2|9|9|9|3|4|3|3|5|3|9|9\\n9|6|9|9|5|3|3|3|3|4|9|1|9|9|9|6|6|9|9|9|1|9|4|3|3|3|3|5|9|9\\n9|9|6|9|1|1|3|5|9|9|4|4|6|9|9|2|2|9|9|6|4|4|9|9|5|3|1|1|9|6\\n9|9|9|6|1|1|5|3|9|1|5|4|9|6|9|9|9|9|6|9|4|5|1|9|3|5|1|1|6|9\\n1|4|9|1|4|3|9|9|5|5|7|2|4|3|2|4|4|2|3|4|2|7|5|5|9|9|3|4|1|9\\n4|1|1|9|3|4|9|1|4|5|2|7|3|4|4|2|2|4|4|3|7|2|5|4|1|9|4|3|9|1\\n9|9|1|4|9|9|4|5|6|4|5|5|2|4|4|3|3|4|4|2|5|5|4|6|5|4|9|9|4|1\\n9|9|4|1|9|1|4|4|4|5|4|5|4|2|3|4|4|3|2|4|5|4|5|4|4|4|1|9|1|4\\n4|3|9|9|9|9|6|9|5|9|7|7|5|5|7|2|2|7|5|5|7|7|9|5|9|6|9|9|9|9\\n3|4|9|1|2|9|9|6|9|5|7|7|4|5|2|7|7|2|5|4|7|7|5|9|6|9|9|2|1|9\\n9|9|4|4|6|9|9|9|7|7|5|9|5|4|5|5|5|5|4|5|9|5|7|7|9|8|8|8|8|4\\n9|1|5|4|9|6|2|9|7|7|9|5|4|6|4|5|5|4|6|4|5|9|7|7|9|8|8|8|8|5\\n9|1|5|4|9|6|2|9|7|7|9|5|4|6|4|5|5|4|6|4|5|9|7|7|9|8|8|8|8|5\\n9|9|4|4|6|9|9|9|7|7|5|9|5|4|5|5|5|5|4|5|9|5|7|7|9|8|8|8|8|4\\n3|4|9|1|2|9|9|6|9|5|7|7|4|5|2|7|7|2|5|4|7|7|5|9|6|8|8|8|8|9\\n4|3|9|9|9|9|6|9|5|9|7|7|5|5|7|2|2|7|5|5|7|7|9|5|9|8|8|8|8|9\\n9|9|4|1|9|1|4|4|4|5|4|5|4|2|3|4|4|3|2|4|5|4|5|4|4|8|8|8|8|4\\n9|9|1|4|9|9|4|5|6|4|5|5|2|4|4|3|3|4|4|2|5|5|4|6|5|8|8|8|8|1\\n4|1|1|9|3|4|9|1|4|5|2|7|3|4|4|2|2|4|4|3|7|2|5|4|1|8|8|8|8|1\\n1|4|9|1|4|3|9|9|5|5|7|2|4|3|2|4|4|2|3|4|2|7|5|5|9|9|3|4|1|9\\n9|9|9|6|1|1|5|3|9|1|5|4|9|6|9|9|9|9|6|9|4|5|1|9|3|5|1|1|6|9\\n9|9|6|9|1|1|3|5|9|9|4|4|6|9|9|2|2|9|9|6|4|4|9|9|5|3|1|1|9|6\\n9|6|9|9|5|3|3|3|3|4|9|1|9|9|9|6|6|9|9|9|1|9|4|3|3|3|3|5|9|9\\n6|9|9|9|3|5|3|3|4|3|9|9|9|2|6|9|9|6|2|9|9|9|3|4|3|3|5|3|9|9\\n1|1|5|3|4|5|6|6|1|9|4|1|9|1|4|4|4|4|1|9|1|4|9|1|6|6|5|4|3|5\\n1|1|3|5|5|4|6|6|9|1|1|4|9|9|4|5|5|4|9|9|4|1|1|9|6|6|4|5|5|3\\n'},\n",
       " {'type': 'text', 'text': '\\nOutput:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n9|9|6|4\\n2|6|9|4\\n2|6|9|4\\n9|9|6|4\\n9|9|2|1\\n6|9|9|9\\n4|1|9|1\\n4|9|9|4\\n9|4|3|9\\n'},\n",
       " {'type': 'text', 'text': '\\nExample 2:\\n'},\n",
       " {'type': 'text', 'text': 'Input:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n9|9|2|3|4|4|7|5|3|3|6|6|3|5|6|4|4|6|5|3|6|6|3|3|5|7|4|4|3|2\\n7|9|3|5|4|4|5|7|3|3|6|6|6|3|4|6|6|4|3|6|6|6|3|3|7|5|4|4|5|3\\n3|2|9|9|7|5|4|4|4|1|3|3|6|4|4|7|7|4|4|6|3|8|8|8|8|8|5|7|9|9\\n2|3|7|9|5|7|4|4|1|4|3|3|4|6|7|4|4|7|6|4|3|8|8|8|8|8|7|5|9|7\\n7|7|9|3|9|9|5|3|3|6|6|4|6|7|9|9|9|9|7|6|4|8|8|8|8|8|9|9|3|9\\n7|7|3|9|7|9|3|2|5|3|4|6|2|6|9|9|9|9|6|2|6|8|8|8|8|8|9|7|9|3\\n9|3|7|7|3|2|9|9|6|4|4|7|9|2|6|7|7|6|2|9|7|4|4|6|9|9|2|3|7|7\\n3|9|7|7|2|3|7|9|4|6|7|4|2|9|2|6|6|2|9|2|4|7|6|4|9|7|3|2|7|7\\n3|3|4|1|3|5|6|4|2|4|7|7|1|6|7|2|2|7|6|1|7|7|4|2|4|6|5|3|1|4\\n3|3|1|4|6|3|4|6|2|2|7|1|6|1|2|7|7|2|1|6|1|7|2|2|6|4|3|6|4|1\\n6|6|3|3|6|4|4|7|1|1|2|4|7|2|1|6|6|1|2|7|4|2|1|1|7|4|4|6|3|3\\n6|6|3|3|4|6|7|4|1|3|2|2|2|7|6|1|1|6|7|2|2|2|3|1|4|7|6|4|3|3\\n3|6|6|4|6|2|9|2|9|9|9|7|2|4|1|7|7|1|4|2|7|9|9|9|2|9|2|6|4|6\\n5|3|4|6|7|6|2|9|9|9|7|9|2|2|7|7|7|7|2|2|9|7|9|9|9|2|6|7|6|4\\n6|4|4|7|9|9|6|2|9|7|9|9|3|1|2|4|4|2|1|3|9|9|7|9|2|6|9|9|7|4\\n4|6|7|4|9|9|7|6|7|9|9|9|1|1|2|2|2|2|1|1|9|9|9|7|6|7|9|9|4|7\\n4|6|7|4|9|9|7|6|7|9|9|9|1|1|2|2|2|2|1|1|9|9|9|7|6|7|9|9|4|7\\n6|4|4|7|9|9|6|2|9|7|9|9|3|1|2|4|4|2|1|3|9|9|7|9|2|6|9|9|7|4\\n5|3|4|6|7|6|2|9|9|9|7|9|2|2|7|7|7|7|2|2|9|7|9|9|9|2|6|7|6|4\\n3|6|6|4|6|2|9|2|9|9|9|7|2|4|1|7|7|1|4|2|7|9|9|9|2|9|2|6|4|6\\n6|6|3|3|4|6|7|4|1|3|2|2|2|7|6|1|1|6|7|2|2|2|3|1|4|7|6|4|3|3\\n6|6|3|3|6|4|4|7|1|1|2|4|7|2|1|6|6|1|2|7|4|2|1|1|7|4|4|6|3|3\\n3|3|1|4|6|3|4|6|2|2|7|1|6|1|2|7|7|2|1|6|1|7|2|2|6|4|3|6|4|1\\n3|3|4|1|3|5|6|4|2|4|7|7|1|6|7|2|2|7|6|1|7|7|4|2|4|6|5|3|1|4\\n3|9|7|7|2|3|7|9|4|6|7|4|2|9|2|6|6|2|9|2|4|7|6|4|9|7|3|2|7|7\\n9|3|7|7|3|2|9|9|6|4|4|7|9|2|6|7|7|6|2|9|7|4|4|6|9|9|2|3|7|7\\n7|7|3|9|7|9|3|2|5|3|4|6|2|6|9|9|9|9|6|2|6|4|3|5|2|3|9|7|9|3\\n7|7|9|3|9|9|5|3|3|6|6|4|6|7|9|9|9|9|7|6|4|6|6|3|3|5|9|9|3|9\\n2|3|7|9|5|7|4|4|1|4|3|3|4|6|7|4|4|7|6|4|3|3|4|1|4|4|7|5|9|7\\n3|2|9|9|7|5|4|4|4|1|3|3|6|4|4|7|7|4|4|6|3|3|1|4|4|4|5|7|9|9\\n'},\n",
       " {'type': 'text', 'text': '\\nOutput:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n3|1|4|4|4\\n3|4|1|4|4\\n6|6|3|3|5\\n4|3|5|2|3\\n'},\n",
       " {'type': 'text', 'text': '\\nExample 3:\\n'},\n",
       " {'type': 'text', 'text': 'Input:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n1|9|4|4|9|9|2|7|6|6|9|9|7|6|7|2|2|7|6|7|9|9|6|6|7|2|9|9|4|4\\n7|1|4|4|9|9|7|2|6|6|9|9|6|7|2|7|7|2|7|6|9|9|6|6|2|7|9|9|4|4\\n2|7|1|9|2|7|9|9|4|4|6|6|7|2|5|1|1|5|2|7|6|6|4|4|9|9|7|2|9|1\\n7|2|7|1|7|2|9|9|4|4|6|6|2|7|5|5|5|5|7|2|6|6|4|4|9|9|2|7|1|7\\n9|6|7|2|1|9|4|4|7|6|7|2|9|2|6|4|4|6|2|9|2|7|6|7|4|4|9|1|2|7\\n6|9|2|7|7|1|4|4|6|7|2|7|9|9|4|6|6|4|9|9|7|2|7|6|4|4|1|7|7|2\\n7|2|9|6|2|7|1|9|7|2|5|5|4|5|9|2|2|9|5|4|5|5|2|7|9|1|7|2|6|9\\n2|7|6|9|7|2|7|1|2|7|1|5|5|4|9|9|9|9|4|5|5|1|7|2|1|7|2|7|9|6\\n6|6|4|4|7|6|7|2|3|7|1|4|9|7|7|6|6|7|7|9|4|1|7|3|2|7|6|7|4|4\\n6|6|4|4|6|7|2|7|4|3|4|4|7|9|6|7|7|6|9|7|4|4|3|4|7|2|7|6|4|4\\n9|9|6|6|7|2|5|1|3|7|3|7|7|6|9|7|7|9|6|7|7|3|7|3|1|5|2|7|6|6\\n9|9|6|6|2|7|5|5|7|7|4|3|6|7|7|9|9|7|7|6|3|4|7|7|5|5|7|2|6|6\\n7|6|7|2|9|9|4|5|6|6|5|9|3|7|4|4|4|4|7|3|9|5|6|6|5|4|9|9|2|7\\n6|7|2|7|2|9|5|4|6|6|9|5|4|3|4|1|1|4|3|4|5|9|6|6|4|5|9|2|7|2\\n7|2|5|5|6|4|9|9|5|9|6|6|7|7|3|7|7|3|7|7|6|6|9|5|9|9|4|6|5|5\\n2|7|1|5|4|6|2|9|9|5|6|6|7|3|4|3|3|4|3|7|6|6|5|9|9|2|6|4|5|1\\n2|7|1|5|4|6|2|9|9|5|6|6|7|3|4|3|3|4|3|7|6|6|5|9|9|2|6|4|5|1\\n7|2|5|5|6|4|9|9|5|9|6|6|7|7|3|7|7|3|7|7|6|6|9|5|9|9|4|6|5|5\\n6|7|2|7|2|9|5|4|6|6|9|5|4|3|4|1|1|4|3|4|5|9|6|6|4|5|9|2|7|2\\n7|6|7|2|9|9|4|5|6|6|5|9|8|8|8|8|8|8|8|3|9|5|6|6|5|4|9|9|2|7\\n9|9|6|6|2|7|5|5|7|7|4|3|8|8|8|8|8|8|8|6|3|4|7|7|5|5|7|2|6|6\\n9|9|6|6|7|2|5|1|3|7|3|7|8|8|8|8|8|8|8|7|7|3|7|3|1|5|2|7|6|6\\n6|6|4|4|6|7|2|7|4|3|4|4|7|9|6|7|7|6|9|7|4|4|3|4|7|2|7|6|4|4\\n6|6|4|4|7|6|7|2|3|7|1|4|9|7|7|6|6|7|7|9|4|1|7|3|2|7|6|7|4|4\\n2|7|6|9|7|2|7|1|2|7|1|5|5|4|9|9|9|9|4|5|5|1|7|2|1|7|2|7|9|6\\n7|2|9|6|2|7|1|9|7|2|5|5|4|5|9|2|2|9|5|4|5|5|2|7|9|1|7|2|6|9\\n6|9|2|7|7|1|4|4|6|7|2|7|9|9|4|6|6|4|9|9|7|2|7|6|4|4|1|7|7|2\\n9|6|7|2|1|9|4|4|7|6|7|2|9|2|6|4|4|6|2|9|2|7|6|7|4|4|9|1|2|7\\n7|2|7|1|7|2|9|9|4|4|6|6|2|7|5|5|5|5|7|2|6|6|4|4|9|9|2|7|1|7\\n2|7|1|9|2|7|9|9|4|4|6|6|7|2|5|1|1|5|2|7|6|6|4|4|9|9|7|2|9|1\\n'},\n",
       " {'type': 'text', 'text': '\\nOutput:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n3|7|4|4|4|4|7\\n6|7|7|9|9|7|7\\n7|6|9|7|7|9|6\\n'},\n",
       " {'type': 'text', 'text': '\\nExample 4:\\n'},\n",
       " {'type': 'text', 'text': 'Input:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n3|1|1|9|5|6|7|1|1|4|5|7|3|9|9|1|1|9|9|3|7|5|4|1|1|7|6|5|9|1\\n1|3|9|5|6|5|1|7|4|1|7|5|4|3|1|3|3|1|3|4|5|7|1|4|7|1|5|6|5|9\\n6|9|3|1|7|1|5|6|9|9|1|4|9|1|1|4|4|1|1|9|4|1|9|9|6|5|1|7|1|3\\n9|1|1|3|1|7|6|5|9|9|4|1|1|3|4|1|1|4|3|1|1|4|9|9|5|6|7|1|3|1\\n6|6|6|7|3|1|5|9|3|4|9|1|6|7|2|5|5|2|7|6|1|9|4|3|9|5|1|3|7|6\\n6|6|7|6|1|3|9|1|9|3|1|3|7|6|5|2|2|5|6|7|3|1|3|9|1|9|3|1|6|7\\n6|7|6|6|1|9|3|1|9|1|1|4|6|9|6|7|7|6|9|6|4|1|1|9|1|3|9|1|6|6\\n7|6|6|6|9|6|1|3|1|3|4|1|9|6|7|6|6|7|6|9|1|4|3|1|3|1|8|8|8|8\\n1|4|9|9|3|9|9|1|1|1|6|1|5|2|5|5|5|5|2|5|1|6|1|1|1|9|8|8|8|8\\n4|1|9|9|4|3|1|3|1|1|1|6|2|5|5|5|5|5|5|2|6|1|1|1|3|1|8|8|8|8\\n5|7|1|4|9|1|1|4|2|2|1|1|5|5|5|2|2|5|5|5|1|1|2|2|4|1|8|8|8|8\\n7|5|4|1|1|3|4|1|2|1|1|1|5|5|2|5|5|2|5|5|1|1|1|2|1|4|3|1|1|4\\n3|4|9|1|6|7|6|9|7|6|3|3|1|1|6|1|1|6|1|1|3|3|6|7|9|6|7|6|1|9\\n9|3|1|3|7|6|9|6|6|7|3|3|1|1|1|6|6|1|1|1|3|3|7|6|6|9|6|7|3|1\\n9|1|1|4|2|5|6|7|3|3|7|6|1|2|1|1|1|1|2|1|6|7|3|3|7|6|5|2|4|1\\n1|3|4|1|5|2|7|6|3|3|6|7|2|2|1|1|1|1|2|2|7|6|3|3|6|7|2|5|1|4\\n1|3|4|1|5|2|7|6|3|3|6|7|2|2|1|1|1|1|2|2|7|6|3|3|6|7|2|5|1|4\\n9|1|1|4|2|5|6|7|3|3|7|6|1|2|1|1|1|1|2|1|6|7|3|3|7|6|5|2|4|1\\n9|3|1|3|7|6|9|6|6|7|3|3|1|1|1|6|6|1|1|1|3|3|7|6|6|9|6|7|3|1\\n3|4|9|1|6|7|6|9|7|6|3|3|1|1|6|1|1|6|1|1|3|3|6|7|9|6|7|6|1|9\\n7|5|4|1|1|3|4|1|2|1|1|1|5|5|2|5|5|2|5|5|1|1|1|2|1|4|3|1|1|4\\n5|7|1|4|9|1|1|4|2|2|1|1|5|5|5|2|2|5|5|5|1|1|2|2|4|1|1|9|4|1\\n4|1|9|9|4|3|1|3|1|1|1|6|2|5|5|5|5|5|5|2|6|1|1|1|3|1|3|4|9|9\\n1|4|9|9|3|9|9|1|1|1|6|1|5|2|5|5|5|5|2|5|1|6|1|1|1|9|9|3|9|9\\n7|6|6|6|9|6|1|3|1|3|4|1|9|6|7|6|6|7|6|9|1|4|3|1|3|1|6|9|6|6\\n6|7|6|6|1|9|3|1|9|1|1|4|6|9|6|7|7|6|9|6|4|1|1|9|1|3|9|1|6|6\\n6|6|7|6|1|3|9|1|9|3|1|3|7|6|5|2|2|5|6|7|3|1|3|9|1|9|3|1|6|7\\n6|6|6|7|3|1|5|9|3|4|9|1|6|7|2|5|5|2|7|6|1|9|4|3|9|5|1|3|7|6\\n9|1|1|3|1|7|6|5|9|9|4|1|1|3|4|1|1|4|3|1|1|4|9|9|5|6|7|1|3|1\\n6|9|3|1|7|1|5|6|9|9|1|4|9|1|1|4|4|1|1|9|4|1|9|9|6|5|1|7|1|3\\n'},\n",
       " {'type': 'text', 'text': '\\nOutput:\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\nASCII representation:\\n6|9|6|6\\n9|3|9|9\\n3|4|9|9\\n1|9|4|1\\n'},\n",
       " {'type': 'text', 'text': '\\n## Similar Programs from Library\\n'},\n",
       " {'type': 'text',\n",
       "  'text': 'The following programs may be useful to solve the current tasks, feel free to use parts of each program, and/or combine them. Most importanly, use them as guidance:\\n\\nSimilar Program 1 (similarity: 0.11, task: 1cf80156):\\n```python\\ndef solve_1cf80156(I):\\n    x1 = as_objects(I, True, True, True)\\n    x2 = get_first(x1)\\n    O = smallest_subgrid_containing(x2, I)\\n    return O\\n\\n```\\n\\nSimilar Program 2 (similarity: 0.10, task: ce4f8723):\\n```python\\ndef solve_ce4f8723(I):\\n    x1 = top_half(I)\\n    x2 = bottom_half(I)\\n    x3 = of_color(x1, COLOR_ZERO)\\n    x4 = of_color(x2, COLOR_ZERO)\\n    x5 = intersection(x3, x4)\\n    x6 = as_tuple(4, 4)\\n    x7 = create_grid(COLOR_THREE, x6)\\n    O = fill(x7, COLOR_ZERO, x5)\\n    return O\\n\\n```\\n\\nSimilar Program 3 (similarity: 0.10, task: f25fbde4):\\n```python\\ndef solve_f25fbde4(I):\\n    x1 = as_objects(I, True, True, True)\\n    x2 = get_first(x1)\\n    x3 = smallest_subgrid_containing(x2, I)\\n    O = upscale(x3, 2)\\n    return O\\n\\n```\\n'},\n",
       " {'type': 'text',\n",
       "  'text': '\\n## Analysis Protocol\\n\\nYou will analyze these examples systematically, allowing your hypothesis to evolve \\nnaturally as you see more data - like a human solving a puzzle.\\n\\n**Core principle:** Look for DIFFERENCES within each example (inputâ†’output changes) \\nand SIMILARITIES across all examples (the consistent pattern).\\n\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\\nWork through examples sequentially, reasoning in plain English about what you observe.\\n\\nStep 1.1: First Example Analysis\\nGiven: Input 1 â†’ Output 1\\n\\n<observation_1>\\nDescribe what you see:\\n\\n    What\\'s the size/shape change?\\n\\nWhat colors changed?What geometric transformations occurred?What patterns do you notice?</observation_1>\\n\\n<hypothesis_1>\\nState your initial guess about the transformation rule in natural language.\\nExample: \"The grid appears to be flipped horizontally\"\\n</hypothesis_1>\\n\\nStep 1.2: Second Example Validation\\nGiven: Input 2 â†’ Output 2\\n\\n<observation_2>\\n\\n    Does your hypothesis from Example 1 still hold?\\n\\nWhat\\'s similar to Example 1?What\\'s different from Example 1?</observation_2>\\n\\n<hypothesis_2>\\nRefine your hypothesis:\\n\\n    If it still works: Confirm and strengthen\\n\\nIf it breaks: Revise with a more general patternExample: \"Actually, it\\'s mirrored horizontally, THEN the original is stacked on top\"\\n</hypothesis_2>\\n\\nStep 1.3: Third Example Confirmation\\nGiven: Input 3 â†’ Output 3\\n\\n<observation_3>\\n\\n    Does hypothesis_2 work here?\\n\\nAny new edge cases or variations?</observation_3>\\n\\n<hypothesis_3>\\nFinal refined hypothesis in natural language.\\n</hypothesis_3>\\n\\nStep 1.N: Additional Examples\\nContinue for all remaining examples...\\n\\nStep 1.Final: Pattern Synthesis\\n<pattern_summary>\\nIn plain English, the transformation rule is:\\n\\n    [First operation in natural language]\\n\\n[Second operation in natural language][Any conditions or special cases]\\nEdge cases to consider:\\n\\n    [Any variations you noticed]\\n\\n\\nWhy this works:\\n\\n    [Your reasoning about WHY this pattern makes sense]\\n\\n</pattern_summary>\\n\\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\n\\nBegin your analysis:'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase1_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2dd2b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phase1_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m phase2_prompt = prompter.build_phase2_prompt(task, \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mphase1_output\u001b[49m,\n\u001b[32m      3\u001b[39m     similar_programs\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m phase2_output = vlm_client_phase2.query(\n\u001b[32m      7\u001b[39m     phase2_prompt,\n\u001b[32m      8\u001b[39m     system_prompt=\u001b[33m\"\u001b[39m\u001b[33mYou are an expert at generating code using the given DSL primitives to solve ARC puzzles. You are provided with a natural language description of the pattern to implement, as well as training examples and some similar programs you might find useful as reference. Generate a Python function `def solve(I):` that implements the described transformation using ONLY the provided DSL primitives. Ensure your code is syntactically correct and follows best practices.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[31mNameError\u001b[39m: name 'phase1_output' is not defined"
     ]
    }
   ],
   "source": [
    "phase2_prompt = prompter.build_phase2_prompt(task, \n",
    "    phase1_output,\n",
    "    similar_programs\n",
    ")\n",
    "\n",
    "phase2_output = vlm_client_phase2.query(\n",
    "    phase2_prompt,\n",
    "    system_prompt=\"You are an expert at generating code using the given DSL primitives to solve ARC puzzles. You are provided with a natural language description of the pattern to implement, as well as training examples and some similar programs you might find useful as reference. Generate a Python function `def solve(I):` that implements the described transformation using ONLY the provided DSL primitives. Ensure your code is syntactically correct and follows best practices.\"\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    print(f\"   âœ“ Phase 2 complete ({len(phase2_output)} chars)\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e202534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated score: 0.13\n"
     ]
    }
   ],
   "source": [
    "generated_code = extract_code_from_response(phase2_output)\n",
    "score, results = test_program(generated_code, task)\n",
    "print(f\"   Generated score: {score:.2f}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd438358",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m sglang.launch_server --model-path /home/flowers/work/hf/Qwen3-4B-Instruct-2507 --tp 1 --port 8000 --mem-fraction-static 0.9 --random-seed 42 --host 0.0.0.0 --log-level info --trust-remote-code --quantization fp8 --context-length 32000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_directory(\n",
    "    data_dir='data_v1/eval_size_10',\n",
    "    vlm_client_phase1=vlm_client_phase1,\n",
    "    vlm_client_phase2=vlm_client_phase2,\n",
    "    prompter=prompter,\n",
    "    library=library,\n",
    "    verbose=True,\n",
    "    n_workers=None,  # Auto-detect CPUs (recommended)\n",
    "    timeout=2        # 2 second timeout per program\n",
    ")\n",
    "\n",
    "# save_results(results, output_dir='results/images')#TODO change output dir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
